<!DOCTYPE html>
<!-- saved from url=(0083)file:///D:/LLM_from_scratch/Images/The%20Big%20LLM%20Architecture%20Comparison.html -->
<html lang="en" class="" data-headlessui-focus-visible="" style="background: rgb(255, 255, 255);"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        
        <meta name="norton-safeweb-site-verification" content="24usqpep0ejc5w6hod3dulxwciwp0djs6c6ufp96av3t4whuxovj72wfkdjxu82yacb7430qjm8adbd5ezlt4592dq4zrvadcn9j9n-0btgdzpiojfzno16-fnsnu7xd">
        
        <link rel="preconnect" href="https://substackcdn.com/">
        

        

        

        <style>
          @layer legacy, tailwind, pencraftReset, pencraft;
        </style>

        
        <link rel="preload" as="style" href="./The Big LLM Architecture Comparison_files/main.88280fc045592e54bb5e.css">
        
        
        

        
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/5340.e1c57fe5.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/1832.1f3239f5.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/main.b8ef093a.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/9335.1cd242d9.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/1481.8fbd7ad5.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/9294.03de8b18.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/5758.602fd321.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/1333.27122476.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/7294.aabc06b7.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/6364.9c2db661.css">
            
                <link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/3191.f2f22825.css">
            
        

        
        
        
        
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0, viewport-fit=cover">
        <meta name="author" content="Sebastian Raschka, PhD">
        <meta property="og:url" content="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison">
        <title>The Big LLM Architecture Comparison</title>
        
        <link rel="canonical" href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison">
        

        

        

        
            
                <link rel="shortcut icon" href="https://substackcdn.com/image/fetch/$s_!H1CS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Ffavicon.ico">
            
        
            
                <link rel="icon" type="image/png" sizes="16x16" href="https://substackcdn.com/image/fetch/$s_!fDFi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Ffavicon-16x16.png">
            
        
            
                <link rel="icon" type="image/png" sizes="32x32" href="https://substackcdn.com/image/fetch/$s_!flCl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Ffavicon-32x32.png">
            
        
            
                <link rel="icon" type="image/png" sizes="48x48" href="https://substackcdn.com/image/fetch/$s_!ktrK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Ffavicon-48x48.png">
            
        
            
                <link rel="apple-touch-icon" sizes="57x57" href="https://substackcdn.com/image/fetch/$s_!_bhP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-57x57.png">
            
        
            
                <link rel="apple-touch-icon" sizes="60x60" href="https://substackcdn.com/image/fetch/$s_!Ilxb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-60x60.png">
            
        
            
                <link rel="apple-touch-icon" sizes="72x72" href="https://substackcdn.com/image/fetch/$s_!AKdi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-72x72.png">
            
        
            
                <link rel="apple-touch-icon" sizes="76x76" href="https://substackcdn.com/image/fetch/$s_!MbC4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-76x76.png">
            
        
            
                <link rel="apple-touch-icon" sizes="114x114" href="https://substackcdn.com/image/fetch/$s_!R5rH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-114x114.png">
            
        
            
                <link rel="apple-touch-icon" sizes="120x120" href="https://substackcdn.com/image/fetch/$s_!naEp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-120x120.png">
            
        
            
                <link rel="apple-touch-icon" sizes="144x144" href="https://substackcdn.com/image/fetch/$s_!kvAt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-144x144.png">
            
        
            
                <link rel="apple-touch-icon" sizes="152x152" href="https://substackcdn.com/image/fetch/$s_!5uKp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-152x152.png">
            
        
            
                <link rel="apple-touch-icon" sizes="167x167" href="https://substackcdn.com/image/fetch/$s_!HPVF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-167x167.png">
            
        
            
                <link rel="apple-touch-icon" sizes="180x180" href="https://substackcdn.com/image/fetch/$s_!oGr3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-180x180.png">
            
        
            
                <link rel="apple-touch-icon" sizes="1024x1024" href="https://substackcdn.com/image/fetch/$s_!VgZ6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342e39ec-514c-4b4d-8a76-6acf0cbd8248%2Fapple-touch-icon-1024x1024.png">
            
        
            
        
            
        
            
        

        

        
            <link rel="alternate" type="application/rss+xml" href="https://magazine.sebastianraschka.com/feed" title="Ahead of AI">
        

        
        
        

        <style>:root{--color_theme_bg_pop:#c5030c;--background_pop:#c5030c;--cover_bg_color:#FFFFFF;--background_pop_darken:#ac030a;--print_on_pop:#ffffff;--color_theme_bg_pop_darken:#ac030a;--color_theme_print_on_pop:#ffffff;--color_theme_bg_pop_20:rgba(197, 3, 12, 0.2);--color_theme_bg_pop_30:rgba(197, 3, 12, 0.3);--border_subtle:rgba(204, 204, 204, 0.5);--background_subtle:rgba(246, 217, 219, 0.4);--print_pop:#c5030c;--color_theme_accent:#c5030c;--cover_print_primary:#363737;--cover_print_secondary:#757575;--cover_print_tertiary:#b6b6b6;--cover_border_color:#c5030c;--font_family_body_preset:'SF Pro Display', -apple-system, system-ui, BlinkMacSystemFont, 'Inter', 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';--font_weight_body_preset:400;--font_preset_body:sans;--home_hero:newspaper;--home_posts:custom;--home_show_top_posts:true;--web_bg_color:#ffffff;--background_contrast_1:#f0f0f0;--color_theme_bg_contrast_1:#f0f0f0;--background_contrast_2:#dddddd;--color_theme_bg_contrast_2:#dddddd;--background_contrast_3:#b7b7b7;--color_theme_bg_contrast_3:#b7b7b7;--background_contrast_4:#929292;--color_theme_bg_contrast_4:#929292;--background_contrast_5:#515151;--color_theme_bg_contrast_5:#515151;--color_theme_bg_elevated:#ffffff;--color_theme_bg_elevated_secondary:#f0f0f0;--color_theme_detail:#e6e6e6;--background_contrast_pop:rgba(197, 3, 12, 0.4);--color_theme_bg_contrast_pop:rgba(197, 3, 12, 0.4);--input_background:#ffffff;--cover_input_background:#ffffff;--tooltip_background:#191919;--web_bg_color_h:0;--web_bg_color_s:0%;--web_bg_color_l:100%;--print_on_web_bg_color:#363737;--print_secondary_on_web_bg_color:#868787;--selected_comment_background_color:#fdf9f3;--background_pop_rgb:197, 3, 12;--background_pop_rgb_pc:197 3 12;--color_theme_bg_pop_rgb:197, 3, 12;--color_theme_bg_pop_rgb_pc:197 3 12;--color_theme_accent_rgb:197, 3, 12;--color_theme_accent_rgb_pc:197 3 12;}</style>

        
            <link rel="stylesheet" href="./The Big LLM Architecture Comparison_files/main.88280fc045592e54bb5e.css">
        

        <style></style>

        

        

        

        
            <script async="" src="./The Big LLM Architecture Comparison_files/datadog-rum.js.download"></script><script async="" src="./The Big LLM Architecture Comparison_files/js(2)"></script><script async="" src="./The Big LLM Architecture Comparison_files/datadog-rum(1).js.download"></script><script async="true" src="./The Big LLM Architecture Comparison_files/js(3)">
            </script>
        
    <style type="text/css">/*
  code is extracted from Calendly's embed stylesheet: https://assets.calendly.com/assets/external/widget.css
*/

.calendly-inline-widget,
.calendly-inline-widget *,
.calendly-badge-widget,
.calendly-badge-widget *,
.calendly-overlay,
.calendly-overlay * {
    font-size:16px;
    line-height:1.2em
}

.calendly-inline-widget iframe,
.calendly-badge-widget iframe,
.calendly-overlay iframe {
    display:inline;
    width:100%;
    height:100%
}

.calendly-popup-content {
    position:relative
}

.calendly-popup-content.calendly-mobile {
    -webkit-overflow-scrolling:touch;
    overflow-y:auto
}

.calendly-overlay {
    position:fixed;
    top:0;
    left:0;
    right:0;
    bottom:0;
    overflow:hidden;
    z-index:9999;
    background-color:#a5a5a5;
    background-color:rgba(31,31,31,0.4)
}

.calendly-overlay .calendly-close-overlay {
    position:absolute;
    top:0;
    left:0;
    right:0;
    bottom:0
}

.calendly-overlay .calendly-popup {
    box-sizing:border-box;
    position:absolute;
    top:50%;
    left:50%;
    -webkit-transform:translateY(-50%) translateX(-50%);
    transform:translateY(-50%) translateX(-50%);
    width:80%;
    min-width:900px;
    max-width:1000px;
    height:90%;
    max-height:680px
}

@media (max-width: 975px) {
    .calendly-overlay .calendly-popup {
        position:fixed;
        top:50px;
        left:0;
        right:0;
        bottom:0;
        -webkit-transform:none;
        transform:none;
        width:100%;
        height:auto;
        min-width:0;
        max-height:none
    }
}

.calendly-overlay .calendly-popup .calendly-popup-content {
    height:100%;
}

.calendly-overlay .calendly-popup-close {
    position:absolute;
    top:25px;
    right:25px;
    color:#fff;
    width:19px;
    height:19px;
    cursor:pointer;
    background:url(https://assets.calendly.com/assets/external/close-icon.svg) no-repeat;
    background-size:contain
}

@media (max-width: 975px) {
    .calendly-overlay .calendly-popup-close {
        top:15px;
        right:15px
    }
}

.calendly-badge-widget {
    position:fixed;
    right:20px;
    bottom:15px;
    z-index:9998
}

.calendly-badge-widget .calendly-badge-content {
    display:table-cell;
    width:auto;
    height:45px;
    padding:0 30px;
    border-radius:25px;
    box-shadow:rgba(0,0,0,0.25) 0 2px 5px;
    font-family:sans-serif;
    text-align:center;
    vertical-align:middle;
    font-weight:bold;
    font-size:14px;
    color:#fff;
    cursor:pointer
}

.calendly-badge-widget .calendly-badge-content.calendly-white {
    color:#666a73
}

.calendly-badge-widget .calendly-badge-content span {
    display:block;
    font-size:12px
}

.calendly-spinner {
    position:absolute;
    top:50%;
    left:0;
    right:0;
    -webkit-transform:translateY(-50%);
    transform:translateY(-50%);
    text-align:center;
    z-index:-1
}

.calendly-spinner>div {
    display:inline-block;
    width:18px;
    height:18px;
    background-color:#e1e1e1;
    border-radius:50%;
    vertical-align:middle;
    -webkit-animation:calendly-bouncedelay 1.4s infinite ease-in-out;
    animation:calendly-bouncedelay 1.4s infinite ease-in-out;
    -webkit-animation-fill-mode:both;
    animation-fill-mode:both
}

.calendly-spinner .calendly-bounce1 {
    -webkit-animation-delay:-0.32s;
    animation-delay:-0.32s
}

.calendly-spinner .calendly-bounce2 {
    -webkit-animation-delay:-0.16s;
    animation-delay:-0.16s
}

@-webkit-keyframes calendly-bouncedelay {
    0%,80%,100% {
        -webkit-transform:scale(0);
        transform:scale(0)
    } 
    
    40%{
        -webkit-transform:scale(1);
        transform:scale(1)
    }
}

@keyframes calendly-bouncedelay{ 
    0%,80%,100% {
        -webkit-transform:scale(0);
        transform:scale(0)
    }
    
    40% {
        -webkit-transform:scale(1);
        transform:scale(1)
    }
}</style><meta property="og:type" content="article" data-preact-helmet="true"><meta name="theme-color" content="#ffffff" data-preact-helmet="true"><meta name="twitter:card" content="summary_large_image" data-preact-helmet="true"><link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/4918.b663e246.css"><link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/3280.8db3c09e.css"><script type="text/javascript" async="" src="./The Big LLM Architecture Comparison_files/f(2).txt"></script><meta property="og:title" content="The Big LLM Architecture Comparison" data-preact-helmet="true"><meta name="twitter:title" content="The Big LLM Architecture Comparison" data-preact-helmet="true"><meta name="description" content="From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design" data-preact-helmet="true"><meta property="og:description" content="From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design" data-preact-helmet="true"><meta name="twitter:description" content="From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design" data-preact-helmet="true"><meta property="og:image" content="https://substackcdn.com/image/fetch/$s_!LmVE!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45c50202-0e8b-4e64-8296-4e2ccf4cb287_1756x1227.png" data-preact-helmet="true"><meta name="twitter:image" content="https://substackcdn.com/image/fetch/$s_!ofPU!,f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Fsebastianraschka.substack.com%2Fapi%2Fv1%2Fpost_preview%2F168650848%2Ftwitter.jpg%3Fversion%3D4" data-preact-helmet="true"><meta property="interactionStatistic" content="[{&quot;@type&quot;:&quot;InteractionCounter&quot;,&quot;interactionType&quot;:&quot;https://schema.org/LikeAction&quot;,&quot;userInteractionCount&quot;:1181},{&quot;@type&quot;:&quot;InteractionCounter&quot;,&quot;interactionType&quot;:&quot;https://schema.org/CommentAction&quot;,&quot;userInteractionCount&quot;:60}]" data-preact-helmet="true"><link rel="stylesheet" type="text/css" href="./The Big LLM Architecture Comparison_files/9290.a028dfa8.css"></head>

    <body class=" ">
        

        

        

        

        

        

        <div id="entry"><iframe src="./The Big LLM Architecture Comparison_files/channel-frame.html" width="0" height="0" class="channel-frame"></iframe><iframe src="./The Big LLM Architecture Comparison_files/session-attribution-frame.html" width="0" height="0" class="visitedSurfacesIFrame-yy8AJL"></iframe><div id="main" class="main typography use-theme-bg"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div data-testid="navbar" class="main-menu"><div class="mainMenuContent-DME8DR" style="position: fixed; top: -88px;"><div style="position: relative; height: 87px;" class="pencraft pc-display-flex pc-gap-12 pc-paddingLeft-20 pc-paddingRight-20 pc-justifyContent-space-between pc-alignItems-center pc-reset border-bottom-detail-k1F6C4 topBar-pIF0J1"><div style="flex-basis: 0px; flex-grow: 1;" class="logoContainer-p12gJb"><a href="https://magazine.sebastianraschka.com/" native="true" class="pencraft pc-display-contents pc-reset"><div draggable="false" class="pencraft pc-display-flex pc-position-relative pc-reset"><div style="width: 40px; height: 40px;" class="pencraft pc-display-flex pc-reset bg-white-ZBV5av pc-borderRadius-sm overflow-hidden-WdpwT6 sizing-border-box-DggLA4"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!96vs!,w_80,h_80,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png"><img src="./The Big LLM Architecture Comparison_files/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png" sizes="100vw" alt="Ahead of AI" width="80" height="80" style="width: 40px; height: 40px;" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div><div style="flex-grow: 0;" class="titleContainer-DJYq5v"><h1 class="pencraft pc-reset font-pub-headings-FE5byy reset-IxiVJZ title-oOnUGd titleWithWordmark-GfqxEZ"><a href="https://magazine.sebastianraschka.com/" class="pencraft pc-display-contents pc-reset"><img alt="Ahead of AI" src="./The Big LLM Architecture Comparison_files/https___substack-post-media.s3.amazonaws.com_public_images_5083e6d3-fbc9-4870-95b9-6e85d02f62a6_9366x2023.png" style="display: block; height: 36px;"></a></h1></div><div style="flex-basis: 0px; flex-grow: 1;" class="pencraft pc-display-flex pc-justifyContent-flex-end pc-alignItems-center pc-reset"><div class="buttonsContainer-SJBuep"><div class="pencraft pc-display-flex pc-gap-8 pc-justifyContent-flex-end pc-alignItems-center pc-reset navbar-buttons"><div class="pencraft pc-display-flex pc-gap-4 pc-reset"><span><button type="button" aria-label="Search" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg></button></span><button type="button" aria-label="View more" id="trigger1" aria-expanded="false" aria-haspopup="dialog" aria-controls="dialog2" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg></button></div><button type="button" data-testid="noncontributor-cta-button" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-RfbeYt size_md-gCDS3o" tabindex="0">Upgrade to paid</button><button type="button" native="true" data-href="https://substack.com/sign-in?redirect=%2Fp%2Fthe-big-llm-architecture-comparison&amp;for_pub=sebastianraschka" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_tertiary-rlke8z size_md-gCDS3o" tabindex="0">Sign in</button></div></div></div></div></div><div style="height: 88px;"></div></div></div><div><script type="application/ld+json">{"@context":"https://schema.org","@type":"NewsArticle","url":"https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison","mainEntityOfPage":"https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison","headline":"The Big LLM Architecture Comparison","description":"From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design","image":[{"@type":"ImageObject","url":"https://substack-post-media.s3.amazonaws.com/public/images/45c50202-0e8b-4e64-8296-4e2ccf4cb287_1756x1227.png"}],"datePublished":"2025-07-19T16:41:10+05:30","dateModified":"2025-07-19T16:41:10+05:30","isAccessibleForFree":true,"author":[{"@type":"Person","name":"Sebastian Raschka, PhD","url":"https://substack.com/@rasbt","description":"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \"Build a Large Language Model From Scratch\" (amzn.to/4fqvn0D).","identifier":"user:27393275","sameAs":["https://twitter.com/rasbt"],"image":{"@type":"ImageObject","contentUrl":"https://substackcdn.com/image/fetch/$s_!CfW_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg","thumbnailUrl":"https://substackcdn.com/image/fetch/$s_!CfW_!,w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg"}}],"publisher":{"@type":"Organization","name":"Ahead of AI","url":"https://magazine.sebastianraschka.com","description":"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.","interactionStatistic":{"@type":"InteractionCounter","name":"Subscribers","interactionType":"https://schema.org/SubscribeAction","userInteractionCount":100000},"identifier":"pub:1174659","logo":{"@type":"ImageObject","url":"https://substackcdn.com/image/fetch/$s_!96vs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png","contentUrl":"https://substackcdn.com/image/fetch/$s_!96vs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png","thumbnailUrl":"https://substackcdn.com/image/fetch/$s_!96vs!,w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png"},"image":{"@type":"ImageObject","url":"https://substackcdn.com/image/fetch/$s_!96vs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png","contentUrl":"https://substackcdn.com/image/fetch/$s_!96vs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png","thumbnailUrl":"https://substackcdn.com/image/fetch/$s_!96vs!,w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png"},"sameAs":["https://twitter.com/rasbt"]}}</script><div aria-label="Post" role="main" class="single-post-container"><div class="container"><div class="single-post"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><article class="typography newsletter-post post"><div role="region" aria-label="Post header" class="post-header"><h1 dir="auto" class="post-title published title-X77sOw">The Big LLM Architecture Comparison</h1><h3 dir="auto" class="subtitle subtitle-HEEcLo">From DeepSeek-V3 to gpt-oss: A Look At Modern LLM Architecture Design</h3><div aria-label="Post UFI" role="region" class="pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-16 pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingTop-16 pc-paddingBottom-16 pc-reset"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset byline-wrapper"><div class="pencraft pc-display-flex pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-row pc-gap-8 pc-alignItems-center pc-justifyContent-flex-start pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-row pc-alignItems-center pc-justifyContent-flex-start pc-reset ltr-qDBmby" style="--scale: 36px; --offset: 9px; --border-width: 4.5px;"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/@rasbt" aria-label="View Sebastian Raschka, PhD&#39;s profile" class="pencraft pc-display-contents pc-reset"><div tabindex="0" class="pencraft pc-display-flex pc-width-36 pc-height-36 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6 avatar-u8q6xB last-JfNEJ_" style="--scale: 36px;"><div title="Sebastian Raschka, PhD" class="pencraft pc-display-flex pc-width-36 pc-height-36 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 36px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!CfW_!,w_36,h_36,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg 36w, https://substackcdn.com/image/fetch/$s_!CfW_!,w_72,h_72,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg 72w, https://substackcdn.com/image/fetch/$s_!CfW_!,w_108,h_108,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg 108w" sizes="36px"><img src="./The Big LLM Architecture Comparison_files/61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpg" sizes="36px" alt="Sebastian Raschka, PhD&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!CfW_!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg 36w, https://substackcdn.com/image/fetch/$s_!CfW_!,w_72,h_72,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg 72w, https://substackcdn.com/image/fetch/$s_!CfW_!,w_108,h_108,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg 108w" width="36" height="36" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div></div></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><div class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/@rasbt" class="pencraft pc-reset decoration-hover-underline-ClDVRM reset-IxiVJZ">Sebastian Raschka, PhD</a></div></div><div class="pencraft pc-display-flex pc-gap-4 pc-reset"><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Jul 19, 2025</div></div></div></div></div><div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-16 pc-paddingBottom-16 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG border-top-detail-themed-k9TZAY border-bottom-detail-themed-Ua9186 post-ufi"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="like-button-container post-ufi-button style-button"><a role="button" aria-label="Like (1,181)" aria-pressed="false" class="post-ufi-button style-button has-label with-border"><svg role="img" width="20" height="20" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 20px; width: 20px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><div class="label">1,181</div></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><a role="button" href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison/comments" aria-label="View comments (60)" class="post-ufi-button style-button post-ufi-comment-button has-label with-border"><svg role="img" width="20" height="20" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 20px; width: 20px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg></g></svg><div class="label">60</div></a><a role="button" class="post-ufi-button style-button has-label with-border"><svg role="img" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 20px; width: 20px;"><g><title></title><path d="M21 3V8M21 8H16M21 8L18 5.29962C16.7056 4.14183 15.1038 3.38328 13.3879 3.11547C11.6719 2.84766 9.9152 3.08203 8.32951 3.79031C6.74382 4.49858 5.39691 5.65051 4.45125 7.10715C3.5056 8.5638 3.00158 10.2629 3 11.9996M3 21V16M3 16H8M3 16L6 18.7C7.29445 19.8578 8.89623 20.6163 10.6121 20.8841C12.3281 21.152 14.0848 20.9176 15.6705 20.2093C17.2562 19.501 18.6031 18.3491 19.5487 16.8925C20.4944 15.4358 20.9984 13.7367 21 12" stroke-linecap="round" stroke-linejoin="round"></path></g></svg><div class="label">111</div></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><a role="button" href="javascript:void(0)" aria-label="View share options" class="post-ufi-button style-button no-icon has-label with-border"><div class="label">Share</div></a></div></div></div></div><div class="visibility-check"></div><div><button tabindex="0" type="button" aria-label="Table of Contents" class="pencraft pc-position-fixed pc-reset pencraft trigger-V8d1vI fixed-n4RrZu"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-12 pc-paddingLeft-8 pc-paddingRight-8 pc-paddingTop-12 pc-paddingBottom-12 pc-alignItems-flex-end pc-reset"><div class="line-DsYVXw"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw indent-2-yiZ6hb"></div><div class="line-DsYVXw indent-2-yiZ6hb"></div><div class="line-DsYVXw indent-2-yiZ6hb"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw"></div><div class="line-DsYVXw indent-1-vdeuvl active-Yh0Zwm"></div><div class="line-DsYVXw indent-1-vdeuvl"></div><div class="line-DsYVXw indent-1-vdeuvl"></div></div></button><div class="available-content"><div dir="auto" class="body markup"><p>It has been seven years since the original GPT architecture was developed. At first glance, looking back at GPT-2 (2019) and forward to DeepSeek-V3 and Llama 4 (2024-2025), one might be surprised at how structurally similar these models still are.</p><p>Sure, positional embeddings have evolved from absolute to rotational (RoPE), Multi-Head Attention has largely given way to Grouped-Query Attention, and the more efficient SwiGLU has replaced activation functions like GELU. But beneath these minor refinements, have we truly seen groundbreaking changes, or are we simply polishing the same architectural foundations?</p><p>Comparing LLMs to determine the key ingredients that contribute to their good (or not-so-good) performance is notoriously challenging: datasets, training techniques, and hyperparameters vary widely and are often not well documented.</p><p>However, I think that there is still a lot of value in examining the structural changes of the architectures themselves to see what LLM developers are up to in 2025. (A subset of them are shown in Figure 1 below.)</p><p></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!iCn-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!iCn-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 424w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 848w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1272w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.jpg" width="1456" height="1016" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1016,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1563062,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!iCn-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 424w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 848w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1272w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1456w" sizes="100vw" fetchpriority="high" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 1: A subset of the architectures covered in this article.</figcaption></figure></div><p></p><p>So, in this article, rather than writing about benchmark performance or training algorithms, I will focus on the architectural developments that define today's flagship open models.</p><p><span>(As you may remember, </span><a href="https://magazine.sebastianraschka.com/p/understanding-multimodal-llms" rel="">I wrote about multimodal LLMs</a><span> not too long ago; in this article, I will focus on the text capabilities of recent models and leave the discussion of multimodal capabilities for another time.)</span></p><p><strong>Tip:</strong><span> This is a fairly comprehensive article, so I recommend using the navigation bar to access the table of contents (just hover over the left side of the Substack page).</span></p><p></p><div><hr></div><p><strong>Optional:</strong><span> The video below is a narrated and abridged version of this article.</span></p><div data-component-name="VideoEmbedPlayer" id="media-ed02ea48-ae07-426d-90af-ebee5deabee2" class="videoScrollTarget-SzB20Y"><div class="videoEmbed-_FycLU"><div role="region" aria-label="Video player" class="with-preview full-width video-player-with-background video-player-wrapper full-width"><div class="video-player video-player video-player-with-background videoPlayer-vlcedM" style="padding-bottom: 56.25%;"><video controlslist="nodownload" poster="https://substack-video.s3.amazonaws.com/video_upload/post/168650848/ed02ea48-ae07-426d-90af-ebee5deabee2/transcoded-00001.png?refresh=Mon Sep 22 2025 01:59:54 GMT+0530 (India Standard Time)" crossorigin="anonymous" class="video-P2qgwZ" src="blob:https://magazine.sebastianraschka.com/c44f09f2-4cec-420f-8a99-1542ae504d3c"></video><div class="pencraft pc-position-absolute pc-reset buttonContainer-tH3LP9 video-player-button"><button tabindex="0" type="button" aria-label="r" class="pencraft pc-reset pencraft media-XfKJl4 size_md-gCDS3o priority_primary-RfbeYt iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt rounded-SYxRdz" style="width: 72px; height: 72px;"><svg role="img" width="20" height="20" viewBox="0 0 16 16" fill="none" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" aria-label="Play" class="videoIcon-pEKkgy" style="stroke: none;"><g><title></title><path d="M3.35866 16C2.58101 16 2 15.4101 2 14.4447V1.55531C2 0.598883 2.58101 0 3.35866 0C3.75196 0 4.10056 0.134078 4.54749 0.393296L15.1575 6.54302C15.9531 7.00782 16.3106 7.39218 16.3106 8C16.3106 8.61676 15.9531 9.00112 15.1575 9.45698L4.54749 15.6067C4.10056 15.8659 3.75196 16 3.35866 16Z"></path></g></svg></button></div></div></div></div></div><div><hr></div><p></p><h1 class="header-anchor-post">1. DeepSeek V3/R1<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§deepseek-vr" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/deepseek-vr" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p><span>As you have probably heard more than once by now, </span><a href="https://arxiv.org/abs/2501.12948" rel="">DeepSeek R1</a><span> made a big impact when it was released in January 2025. DeepSeek R1 is a reasoning model built on top of the </span><a href="https://arxiv.org/abs/2412.19437" rel="">DeepSeek V3 architecture</a><span>, which was introduced in December 2024.</span></p><p>While my focus here is on architectures released in 2025, I think it’s reasonable to include DeepSeek V3, since it only gained widespread attention and adoption following the launch of DeepSeek R1 in 2025.</p><p>If you are interested in the training of DeepSeek R1 specifically, you may also find my article from earlier this year useful: </p><div data-component-name="DigestPostEmbed" class="digestPostEmbed-flwiST"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank"></a><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank"><div class="pencraft pc-reset" style="width: 70px; height: 70px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QwUc!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png"><img src="./The Big LLM Architecture Comparison_files/d6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.jpg" sizes="100vw" alt="Understanding Reasoning LLMs" width="140" height="140" class="img-OACg1c smSquare-NGbPBa pencraft pc-reset"></picture></div></a><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank"><h4 class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-24-jnGwiv font-display-nhmvtD size-20-P_cSRT weight-bold-DmI9lw reset-IxiVJZ">Understanding Reasoning LLMs</h4></a><div class="pencraft pc-display-flex pc-gap-4 pc-alignItems-center pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank"></a><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank"></a><a href="https://substack.com/profile/27393275-sebastian-raschka-phd" class="inheritColor-WetTGJ">Sebastian Raschka, PhD</a></div><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T reset-IxiVJZ">·</div><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Feb 5</div></div><div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-0 pc-paddingBottom-0 pc-alignItems-center pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" class="pencraft pc-reset align-center-y7ZD4w line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-medium-fw81nC reset-IxiVJZ"><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset link-HREYZo"><span class="pencraft pc-reset color-accent-BVX_7M line-height-20-t4M0El font-text-qe4AeH size-14-MLPa7j weight-semibold-uqA4FV reset-IxiVJZ">Read full story</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></div></a></div></div></div></div><p>In this section, I’ll focus on two key architectural techniques introduced in DeepSeek V3 that improved its computational efficiency and distinguish it from many other LLMs:</p><ul><li><p>Multi-Head Latent Attention (MLA)</p></li><li><p>Mixture-of-Experts (MoE)</p></li></ul><h2 class="header-anchor-post"><strong>1.1 Multi-Head Latent Attention (MLA)</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§multi-head-latent-attention-mla" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/multi-head-latent-attention-mla" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>Before discussing Multi-Head Latent Attention (MLA), let's briefly go over some background to motivate why it's used. For that, let's start with Grouped-Query Attention (GQA), which has become the new standard replacement for a more compute- and parameter-efficient alternative to Multi-Head Attention (MHA) in recent years.</p><p>So, here's a brief GQA summary. Unlike MHA, where each head also has its own set of keys and values, to reduce memory usage, GQA groups multiple heads to share the same key and value projections. </p><p>For example, as further illustrated in Figure 2 below, if there are 2 key-value groups and 4 attention heads, then heads 1 and 2 might share one set of keys and values, while heads 3 and 4 share another. This reduces the total number of key and value computations, which leads to lower memory usage and improved efficiency (without noticeably affecting the modeling performance, according to ablation studies).</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!uVhV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!uVhV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 424w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 848w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1272w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png" width="1023" height="474" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:474,&quot;width&quot;:1023,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!uVhV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 424w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 848w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1272w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 2: A comparison between MHA and GQA. Here, the group size is 2, where a key and value pair is shared among 2 queries.</em></figcaption></figure></div><p></p><p>So, the core idea behind GQA is to reduce the number of key and value heads by sharing them across multiple query heads. This (1) lowers the model's parameter count and (2) reduces the memory bandwidth usage for key and value tensors during inference since fewer keys and values need to be stored and retrieved from the KV cache.</p><p><span>(If you are curious how GQA looks in code, see my</span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" rel=""> GPT-2 to Llama 3 conversion guide</a><span> for a version without KV cache and my KV-cache variant </span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py" rel="">here</a><span>.)</span></p><p><span>While GQA is mainly a computational-efficiency workaround for MHA, ablation studies (such as those in the</span><a href="https://arxiv.org/abs/2305.13245" rel=""> original GQA paper</a><span> and the </span><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2 paper</a><span>) show it performs comparably to standard MHA in terms of LLM modeling performance.</span></p><p>Now, Multi-Head Latent Attention (MLA) offers a different memory-saving strategy that also pairs particularly well with KV caching. Instead of sharing key and value heads like GQA, MLA compresses the key and value tensors into a lower-dimensional space before storing them in the KV cache. </p><p>At inference time, these compressed tensors are projected back to their original size before being used, as shown in the Figure 3 below. This adds an extra matrix multiplication but reduces memory usage.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!jagJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!jagJ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 424w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 848w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1272w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/eb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.jpg" width="1456" height="806" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:806,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!jagJ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 424w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 848w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1272w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 3: Comparison between MLA (used in DeepSeek V3 and R1) and regular MHA.</em></figcaption></figure></div><p></p><p>(As a side note, the queries are also compressed, but only during training, not inference.)</p><p><span>By the way, MLA is not new in DeepSeek V3, as its </span><a href="https://arxiv.org/abs/2405.04434" rel="">DeepSeek-V2 predecessor</a><span> also used (and even introduced) it. Also, the V2 paper contains a few interesting ablation studies that may explain why the DeepSeek team chose MLA over GQA (see Figure 4 below).</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!efDX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!efDX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 424w, https://substackcdn.com/image/fetch/$s_!efDX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 848w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1272w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.jpg" width="644" height="610.4806201550388" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:856,&quot;width&quot;:903,&quot;resizeWidth&quot;:644,&quot;bytes&quot;:288103,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!efDX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 424w, https://substackcdn.com/image/fetch/$s_!efDX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 848w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1272w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 4: Annotated tables from the DeepSeek-V2 paper, https://arxiv.org/abs/2405.04434</figcaption></figure></div><p></p><p></p><p>As shown in Figure 4 above, GQA appears to perform worse than MHA, whereas MLA offers better modeling performance than MHA, which is likely why the DeepSeek team chose MLA over GQA. (It would have been interesting to see the "KV Cache per Token" savings comparison between MLA and GQA as well!)</p><p>To summarize this section before we move on to the next architecture component, MLA is a clever trick to reduce KV cache memory use while even slightly outperforming MHA in terms of modeling performance.</p><h2 class="header-anchor-post"><strong>1.2 Mixture-of-Experts (MoE)</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§mixture-of-experts-moe" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/mixture-of-experts-moe" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>The other major architectural component in DeepSeek worth highlighting is its use of Mixture-of-Experts (MoE) layers. While DeepSeek did not invent MoE, it has seen a resurgence this year, and many of the architectures we will cover later also adopt it.</p><p>You are likely already familiar with MoE, but a quick recap may be helpful.</p><p>The core idea in MoE is to replace each FeedForward module in a transformer block with multiple expert layers, where each of these expert layers is also a FeedForward module. This means that we swap a single FeedForward block for multiple FeedForward blocks, as illustrated in the Figure 5 below.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!d_xI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!d_xI!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png 424w, https://substackcdn.com/image/fetch/$s_!d_xI!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png 848w, https://substackcdn.com/image/fetch/$s_!d_xI!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png 1272w, https://substackcdn.com/image/fetch/$s_!d_xI!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png" width="1304" height="822" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:822,&quot;width&quot;:1304,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:200948,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!d_xI!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png 424w, https://substackcdn.com/image/fetch/$s_!d_xI!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png 848w, https://substackcdn.com/image/fetch/$s_!d_xI!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png 1272w, https://substackcdn.com/image/fetch/$s_!d_xI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F632d3212-432a-4d43-b271-f2269be1d8ec_1304x822.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 5: An illustration of the Mixture-of-Experts (MoE) module in DeepSeek V3/R1 (right) compared to an LLM with a standard FeedForward block (left).</figcaption></figure></div><p></p><p>The FeedForward block inside a transformer block (shown as the dark gray block in the figure above) typically contains a large number of the model's total parameters. (Note that the transformer block, and thereby the FeedForward block, is repeated many times in an LLM; in the case of DeepSeek-V3, 61 times.)</p><p><span>So, replacing </span><em>a single</em><span> FeedForward block with </span><em>multiple</em><span> FeedForward blocks (as done in a MoE setup) substantially increases the model's total parameter count. However, the key trick is that we don't use ("activate") all experts for every token. Instead, a router selects only a small subset of experts per token. (In the interest of time, or rather article space, I'll cover the router in more detail another time.)</span></p><p><span>Because only a few experts are active at a time, MoE modules are often referred to as </span><em>sparse</em><span>, in contrast to </span><em>dense</em><span> modules that always use the full parameter set. However, the large total number of parameters via an MoE increases the capacity of the LLM, which means it can take up more knowledge during training. The sparsity keeps inference efficient, though, as we don't use all the parameters at the same time.</span></p><p>For example, DeepSeek-V3 has 256 experts per MoE module and a total of 671 billion parameters. Yet during inference, only 9 experts are active at a time (1 shared expert plus 8 selected by the router). This means just 37 billion parameters are used per inference step as opposed to all 671 billion.</p><p><span>One notable feature of DeepSeek-V3's MoE design is the use of a shared expert. This is an expert that is always active for every token. This idea is not new and was already introduced in the </span><a href="https://arxiv.org/abs/2401.06066" rel="">DeepSeek 2024 MoE</a><span> and </span><a href="https://arxiv.org/abs/2201.05596" rel="">2022 DeepSpeedMoE paper</a><span>s. </span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!i4ms!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i4ms!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 424w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 848w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1272w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.jpg" width="1039" height="569" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:569,&quot;width&quot;:1039,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!i4ms!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 424w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 848w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1272w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 6: An annotated figure from "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", https://arxiv.org/abs/2401.06066</em></figcaption></figure></div><p><span>The benefit of having a shared expert was first noted in the </span><a href="https://arxiv.org/abs/2201.05596" rel="">DeepSpeedMoE paper</a><span>, where they found that it boosts overall modeling performance compared to no shared experts. This is likely because common or repeated patterns don't have to be learned by multiple individual experts, which leaves them with more room for learning more specialized patterns.</span></p><h2 class="header-anchor-post"><strong>1.3 DeepSeek Summary</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§deepseek-summary" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/deepseek-summary" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>To summarize, DeepSeek-V3 is a massive 671-billion-parameter model that, at launch, outperformed other open-weight models, including the 405B Llama 3. Despite being larger, it is much more efficient at inference time thanks to its Mixture-of-Experts (MoE) architecture, which activates only a small subset of (just 37B) parameters per token.</p><p>Another key distinguishing feature is DeepSeek-V3's use of Multi-Head Latent Attention (MLA) instead of Grouped-Query Attention (GQA). Both MLA and GQA are inference-efficient alternatives to standard Multi-Head Attention (MHA), particularly when using KV caching. While MLA is more complex to implement, a study in the DeepSeek-V2 paper has shown it delivers better modeling performance than GQA.</p><h1 class="header-anchor-post">2. OLMo 2<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§olmo" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/olmo" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p>The OLMo series of models by the non-profit Allen Institute for AI is noteworthy due to its transparency in terms of training data and code, as well as the relatively detailed technical reports.</p><p>While you probably won’t find OLMo models at the top of any benchmark or leaderboard, they are pretty clean and, more importantly, a great blueprint for developing LLMs, thanks to their transparency.</p><p><span>And while OLMo models are popular because of their transparency, they are not that bad either. In fact, at the time of release in January (before Llama 4, Gemma 3, and Qwen 3), </span><a href="https://arxiv.org/abs/2501.00656" rel="">OLMo 2</a><span> models were sitting at the Pareto frontier of compute to performance, as shown in Figure 7 below.</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!7DYj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!7DYj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 424w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 848w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1272w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/bb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.jpg" width="666" height="533.7078870496592" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:823,&quot;width&quot;:1027,&quot;resizeWidth&quot;:666,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!7DYj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 424w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 848w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1272w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 7: Modeling benchmark performance (higher is better) vs pre-training cost (FLOPs; lower is better) for different LLMs. This is an annotated figure from the OLMo 2 paper, https://arxiv.org/abs/2501.00656</figcaption></figure></div><p></p><p>As mentioned earlier in this article, I aim to focus only on the LLM architecture details (not training or data) to keep it at a manageable length. So, what were the interesting architectural design choices in OLMo2 ? It mainly comes down to normalizations: the placement of RMSNorm layers as well as the addition of a QK-norm, which I will discuss below.</p><p>Another thing worth mentioning is that OLMo 2 still uses traditional Multi-Head Attention (MHA) instead of MLA or GQA.</p><h2 class="header-anchor-post"><strong>2.1 Normalization Layer Placement</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§normalization-layer-placement" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/normalization-layer-placement" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>Overall, OLMo 2 largely follows the architecture of the original GPT model,  similar to other contemporary LLMs. However, there are some noteworthy deviations. Let's start with the normalization layers.</p><p>Similar to Llama, Gemma, and most other LLMs, OLMo 2 switched from LayerNorm to RMSNorm.</p><p><span>But since RMSNorm is old hat (it's basically a simplified version of LayerNorm with fewer trainable parameters), I will skip the discussion of RMSNorm vs LayerNorm. (Curious readers can find an RMSNorm code implementation in my </span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb" rel="">GPT-2 to Llama conversion guide</a><span>.)</span></p><p><span>However, it's worth discussing the placement of the RMSNorm layer. The original transformer (from the "</span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention is all you need</a><span>" paper) placed the two normalization layers in the transformer block </span><em>after</em><strong> </strong><span>the attention module and the FeedForward module, respectively.</span></p><p>This is also known as Post-LN or Post-Norm.</p><p><span>GPT and most other LLMs that came after placed the normalization layers </span><em>before</em><span> the attention and FeedForward modules, which is known as Pre-LN or Pre-Norm. A comparison between Post- and Pre-Norm is shown in the figure below.</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wYj9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wYj9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 424w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 848w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1272w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png" width="1444" height="789" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:789,&quot;width&quot;:1444,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wYj9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 424w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 848w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1272w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 8: A comparison of Post-Norm, Pre-Norm, and OLMo 2's flavor of Post-Norm.</em></figcaption></figure></div><p></p><p><span>In </span><a href="https://arxiv.org/abs/2002.04745" rel="">2020, Xiong et al.</a><span> showed that Pre-LN results in more well-behaved gradients at initialization. Furthermore, the researchers mentioned that Pre-LN even works well without careful learning rate warm-up, which is otherwise a crucial tool for Post-LN.</span></p><p><span>Now, the reason I am mentioning that is that OLMo 2 adopted a form of Post-LN (but with RMSNorm instead of LayerNorm, so I am calling it </span><em>Post-Norm</em><span>).</span></p><p>In OLMo 2, instead of placing the normalization layers before the attention and FeedForward layers, they place them after, as shown in the figure above. However, notice that in contrast to the original transformer architecture, the normalization layers are still inside the residual layers (skip connections).</p><p><span>So, why did they move the position of the normalization layers?</span><strong> </strong><span>The reason is that it helped with training stability, as shown in the figure below.</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ebW0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ebW0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 424w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 848w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1272w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.jpg" width="1289" height="407" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:407,&quot;width&quot;:1289,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ebW0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 424w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 848w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1272w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 9: A plot showing the training stability for Pre-Norm (like in GPT-2, Llama 3, and many others) versus OLMo 2's flavor of Post-Norm. This is an annotated figure from the OLMo 2 paper, https://arxiv.org/abs/2501.00656</em></figcaption></figure></div><p></p><p>Unfortunately this figure shows the results of the reordering together with QK-Norm, which is a separate concept. So, it’s hard to tell how much the normalization layer reordering contributed by itself.</p><h2 class="header-anchor-post"><strong>2.2 QK-Norm</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§qk-norm" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qk-norm" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>Since the previous section already mentioned the QK-norm, and other LLMs we discuss later, such as Gemma 2 and Gemma 3, also use QK-norm, let's briefly discuss what this is.</p><p><span>QK-Norm is essentially yet another RMSNorm layer. It's placed inside the Multi-Head Attention (MHA) module and applied to the queries (q) and keys (k) before applying RoPE. To illustrate this, below is an excerpt of a Grouped-Query Attention (GQA) layer I wrote for my </span><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3" rel="">Qwen3 from-scratch implementation</a><span> (the QK-norm application in GQA is similar to MHA in OLMo):</span></p><pre><code>class GroupedQueryAttention(nn.Module):
    def __init__(
        self, d_in, num_heads, num_kv_groups,
        head_dim=None, qk_norm=False, dtype=None
    ):
        # ...

        if qk_norm:
            self.q_norm = RMSNorm(head_dim, eps=1e-6)
            self.k_norm = RMSNorm(head_dim, eps=1e-6)
        else:
            self.q_norm = self.k_norm = None

    def forward(self, x, mask, cos, sin):
        b, num_tokens, _ = x.shape

        # Apply projections
        queries = self.W_query(x) 
        keys = self.W_key(x)
        values = self.W_value(x) 

        # ...

        # Optional normalization
        if self.q_norm:
            queries = self.q_norm(queries)
        if self.k_norm:
            keys = self.k_norm(keys)

        # Apply RoPE
        queries = apply_rope(queries, cos, sin)
        keys = apply_rope(keys, cos, sin)

        # Expand K and V to match number of heads
        keys = keys.repeat_interleave(self.group_size, dim=1)
        values = values.repeat_interleave(self.group_size, dim=1)

        # Attention
        attn_scores = queries @ keys.transpose(2, 3)
        # ...
</code></pre><p><span>As mentioned earlier, together with Post-Norm, QK-Norm stabilizes the training. Note that QK-Norm was not invented by OLMo 2 but goes back to the </span><a href="https://arxiv.org/abs/2302.05442" rel="">2023 Scaling Vision Transformers paper</a><span>.</span></p><h2 class="header-anchor-post"><strong>2.3 OLMo 2 Summary</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§olmo-summary" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/olmo-summary" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>In short, the noteworthy OLMo 2 architecture design decisions are primarily the RMSNorm placements: RMSNorm after instead of before the attention and FeedForward modules (a flavor of Post-Norm), as well as the addition of RMSNorm for the queries and keys inside the attention mechanism (QK-Norm), which both, together, help stabilize the training loss.</p><p><span>Below is a figure that further compares OLMo 2 to Llama 3 side by side; as one can see, the architectures are otherwise relatively similar except for the fact that OLMo 2 still uses the traditional MHA instead of GQA. (However, the </span><a href="https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct" rel="">OLMo 2 team released a 32B variant</a><span> 3 months later that uses GQA.)</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!S6Y9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!S6Y9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 424w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 848w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1272w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png" width="1329" height="737" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:737,&quot;width&quot;:1329,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:153520,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!S6Y9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 424w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 848w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1272w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 10: An architecture comparison between Llama 3 and OLMo 2.</figcaption></figure></div><p></p><p></p><p></p><h1 class="header-anchor-post">3. Gemma 3<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§gemma" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/gemma" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p>Google's Gemma models have always been really good, and I think they have always been a bit underhyped compared to other popular models, like the Llama series. </p><p>One of the distinguishing aspects of Gemma is the rather large vocabulary size (to support multiple languages better), and the stronger focus on the 27B size (versus 8B or 70B). But note that Gemma 2 also comes in smaller sizes: 1B, 4B, and 12B.</p><p>The 27B size hits a really nice sweet spot: it's much more capable than an 8B model but not as resource-intensive as a 70B model, and it runs just fine locally on my Mac Mini.</p><p><span>So, what else is interesting in </span><a href="https://arxiv.org/abs/2503.19786" rel="">Gemma 3</a><span>? As discussed earlier, other models like Deepseek-V3/R1 use a Mixture-of-Experts (MoE) architecture to reduce memory requirements at inference, given a fixed model size. (The MoE approach is also used by several other models we will discuss later.)</span></p><p>Gemma 3 uses a different "trick" to reduce computational costs, namely sliding window attention.</p><h2 class="header-anchor-post"><strong>3.1 Sliding Window Attention</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§sliding-window-attention" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/sliding-window-attention" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p><span>With sliding window attention (originally introduced in the </span><a href="https://arxiv.org/abs/2004.05150" rel="">LongFormer paper in 2020</a><span> and also already used by </span><a href="http://arxiv.org/abs/2408.00118" rel="">Gemma 2</a><span>), the Gemma 3 team was able to reduce the memory requirements in the KV cache by a substantial amount, as shown in the figure below.</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LQA4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LQA4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 424w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 848w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1272w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/b5363ce6-0ec8-49e6-b296-9836c248e159_665x302.jpg" width="555" height="252.04511278195488" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:302,&quot;width&quot;:665,&quot;resizeWidth&quot;:555,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LQA4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 424w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 848w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1272w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 11: An annotated figure from Gemma 3 paper (https://arxiv.org/abs/2503.19786) showing the KV cache memory savings via sliding window attention.</em></figcaption></figure></div><p></p><p><span>So, what is sliding window attention? If we think of regular self-attention as a </span><em>global</em><span> attention mechanism, since each sequence element can access every other sequence element, then we can think of sliding window attention as </span><em>local</em><span> attention, because here we restrict the context size around the current query position. This is illustrated in the figure below.</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!tTJ5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!tTJ5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 424w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 848w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1272w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/f32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.jpg" width="1456" height="721" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!tTJ5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 424w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 848w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1272w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 12: A comparison between regular attention (left) and sliding window attention (right).</em></figcaption></figure></div><p></p><p>Please note that sliding window attention can be used with both Multi-Head Attention and Grouped-Query Attention; Gemma 3 uses grouped-query attention.</p><p><span>As mentioned above, sliding window attention is also referred to as </span><em>local</em><span> attention because the local window surrounds and moves with the current query position. In contrast, regular attention is </span><em>global</em><span> as each token can access all other tokens.</span></p><p>Now, as briefly mentioned above, the Gemma 2 predecessor architecture also used sliding window attention before. The difference in Gemma 3 is that they adjusted the ratio between global (regular) and local (sliding) attention.</p><p>For instance, Gemma 2 uses a hybrid attention mechanism that combines sliding window (local) and global attention in a 1:1 ratio. Each token can attend to a 4k-token window of nearby context.</p><p>Where Gemma 2 used sliding window attention in every other layer, Gemma 3 now has a 5:1 ratio, meaning there's only 1 full attention layer for every 5 sliding windows (local) attention layers; moreover, the sliding window size was reduced from 4096 (Gemma 2) to just 1024 (Gemma 3). This shifts the model's focus towards more efficient, localized computations.</p><p>According to their ablation study, the use of sliding window attention has minimal impact on modeling performance, as shown in the figure below.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!YSZb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!YSZb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 424w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 848w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1272w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png" width="1456" height="434" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!YSZb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 424w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 848w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1272w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 13: An annotated figure from Gemma 3 paper (https://arxiv.org/abs/2503.19786) showing that sliding window attention has little to no impact on the LLM-generated output perplexity.</figcaption></figure></div><p></p><p>While sliding window attention is the most notable architecture aspect of Gemma 3, I want to also briefly go over the placement of the normalization layers as a follow-up to the previous OLMo 2 section.</p><h2 class="header-anchor-post"><strong>3.2 Normalization Layer Placement in Gemma 3</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§normalization-layer-placement-in-gemma" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/normalization-layer-placement-in-gemma" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>A small but interesting tidbit to highlight is that Gemma 3 uses RMSNorm in both a Pre-Norm and Post-Norm setting around its grouped-query attention module.</p><p>This is similar to Gemma 2 but still worth highlighting, as it differs from (1) the Post-Norm used in the original transformer (“Attention is all you need”), (2) the Pre-Norm, which was popularized by GPT-2 and used in many other architectures afterwards, and (3) the Post-Norm flavor in OLMo 2 that we saw earlier.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!A1BM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!A1BM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 424w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 848w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1272w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png" width="1068" height="855" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:855,&quot;width&quot;:1068,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!A1BM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 424w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 848w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1272w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 14: An architecture comparison between OLMo2 and Gemma 3; note the additional normalization layers in Gemma 3.</em></figcaption></figure></div><p></p><p>I think this normalization layer placement is a relatively intuitive approach as it gets the best of both worlds: Pre-Norm and Post-Norm. In my opinion, a bit of extra normalization can't hurt. In the worst case, if the extra normalization is redundant, this adds a bit of inefficiency through redundancy. In practice, since RMSNorm is relatively cheap in the grand scheme of things, this shouldn't have any noticeable impact, though.</p><h2 class="header-anchor-post"><strong>3.3 Gemma 3 Summary</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§gemma-summary" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/gemma-summary" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>Gemma 3 is a well-performing open-weight LLM that, in my opinion, is a bit underappreciated in the open-source circles. The most interesting part is the use of sliding window attention to improve efficiency (it will be interesting to combine it with MoE in the future).</p><p>Also, Gemma 3 has a unique normalization layer placement, placing RMSNorm layers both before and after the attention and FeedForward modules.</p><h2 class="header-anchor-post"><strong>3.4 Bonus: Gemma 3n</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§bonus-gemma-n" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/bonus-gemma-n" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p><span>A few months after the Gemma 3 release, Google shared </span><a href="https://developers.googleblog.com/en/introducing-gemma-3n/" rel="">Gemma 3n</a><span>, which is a Gemma 3 model that has been optimized for small-device efficiency with the goal of running on phones.</span></p><p>One of the changes in Gemma 3n to achieve better efficiency is the so-called Per-Layer Embedding (PLE) parameters layer. The key idea here is to keep only a subset of the model's parameters in GPU memory. Token-layer specific embeddings, such as those for text, audio, and vision modalities, are then streamed from the CPU or SSD on demand.</p><p>The figure below illustrates the PLE memory savings, listing 5.44 billion parameters for a standard Gemma 3 model. This likely refers to the Gemma 3 4-billion variant.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Su7d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Su7d!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 424w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 848w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1272w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/b05999d6-88ca-4739-8b0b-266b48da288b_662x483.png" width="606" height="442.1419939577039" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b05999d6-88ca-4739-8b0b-266b48da288b_662x483.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:662,&quot;resizeWidth&quot;:606,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Su7d!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 424w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 848w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1272w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 15: An annotated figure from Google's Gemma 3n blog (https://developers.googleblog.com/en/introducing-gemma-3n/) illustrating the PLE memory savings.</figcaption></figure></div><p>The 5.44 vs. 4 billion parameter discrepancy is because Google has an interesting way of reporting parameter counts in LLMs. They often exclude embedding parameters to make the model appear smaller, except in cases like this, where it is convenient to include them to make the model appear larger. This is not unique to Google, as this approach has become a common practice across the field.</p><p><span>Another interesting trick is the</span><a href="https://arxiv.org/abs/2310.07707" rel=""> MatFormer</a><span> concept (short for Matryoshka Transformer). For instance, Gemma 3n uses a single shared LLM (transformer) architecture that can be sliced into smaller, independently usable models. Each slice is trained to function on its own, so at inference time, we can run just the part you need (instead of the large model).</span></p><h1 class="header-anchor-post">4. Mistral Small 3.1<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§mistral-small" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/mistral-small" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p><a href="https://mistral.ai/news/mistral-small-3-1" rel="">Mistral Small 3.1 24B</a><span>, which was released in March shortly after Gemma 3, is noteworthy for outperforming Gemma 3 27B on several benchmarks (except for math) while being faster.</span></p><p>The reasons for the lower inference latency of Mistral Small 3.1 over Gemma 3 are likely due to their custom tokenizer, as well as shrinking the KV cache and layer count. Otherwise, it's a standard architecture as shown in the figure below.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!SdCR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!SdCR!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png 424w, https://substackcdn.com/image/fetch/$s_!SdCR!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png 848w, https://substackcdn.com/image/fetch/$s_!SdCR!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png 1272w, https://substackcdn.com/image/fetch/$s_!SdCR!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png" width="1402" height="757" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:757,&quot;width&quot;:1402,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:182299,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!SdCR!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png 424w, https://substackcdn.com/image/fetch/$s_!SdCR!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png 848w, https://substackcdn.com/image/fetch/$s_!SdCR!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png 1272w, https://substackcdn.com/image/fetch/$s_!SdCR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5140cd93-4e54-4a3d-af74-f8d1b8ea1647_1402x757.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 16: An architecture comparison between Gemma 3 27B and Mistral 3.1 Small 24B.</figcaption></figure></div><p>Interestingly, earlier Mistral models had utilized sliding window attention, but they appear to have abandoned it in Mistral Small 3.1. So, since Mistral uses regular Grouped-Query Attention instead of Grouped-Query Attention with a sliding window as in Gemma 3, maybe there are additional inference compute savings due to being able to use more optimized code (i.e., FlashAttention). For instance, I speculate that while sliding window attention reduces memory usage, it doesn't necessarily reduce inference latency, which is what Mistral Small 3.1 is focused on.</p><h1 class="header-anchor-post">5. Llama 4<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§llama" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/llama" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p><span>The extensive introductory discussion on Mixture-of-Experts (MoE) earlier in this article pays off again. </span><a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" rel="">Llama 4</a><span> has also adopted an MoE approach and otherwise follows a relatively standard architecture that is very similar to DeepSeek-V3, as shown in the figure below. (Llama 4 includes native multimodal support, similar to models like Gemma and Mistral. However, since this article focuses on language modeling, we only focus on the text model.)</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ShdO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ShdO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 424w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 848w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1272w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/17518ff9-1f60-4aca-b654-034dabe20626_1600x823.jpg" width="1456" height="749" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:749,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ShdO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 424w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 848w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1272w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 17: An architecture comparison between DeepSeek V3 (671-billion parameters) and Llama 4 Maverick (400-billion parameters).</figcaption></figure></div><p></p><p>While the Llama 4 Maverick architecture looks very similar to DeepSeek-V3 overall, there are some interesting differences worth highlighting.</p><p>First, Llama 4 uses Grouped-Query Attention similar to its predecessors, whereas DeepSeek-V3 uses Multi-Head Latent Attention, which we discussed at the beginning of this article. Now, both DeepSeek-V3 and Llama 4 Maverick are very large architectures, with DeepSeek-V3 being approximately 68% larger in its total parameter count. However, with 37 billion active parameters, DeepSeek-V3 has more than twice as many active parameters as Llama 4 Maverick (17B).</p><p>Llama 4 Maverick uses a more classic MoE setup with fewer but larger experts (2 active experts with 8,192 hidden size each) compared to DeepSeek-V3 (9 active experts with 2,048 hidden size each). Also, DeepSeek uses MoE layers in each transformer block (except the first 3), whereas Llama 4 alternates MoE and dense modules in every other transformer block.</p><p>Given the many small differences between architectures, it is difficult to determine their exact impact on final model performance. The main takeaway, however, is that MoE architectures have seen a significant rise in popularity in 2025.</p><h1 class="header-anchor-post">6. Qwen3<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§qwen" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qwen" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p>The Qwen team consistently delivers high-quality open-weight LLMs. When I helped co-advising the LLM efficiency challenge at NeurIPS 2023, I remember that the top winning solutions were all Qwen2-based.</p><p>Now, Qwen3 is another hit model series at the top of the leaderboards for their size classes. There are 7 dense models: 0.6B, 1.7B, 4B, 8B, 14B, and 32B. And there are 2 MoE models: 30B-A3B, and 235B-A22B.</p><p>(By the way, note that the missing whitespace in "Qwen3" is not a typo; I simply try to preserve the original spelling the Qwen developers chose.)</p><h2 class="header-anchor-post"><strong>6.1 Qwen3 (Dense)</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§qwen-dense" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qwen-dense" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>Let's discuss the dense model architecture first. As of this writing, the 0.6B model may well be the smallest current-generation open-weight model out there. And based on my personal experience, it performs really well given its small size. It has great token/sec throughput and a low memory footprint if you are planning to run it locally. But what's more, it's also easy to train locally (for educational purposes) due to its small size.</p><p>So, Qwen3 0.6B has replaced Llama 3 1B for me for most purposes. A comparison between these two architectures is shown below.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!pu4b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!pu4b!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png 424w, https://substackcdn.com/image/fetch/$s_!pu4b!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png 848w, https://substackcdn.com/image/fetch/$s_!pu4b!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png 1272w, https://substackcdn.com/image/fetch/$s_!pu4b!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.jpg" width="1456" height="919" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:919,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:223406,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!pu4b!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png 424w, https://substackcdn.com/image/fetch/$s_!pu4b!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png 848w, https://substackcdn.com/image/fetch/$s_!pu4b!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png 1272w, https://substackcdn.com/image/fetch/$s_!pu4b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d542035-0713-4c71-8d6d-68e0e37bf0c7_1466x925.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 18: An architecture comparison between Qwen3 0.6B and Llama 3 1B; notice that Qwen3 is a deeper architecture with more layers, whereas Llama 3 is a wider architecture with more attention heads.</figcaption></figure></div><p></p><p></p><p><span>If you are interested in a human-readable Qwen3 implementation without external third-party LLM library dependencies, I recently implemented </span><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3" rel="">Qwen3 from scratch (in pure PyTorch)</a><span>.</span></p><p>The computational performance numbers in the figure above are based on my from-scratch PyTorch implementations when run on an A100 GPU. As one can see, Qwen3 has a smaller memory footprint as it is a smaller architecture overall, but also uses smaller hidden layers and fewer attention heads. However, it uses more transformer blocks than Llama 3, which leads to a slower runtime (lower tokens/sec generation speed).</p><h2 class="header-anchor-post"><strong>6.2 Qwen3 (MoE)</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§qwen-moe" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qwen-moe" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>As mentioned earlier, Qwen3 also comes in two MoE flavors: 30B-A3B and 235B-A22B. Why do some architectures, like Qwen3, come as regular (dense) and MoE (sparse) variants?</p><p>As mentioned at the beginning of this article, MoE variants help reduce inference costs for large base models. Offering both dense and MoE versions gives users flexibility depending on their goals and constraints.</p><p>Dense models are typically more straightforward to fine-tune, deploy, and optimize across various hardware.</p><p>On the other hand, MoE models are optimized for scaling inference. For instance, at a fixed inference budget, they can achieve a higher overall model capacity (i.e., knowledge uptake during training due to being larger) without proportionally increasing inference costs.</p><p>By releasing both types, the Qwen3 series can support a broader range of use cases: dense models for robustness, simplicity, and fine-tuning, and MoE models for efficient serving at scale.</p><p>To round up this section, let's look at Qwen3 235B-A22B (note that the A22B stands for "22B active parameters) to DeepSeek-V3, which has almost twice as many active parameters (37B).</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!6Cx4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6Cx4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png 424w, https://substackcdn.com/image/fetch/$s_!6Cx4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png 848w, https://substackcdn.com/image/fetch/$s_!6Cx4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png 1272w, https://substackcdn.com/image/fetch/$s_!6Cx4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.jpg" width="1456" height="723" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:723,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:285792,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6Cx4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png 424w, https://substackcdn.com/image/fetch/$s_!6Cx4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png 848w, https://substackcdn.com/image/fetch/$s_!6Cx4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png 1272w, https://substackcdn.com/image/fetch/$s_!6Cx4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4627dac1-ced7-4e8d-8de4-d9238b1c427d_1632x810.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 19: An architecture comparison between DeepSeek-V3 and Qwen3 235B-A22B.</figcaption></figure></div><p></p><p></p><p></p><p><span>As shown in the figure above, the DeepSeek-V3 and Qwen3 235B-A22B architectures are remarkably similar. What's noteworthy, though, is that the Qwen3 model moved away from using a shared expert (earlier Qwen models, such as </span><a href="https://qwenlm.github.io/blog/qwen2.5-max/" rel="">Qwen2.5-MoE</a><span> did use a shared expert).</span></p><p>Unfortunately, the Qwen3 team did not disclose any reason as to why they moved away from shared experts. If I had to guess, it was perhaps simply not necessary for training stability for their setup when they increased the experts from 2 (in Qwen2.5-MoE) to 8 (in Qwen3). And then they were able to save the extra compute/memory cost by using only 8 instead of 8+1 experts. (However, this doesn't explain why DeepSeek-V3 is still keeping their shared expert.)</p><p><strong>Update.</strong><span> </span><a href="https://x.com/JustinLin610/status/1947364862184853626" rel="">Junyang Lin</a><span>, one of the developers of Qwen3, responded as follows:</span></p><blockquote><p>At that moment we did not find significant enough improvement on shared expert and we were worrying about the optimization for inference caused by shared expert. No straight answer to this question honestly.</p></blockquote><h1 class="header-anchor-post">7. SmolLM3<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§smollm" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/smollm" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p><a href="https://huggingface.co/blog/smollm3" rel="">SmolLM3</a><span> is perhaps not as nearly as popular as the other LLMs covered in this article, but I thought it is still an interesting model to include as it offers really good modeling performance at a relatively small and convenient 3-billion parameter model size that sits between the 1.7B and 4B Qwen3 model, as shown in the figure below.</span></p><p>Moreover, it also shared a lot of the training details, similar to OLMo, which is rare and always appreciated!</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!vPTQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!vPTQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 424w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 848w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1272w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/ebfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.jpg" width="592" height="413.5235531628533" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ebfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:519,&quot;width&quot;:743,&quot;resizeWidth&quot;:592,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!vPTQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 424w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 848w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1272w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>Figure 20: An annotated figure from the SmolLM3 announcement post, https://huggingface.co/blog/smollm3, comparing the SmolLM3 win rate to Qwen3 1.7B and 4B as well as Llama 3 3B and Gemma 3 4B.</em></figcaption></figure></div><p></p><p>As shown in the architecture comparison figure below, the SmolLM3 architecture looks fairly standard. The perhaps most interesting aspect is its use of NoPE (No Positional Embeddings), though.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!oGOS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!oGOS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png 424w, https://substackcdn.com/image/fetch/$s_!oGOS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png 848w, https://substackcdn.com/image/fetch/$s_!oGOS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png 1272w, https://substackcdn.com/image/fetch/$s_!oGOS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/f761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png" width="1447" height="770" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:770,&quot;width&quot;:1447,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:178756,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!oGOS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png 424w, https://substackcdn.com/image/fetch/$s_!oGOS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png 848w, https://substackcdn.com/image/fetch/$s_!oGOS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png 1272w, https://substackcdn.com/image/fetch/$s_!oGOS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff761a811-e394-4ea1-bb80-fbed44f48d89_1447x770.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 21: A side-by-side architecture comparison between Qwen3 4B and SmolLM3 3B.</figcaption></figure></div><p></p><h2 class="header-anchor-post"><strong>7.1 No Positional Embeddings (NoPE)</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§no-positional-embeddings-nope" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/no-positional-embeddings-nope" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p><span>NoPE is, in LLM contexts, an older idea that goes back to a 2023 paper (</span><a href="https://arxiv.org/abs/2305.19466" rel="">The Impact of Positional Encoding on Length Generalization in Transformers</a><span>) to remove explicit positional information injection (like through classic absolute positional embedding layers in early GPT architectures or nowadays RoPE).</span></p><p>In transformer-based LLMs, positional encoding is typically necessary because self-attention treats tokens independently of order. Absolute position embeddings solve this by adding an additional embedding layer that adds information to the token embeddings.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!lLgK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lLgK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 424w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 848w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1272w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/d79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.jpg" width="1190" height="548" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1190,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!lLgK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 424w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 848w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1272w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 22: A modified figure from my Build A Large Language Model (From Scratch) book (https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167) illustrating absolute positional embeddings.</figcaption></figure></div><p>RoPE, on the other hand, solves this by rotating the query and key vectors relative to their token position.</p><p>In NoPE layers, however, no such positional signal is added at all: not fixed, not learned, not relative. Nothing.</p><p><span>Even though there is no positional embedding, the model still knows which tokens come before, thanks to the causal attention mask. This mask prevents each token from attending to future ones. As a result, a token at position </span><em>t</em><span> can only see tokens at positions </span><em>≤ t</em><span>, which preserves the autoregressive ordering.</span></p><p>So while there is no positional information that is explicitly added, there is still an implicit sense of direction baked into the model's structure, and the LLM, in the regular gradient-descent-based training, can learn to exploit it if it finds it beneficial for the optimization objective. (Check out the NoPE paper's theorems for more information.)</p><p><span>So, overall, the </span><a href="https://arxiv.org/abs/2305.19466" rel="">NoPE paper</a><span> not only found that no positional information injection is necessary, but it also found that NoPE has better length generalization, which means that LLM answering performance deteriorates less with increased sequence length, as shown in the figure below.</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!I9j6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!I9j6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 424w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 848w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1272w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/d7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.jpg" width="1364" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!I9j6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 424w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 848w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1272w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 23: An annotated figure from the NoPE paper (https://arxiv.org/abs/2305.19466) showing better length generalization with NoPE.</figcaption></figure></div><p></p><p>Note that the experiments shown above were conducted with a relatively small GPT-style model of approximately 100 million parameters and relatively small context sizes. It is unclear how well these findings generalize to larger, contemporary LLMs.</p><p>For this reason, the SmolLM3 team likely only "applied" NoPE (or rather omitted RoPE) in every 4th layer.</p><h1 class="header-anchor-post">8. Kimi 2<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§kimi" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/kimi" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p><a href="https://moonshotai.github.io/Kimi-K2/" rel="">Kimi 2</a><span> recently made big waves in the AI community due to being an open-weight model with an incredibly good performance. According to benchmarks, it's on par with the best proprietary models like Google's Gemini, Anthropic's Claude, and OpenAI's ChatGPT models.</span></p><p><span>A notable aspect is its use of a variant of the relatively new</span><a href="https://github.com/KellerJordan/Muon" rel=""> Muon</a><span> optimizer over AdamW. As far as I know, this is the first time Muon was used over AdamW for any production model of this size (</span><a href="https://arxiv.org/abs/2502.16982" rel="">previously</a><span>, it has only been shown to scale up to 16B). This resulted in very nice training loss curves, which probably helped catapult this model to the top of the aforementioned benchmarks.</span></p><p>While people commented that the loss was exceptionally smooth (due to the lack of spikes), I think it's not exceptionally smooth (e.g., see the OLMo 2 loss curve in the figure below; also, the L2 norm of the gradient would probably be a better metric to track training stability). However, what's remarkable is how well the loss curve decays.</p><p>However, as mentioned in the introduction of this article, training methodologies are a topic for another time.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_Zh8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_Zh8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 424w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 848w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1272w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.jpg" width="612" height="551.5256916996047" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:759,&quot;resizeWidth&quot;:612,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_Zh8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 424w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 848w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1272w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 24: Annotated figures from the Kimi K2 announcement blog article (https://moonshotai.github.io/Kimi-K2/) and the OLMo 2 paper (https://arxiv.org/abs/2305.19466).</figcaption></figure></div><p>The model itself is 1 trillion parameters large, which is truly impressive.</p><p><span>It may be the biggest LLM of this generation as of this writing (given the constraints that Llama 4 Behemoth is not released, proprietary LLMs don't count, and Google's 1.6 trillion </span><a href="https://arxiv.org/abs/2101.03961" rel="">Switch Transformer</a><span> is an encoder-decoder architecture from a different generation).</span></p><p>It's also coming full circle as Kimi 2 uses the DeepSeek-V3 architecture we covered at the beginning of this article except they made it larger, as shown in the figure below.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!B3em!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!B3em!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 424w, https://substackcdn.com/image/fetch/$s_!B3em!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 848w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1272w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/b721c5ef-057b-405b-9293-f11e161d9230_1599x816.jpg" width="1456" height="743" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b721c5ef-057b-405b-9293-f11e161d9230_1599x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:743,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!B3em!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 424w, https://substackcdn.com/image/fetch/$s_!B3em!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 848w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1272w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 25: An architecture comparison between DeepSeek V3 and Kimi K2.</figcaption></figure></div><p></p><p>As shown in the figure above, Kimi 2.5 is basically the same as DeepSeek V3, except that it uses more experts in the MoE modules and fewer heads in the Multi-head Latent Attention (MLA) module.</p><p><span>Kimi 2 is not coming out of nowhere. The earlier Kimi 1.5 model discussed in the </span><a href="https://arxiv.org/abs/2501.12599" rel="">Kimi k1.5: Scaling Reinforcement Learning with LLMs paper</a><span>, was impressive as well. However, it had the bad luck that the DeepSeek R1 model paper was published on exactly the same date on January 22nd. Moreover, as far as I know, the Kimi 1.5 weights were never publicly shared.</span></p><p>So, most likely the Kimi K2 team took these lessons to heart and shared Kimi K2 as an open-weight model, before DeepSeek R2 was released. As of this writing, Kimi K2 is the most impressive open-weight model.</p><h1 class="header-anchor-post">9. GPT-OSS<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§gpt-oss" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/gpt-oss" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p><span>OpenAI’s </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">released</a><span> gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019, about one week after I wrote this article. Since OpenAI’s open-weight models have been so widely anticipated, I updated this article to include them. I will keep this section brief, but I have written another, much more detailed article dedicated to the gpt-oss models here:</span></p><div data-component-name="DigestPostEmbed" class="digestPostEmbed-flwiST"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"></a><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"><div class="pencraft pc-reset" style="width: 70px; height: 70px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kftt!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529c4cc7-161c-4d7c-b186-06e68c771776_1564x926.png"><img src="./The Big LLM Architecture Comparison_files/529c4cc7-161c-4d7c-b186-06e68c771776_1564x926.jpg" sizes="100vw" alt="From GPT-2 to gpt-oss: Analyzing the Architectural Advances" width="140" height="140" class="img-OACg1c smSquare-NGbPBa pencraft pc-reset"></picture></div></a><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"><h4 class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-24-jnGwiv font-display-nhmvtD size-20-P_cSRT weight-bold-DmI9lw reset-IxiVJZ">From GPT-2 to gpt-oss: Analyzing the Architectural Advances</h4></a><div class="pencraft pc-display-flex pc-gap-4 pc-alignItems-center pc-reset"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"></a><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"></a><a href="https://substack.com/profile/27393275-sebastian-raschka-phd" class="inheritColor-WetTGJ">Sebastian Raschka, PhD</a></div><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T reset-IxiVJZ">·</div><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Aug 9</div></div><div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-0 pc-paddingBottom-0 pc-alignItems-center pc-reset"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" class="pencraft pc-reset align-center-y7ZD4w line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-medium-fw81nC reset-IxiVJZ"><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset link-HREYZo"><span class="pencraft pc-reset color-accent-BVX_7M line-height-20-t4M0El font-text-qe4AeH size-14-MLPa7j weight-semibold-uqA4FV reset-IxiVJZ">Read full story</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></div></a></div></div></div></div><p></p><p>Before summarizing the interesting tidbits, let's start with an overview of the two models, gpt-oss-20b and gpt-oss-120b, as shown in Figure 26 below.</p><p></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!1PO4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1PO4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png 424w, https://substackcdn.com/image/fetch/$s_!1PO4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png 848w, https://substackcdn.com/image/fetch/$s_!1PO4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png 1272w, https://substackcdn.com/image/fetch/$s_!1PO4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.jpg" width="1631" height="788" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1631,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243953,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11b56999-e6e6-4935-8863-a9e637087a0a_1631x788.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!1PO4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png 424w, https://substackcdn.com/image/fetch/$s_!1PO4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png 848w, https://substackcdn.com/image/fetch/$s_!1PO4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png 1272w, https://substackcdn.com/image/fetch/$s_!1PO4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d438dde-c0f6-4e99-a0a8-b41e11366a27_1631x788.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 26: Architecture overview of the two gpt-oss models.</figcaption></figure></div><p>Looking at Figure 26, the architecture contains all the familiar components we have seen in other architectures discussed previously. For instance, Figure 27 puts the smaller gpt-oss architecture next to Qwen3 30B-A3B, which is also an MoE model with a similar number of active parameters (gpt-oss has 3.6B active parameters, and Qwen3 30B-A3B has 3.3B).</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!eSQG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!eSQG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png 424w, https://substackcdn.com/image/fetch/$s_!eSQG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png 848w, https://substackcdn.com/image/fetch/$s_!eSQG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png 1272w, https://substackcdn.com/image/fetch/$s_!eSQG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/c79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.jpg" width="1456" height="750" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:750,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:273573,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!eSQG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png 424w, https://substackcdn.com/image/fetch/$s_!eSQG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png 848w, https://substackcdn.com/image/fetch/$s_!eSQG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png 1272w, https://substackcdn.com/image/fetch/$s_!eSQG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc79215d3-891e-431f-af20-8e2a3fa4be10_1552x799.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 27: Architecture comparison between gpt-oss and Qwen3</figcaption></figure></div><p>One aspect not shown in Figure 27 is that gpt-oss uses sliding window attention (similar to Gemma 3, but in every other layer instead of using a 5:1 ratio). </p><h3 class="header-anchor-post"><strong>9.1 Width Versus Depth</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§width-versus-depth" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/width-versus-depth" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p>Figure 27 shows that gpt-oss and Qwen3 use similar components. But if we look at the two models closely, we see that Qwen3 is a much deeper architecture with its 48 transformer blocks instead of 24.</p><p>On the other hand, gpt-oss is a much wider architecture:</p><ul><li><p>An embedding dimension of 2880 instead of 2048</p></li><li><p>An intermediate expert (feed forward) projection dimension of also 2880 instead of 768</p></li></ul><p>It's also worth noting that gpt-oss uses twice as many attention heads, but this doesn't directly increase the model's width. The width is determined by the embedding dimension.</p><p>Does one approach offer advantages over the other given a fixed number of parameters? As a rule of thumb, deeper models have more flexibility but can be harder to train due to instability issues, due to exploding and vanishing gradients (which RMSNorm and shortcut connections aim to mitigate).</p><p>Wider architectures have the advantage of being faster during inference (with a higher tokens/second throughput) due to better parallelization at a higher memory cost.</p><p><span>When it comes to modeling performance, there's unfortunately no good apples-to-apples comparison I am aware of (where parameter size and datasets are kept constant) except for an ablation study in the </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 paper (Table 9)</a><span>, which found that for a 9B parameter architecture, a wider setup is slightly better than a deeper setup. Across 4 benchmarks, the wider model achieved a 52.0 average score, and the deeper model achieved a 50.8 average score.</span></p><h3 class="header-anchor-post"><strong>9.2 Few Large Versus Many Small Expert</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§few-large-versus-many-small-expert" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/few-large-versus-many-small-expert" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p>As shown in Figure 27 above, it's also noteworthy that gpt-oss has a surprisingly small number of experts (32 instead of 128), and only uses 4 instead of 8 active experts per token. However, each expert is much larger than the experts in Qwen3.</p><p>This is interesting because the recent trends and developments point towards more, smaller models as being beneficial. This change, at a constant total parameter size, is nicely illustrated in Figure 28 below from the DeepSeekMoE paper.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kMwd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kMwd!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png 424w, https://substackcdn.com/image/fetch/$s_!kMwd!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png 848w, https://substackcdn.com/image/fetch/$s_!kMwd!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png 1272w, https://substackcdn.com/image/fetch/$s_!kMwd!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.jpg" width="1046" height="565" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:565,&quot;width&quot;:1046,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:196813,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kMwd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png 424w, https://substackcdn.com/image/fetch/$s_!kMwd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png 848w, https://substackcdn.com/image/fetch/$s_!kMwd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png 1272w, https://substackcdn.com/image/fetch/$s_!kMwd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ead67-6811-4c9d-af7b-4e6b2563ea68_1046x565.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 28: An annotated figure from "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", https://arxiv.org/abs/2401.06066</figcaption></figure></div><p>Notably, unlike DeepSeek's models, neither gpt-oss nor Qwen3 uses shared experts, though.</p><h3 class="header-anchor-post"><strong>9.3 Attention Bias and Attention Sinks</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§attention-bias-and-attention-sinks" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/attention-bias-and-attention-sinks" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h3><p>Both gpt-oss and Qwen3 use grouped query attention. The main difference is that gpt-oss restricts the context size via sliding window attention in each second layer, as mentioned earlier.</p><p>However, there's one interesting detail that caught my eye. It seems that gpt-oss uses bias units for the attention weights, as shown in Figure 29 below.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!hfZq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!hfZq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png 424w, https://substackcdn.com/image/fetch/$s_!hfZq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png 848w, https://substackcdn.com/image/fetch/$s_!hfZq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png 1272w, https://substackcdn.com/image/fetch/$s_!hfZq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.jpg" width="1221" height="352" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:352,&quot;width&quot;:1221,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:170252,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!hfZq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png 424w, https://substackcdn.com/image/fetch/$s_!hfZq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png 848w, https://substackcdn.com/image/fetch/$s_!hfZq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png 1272w, https://substackcdn.com/image/fetch/$s_!hfZq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15ddb1b6-1540-4889-86c5-c54f5918562e_1221x352.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 29: gpt-oss models use bias units in the attention layers. See code example here.</figcaption></figure></div><p><span>I haven't seen these bias units being used since the GPT-2 days, and they are commonly regarded as redundant. Indeed, I found a recent paper that shows mathematically that this is at least true for the key transformation (</span><code>k_proj</code><span>). Furthermore, the empirical results show that there is little difference between with and without bias units (see Figure 30 below).</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!vQ5q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png" data-component-name="Image2ToDOM" rel="" class="image-link image2"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!vQ5q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png 424w, https://substackcdn.com/image/fetch/$s_!vQ5q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png 848w, https://substackcdn.com/image/fetch/$s_!vQ5q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png 1272w, https://substackcdn.com/image/fetch/$s_!vQ5q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.jpg" width="307" height="151" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:151,&quot;width&quot;:307,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:17228,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!vQ5q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png 424w, https://substackcdn.com/image/fetch/$s_!vQ5q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png 848w, https://substackcdn.com/image/fetch/$s_!vQ5q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png 1272w, https://substackcdn.com/image/fetch/$s_!vQ5q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fa2e161-ba61-43ab-a17b-5e6eab448699_307x151.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div></div></div></a><figcaption class="image-caption">Figure 30: Table from https://arxiv.org/pdf/2302.08626 showing the average test loss when the models were trained from scratch with and without bias units.</figcaption></figure></div><p><span>Another detail you may have noticed is the definition of </span><code>sinks</code><span> in the code screenshot in Figure 30. In general models, attention sinks are special "always-attended" tokens placed at the start of the sequence to stabilize attention, which is especially useful in long-context scenarios. I.e., if the context gets very long, this special attended token at the beginning is still attended to, and it can learn to store some generally useful information about the entire sequence. (I think it was originally proposed in the </span><a href="https://arxiv.org/abs/2309.17453" rel="">Efficient Streaming Language Models with Attention Sinks</a><span> paper.)</span></p><p><span>In the gpt-oss implementation, </span><em>attention sinks</em><span> are not actual tokens in the input sequence. Instead, they are learned per-head bias logits that are appended to the attention scores (Figure 31). The goal is the same as with the above-mentioned attention sinks, but without modifying the tokenized inputs.</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xuLx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xuLx!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png 424w, https://substackcdn.com/image/fetch/$s_!xuLx!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png 848w, https://substackcdn.com/image/fetch/$s_!xuLx!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png 1272w, https://substackcdn.com/image/fetch/$s_!xuLx!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/389b83da-f588-4e54-93ca-716d60d7f087_1064x754.jpg" width="1064" height="754" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:754,&quot;width&quot;:1064,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:277488,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!xuLx!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png 424w, https://substackcdn.com/image/fetch/$s_!xuLx!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png 848w, https://substackcdn.com/image/fetch/$s_!xuLx!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png 1272w, https://substackcdn.com/image/fetch/$s_!xuLx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F389b83da-f588-4e54-93ca-716d60d7f087_1064x754.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><span>Figure 31: The use of attention sinks in gpt-oss; based on the Hugging Face code </span><a href="https://github.com/huggingface/transformers/blame/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py" rel="">here</a><span>.</span></figcaption></figure></div><p>For more information about gpt-oss, and how it compares to GPT-2, please see my other gpt-oss article:</p><div data-component-name="DigestPostEmbed" class="digestPostEmbed-flwiST"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"></a><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"><div class="pencraft pc-reset" style="width: 70px; height: 70px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kftt!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F529c4cc7-161c-4d7c-b186-06e68c771776_1564x926.png"><img src="./The Big LLM Architecture Comparison_files/529c4cc7-161c-4d7c-b186-06e68c771776_1564x926.jpg" sizes="100vw" alt="From GPT-2 to gpt-oss: Analyzing the Architectural Advances" width="140" height="140" class="img-OACg1c smSquare-NGbPBa pencraft pc-reset"></picture></div></a><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"><h4 class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-24-jnGwiv font-display-nhmvtD size-20-P_cSRT weight-bold-DmI9lw reset-IxiVJZ">From GPT-2 to gpt-oss: Analyzing the Architectural Advances</h4></a><div class="pencraft pc-display-flex pc-gap-4 pc-alignItems-center pc-reset"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"></a><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" rel="noopener" target="_blank"></a><a href="https://substack.com/profile/27393275-sebastian-raschka-phd" class="inheritColor-WetTGJ">Sebastian Raschka, PhD</a></div><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T reset-IxiVJZ">·</div><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Aug 9</div></div><div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-0 pc-paddingBottom-0 pc-alignItems-center pc-reset"><a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the" class="pencraft pc-reset align-center-y7ZD4w line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-medium-fw81nC reset-IxiVJZ"><div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset link-HREYZo"><span class="pencraft pc-reset color-accent-BVX_7M line-height-20-t4M0El font-text-qe4AeH size-14-MLPa7j weight-semibold-uqA4FV reset-IxiVJZ">Read full story</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></div></a></div></div></div></div><p></p><h1 class="header-anchor-post">10. Grok 2.5<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§grok" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/grok" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p>A few weeks after this article first went online, xAI released the weights of their 270B-parameter Grok 2.5 model.</p><p>I thought it would be worth including here, since Grok 2.5 was xAI's flagship production model last year. Up to this point, all models we discussed were released as open-weight models from the start. For example, gpt-oss is likely not an open-weight clone of GPT-4 but rather a custom model trained specifically for the open-source community.</p><p>With Grok 2.5, we get a rare look at a real production system, even if it is last year's.</p><p>Architecturally, Grok 2.5 looks fairly standard overall (Figure 32), but there are a few noteworthy details.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3Zgv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3Zgv!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png 424w, https://substackcdn.com/image/fetch/$s_!3Zgv!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png 848w, https://substackcdn.com/image/fetch/$s_!3Zgv!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png 1272w, https://substackcdn.com/image/fetch/$s_!3Zgv!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/59c43b82-8214-4def-996f-f22a45fa085c_1683x919.jpg" width="1456" height="795" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:795,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:290496,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3Zgv!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png 424w, https://substackcdn.com/image/fetch/$s_!3Zgv!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png 848w, https://substackcdn.com/image/fetch/$s_!3Zgv!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png 1272w, https://substackcdn.com/image/fetch/$s_!3Zgv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c43b82-8214-4def-996f-f22a45fa085c_1683x919.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 32: Grok 2.5 next to a Qwen3 model of comparable size</figcaption></figure></div><p></p><p></p><p>For instance, Grok 2.5 uses a small number of large experts (eight), which reflects an older trend. As discussed earlier, more recent designs such as those in the DeepSeekMoE paper favor a larger number of smaller experts (this is also present in Qwen3).</p><p>Another interesting choice is the use of what amounts to a shared expert. The additional SwiGLU module shown on the left in Figure 32 functions as an always-on, shared expert. It is not identical to the classic shared-expert design since its intermediate dimension is doubled, but the idea is the same. (I still find it interesting that Qwen3 omitted shared experts, and it will be interesting to see if that changes with Qwen4 and later models.)</p><h1 class="header-anchor-post">11. GLM-4.5<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§glm" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/glm" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p><a href="https://arxiv.org/abs/2508.06471" rel="">GLM-4.5</a><span> is another major release this year. </span></p><p>It is an instruction/reasoning hybrid similar to Qwen3, but even better optimized for function calling and agent-style contexts.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qmQC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qmQC!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg 424w, https://substackcdn.com/image/fetch/$s_!qmQC!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg 848w, https://substackcdn.com/image/fetch/$s_!qmQC!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!qmQC!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/cf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpg" width="1456" height="1030" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1030,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1245604,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qmQC!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg 424w, https://substackcdn.com/image/fetch/$s_!qmQC!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg 848w, https://substackcdn.com/image/fetch/$s_!qmQC!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!qmQC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 33: GLM-4.5 benchmark from the official GitHub repository at https://github.com/zai-org/GLM-4.5</figcaption></figure></div><p></p><p>As shown in Figure 34, GLM-4.5 comes in two variants. The flagship 355-billion-parameter model outperforms Claude 4 Opus on average across 12 benchmarks and trails only slightly behind OpenAI’s o3 and xAI’s Grok 4. There is also GLM-4.5-Air, a more compact 106-billion-parameter version that delivers performance only marginally below the 355-billion model.</p><p>Figure 35 compares the 355-billion architecture to Qwen3. </p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ayu0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ayu0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png 424w, https://substackcdn.com/image/fetch/$s_!ayu0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png 848w, https://substackcdn.com/image/fetch/$s_!ayu0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png 1272w, https://substackcdn.com/image/fetch/$s_!ayu0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/ac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.jpg" width="1456" height="801" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:801,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:271143,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ayu0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png 424w, https://substackcdn.com/image/fetch/$s_!ayu0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png 848w, https://substackcdn.com/image/fetch/$s_!ayu0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png 1272w, https://substackcdn.com/image/fetch/$s_!ayu0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac10a372-7846-4baf-b3db-bc157fa9c7aa_1494x822.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 34: GLM-4.5 next to a similarly-sized Qwen3 model.</figcaption></figure></div><p></p><p>The designs are largely similar, but GLM-4.5 adopts a structural choice first introduced by DeepSeek V3: 3 dense layers precede the Mixture-of-Experts (MoE) blocks. Why? Starting with several dense layers improves convergence stability and overall performance in large MoE systems. If MoE routing is introduced immediately, the instability of sparse expert selection can interfere with early syntactic and semantic feature extraction. So, one might say that by keeping the initial layers dense ensures the model forms stable low-level representations before routing decisions begin to shape higher-level processing.</p><p>Also, GLM-4.5 uses a shared expert similar to DeepSeek-V3 (and unlike Qwen3).</p><p>(Interestingly, GLM-4.5 also retains the attention bias mechanism used in GPT-2 and gpt-oss.)</p><p></p><h1 class="header-anchor-post">12. Qwen3-Next<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§qwen-next" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qwen-next" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h1><p>On 11 September 2025, the Qwen3 team released Qwen3 Next 80B-A3B (Figure 35), available in both Instruct and Thinking variants. While its design builds on the previously discussed Qwen3 architecture, I included it here as a separate entry to keep the figure numbering consistent and to draw attention to some of its design changes.</p><h2 class="header-anchor-post"><strong>12.1 Expert Size and Number</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§expert-size-and-number" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/expert-size-and-number" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p>The new Qwen3 Next architecture stands out because, despite being 3× smaller than the previous 235B-A22B model (Figure 35), it introduces four times as many experts and even adds a shared expert. Both of these design choices (a high expert count and the inclusion of a shared expert) were future directions I had highlighted prior to this release, particularly in the video version of the article that I linked at the top.</p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!iUTU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!iUTU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png 424w, https://substackcdn.com/image/fetch/$s_!iUTU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png 848w, https://substackcdn.com/image/fetch/$s_!iUTU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png 1272w, https://substackcdn.com/image/fetch/$s_!iUTU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/ce674219-3fb4-4c76-b397-408f477ce827_4111x2244.jpg" width="1456" height="795" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:795,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:875361,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!iUTU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png 424w, https://substackcdn.com/image/fetch/$s_!iUTU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png 848w, https://substackcdn.com/image/fetch/$s_!iUTU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png 1272w, https://substackcdn.com/image/fetch/$s_!iUTU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce674219-3fb4-4c76-b397-408f477ce827_4111x2244.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Figure 35: The original Qwen3 model released in May (left) next to the Qwen3 Next model released in September (right).</figcaption></figure></div><p></p><p></p><h2 class="header-anchor-post"><strong>12.2 Gated DeltaNet + Gated Attention Hybrid</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§gated-deltanet-gated-attention-hybrid" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/gated-deltanet-gated-attention-hybrid" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p><span>The other highlight is that they replace the regular attention mechanism by a </span><a href="https://arxiv.org/abs/2412.06464" rel="">Gated DeltaNet</a><span> + </span><a href="https://arxiv.org/abs/2505.06708" rel="">Gated Attention</a><span> hybrid, which helps enable the native 262k token context length in terms of memory usage (the previous 235B-A22B model model supported 32k natively, and 131k with </span><a href="https://arxiv.org/abs/2309.00071" rel="">YaRN</a><span> scaling.)</span></p><p><span>So how does this new attention hybrid work? Compared to grouped‑query attention (GQA), which is still standard scaled dot‑product attention (sharing K/V across query‑head groups to cut KV‑cache size and memory bandwidth as discussed earlier but whose decode cost and cache still grow with sequence length), their hybrid mechanism mixes </span><em>Gated DeltaNet</em><span> blocks with </span><em>Gated Attention</em><span> blocks with in a 3:1 ratio as shown in Figure 36.</span></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5TUn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5TUn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png 424w, https://substackcdn.com/image/fetch/$s_!5TUn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png 848w, https://substackcdn.com/image/fetch/$s_!5TUn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png 1272w, https://substackcdn.com/image/fetch/$s_!5TUn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/cbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.jpg" width="1456" height="983" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:983,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:977047,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5TUn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png 424w, https://substackcdn.com/image/fetch/$s_!5TUn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png 848w, https://substackcdn.com/image/fetch/$s_!5TUn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png 1272w, https://substackcdn.com/image/fetch/$s_!5TUn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbd1f80f-9ace-4b4c-b79a-2397a6f75dc8_3845x2597.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><span>Figure 36: The Gated DeltaNet + Gated Attention hybrid mechanism. Note that these are arranges in a 3:1 ratio, meaning that 3 transformer blocks with Gated DeltaNet are followed by 1 transformer block with Gated Attention. The right subfigure is from the official Qwen3 blog: </span><a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list" rel="">https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list</a></figcaption></figure></div><p><span>We can think of the gated attention block as standard scaled-dot-product attention that can be used in GQA, but it has a few tweaks on top. The main differences between </span><em>gated attention</em><span> and plain GQA block are:</span></p><ol><li><p>an output gate (sigmoid-controlled, usually per-channel) that scales the attention result before it is added back to the residual;</p></li><li><p>zero-centered RMSNorm for QKNorm, rather than a standard RMSNorm;</p></li><li><p>partial RoPE (on a subset of dimensions).</p></li></ol><p>Note that these are essentially just stability changes to GQA.</p><p><span>The Gated DeltaNet is a more significant change. In the DeltaNet block, q, k, v and two gates (α, β) are produced by linear and lightweight convolutional layers with normalization, and the layer replaces attention with a fast‑weight </span><em><a href="https://arxiv.org/abs/2412.06464" rel="">delta rule</a></em><span> update.</span></p><p>However, the tradeoff is that DeltaNet offers less precise content‑based retrieval than full attention, which is why one gated attention layer remains.</p><p>Given that attention grows quadratically, the DeltaNet component was added to help with memory efficiency. In the "linear-time, cache-free" family, the DeltaNet block is a essentially an alternative to Mamba. Mamba keeps a state with a learned state-space filter (essentially a dynamic convolution over time). DeltaNet keeps a tiny fast-weight memory updated with α and β and reads it with q, with small convolutions only used only to help form q, k, v, α, β.</p><h2 class="header-anchor-post"><strong>12.3 Multi-Token Prediction</strong><div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div id="§multi-token-prediction" class="pencraft pc-reset header-anchor offset-top"></div><button tabindex="0" type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/multi-token-prediction" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></button></div></div></h2><p><span>The two subsections above describe two design decisions geared towards efficiency. Since all good things come in threes, the Qwen3 added another technique on top: </span><a href="https://arxiv.org/abs/2404.19737" rel="">Multi-Token Prediction</a><span> (MTP).</span></p><p><span>Multi-token prediction trains the LLM to predict several future tokens, instead of a single one, at each step. Here, at each position </span><em>t</em><span>, small extra heads (linear layers) output logits for </span><em>t+1...t+k</em><span>, and we sum cross-entropy losses for these offsets (in the </span><a href="https://arxiv.org/abs/2404.19737" rel="">MTP</a><span> paper the researchers recommended </span><em>k=4</em><span>). This additional signal speeds up training, and inference may remains one token at a time. However, the extra heads can be used in speculative multi-token decoding, which is what Qwen3-Next seems to do, however, the details are still a bit sparse:</span></p><blockquote><p><span>Qwen3-Next introduces a native Multi-Token Prediction (MTP) mechanism, which not only yields an MTP module with a high acceptance rate for Speculative Decoding but also enhances the overall performance.Additionally, Qwen3-Next specifically optimizes the multi-step inference performance of MTP, further improving the acceptance rate of Speculative Decoding in real scenarios through multi-step training that maintains consistency between training and inference. </span><a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list" rel="">Souce: Qwen3-Next blog post</a></p></blockquote><p></p><p><strong>After all these years, LLM releases remain exciting, and I am curious to see what's next!</strong></p><p></p><div><hr></div><p><em>This magazine is a personal passion project, and your support helps keep it alive. If you would like to contribute, there are a few great ways:</em></p><ul><li><p><em><strong><a href="https://amzn.to/4fqvn0D" rel="">Grab a copy of my book</a></strong><span>. Build a Large Language Model (From Scratch) walks you through building an LLM step by step, from tokenizer to training.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://www.manning.com/livevideo/master-and-build-large-language-models" rel="">Check out the video course</a></strong><span>. There’s now a 17-hour video course based on the book, available from Manning. It follows the book closely, section by section, and works well both as a standalone or as a code-along resource. The video course is ad-free (unlike the YouTube version) and has a cleaner, more structured format. It also contains 5 additional hours of pre-requisite video material created by Abhinav Kimothi.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://magazine.sebastianraschka.com/subscribe" rel="">Subscribe</a></strong><span>. A paid subscription helps to make my writing sustainable and gives you access to additional contents.</span></em></p></li></ul><p><em>Thanks for reading, and for helping support independent research!</em></p><div class="captioned-image-container"><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wVLk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw"><img src="./The Big LLM Architecture Comparison_files/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpg" width="1456" height="878" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw" loading="lazy" class="sizing-normal"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><span>Build A Large Language From Scratch </span><a href="https://www.manning.com/livevideo/master-and-build-large-language-models" rel="">video course</a></figcaption></figure></div></div></div><div class="visibility-check"></div><div class="pencraft pc-display-flex pc-paddingTop-16 pc-reset divider-Ti4OTa"><hr></div><div class="visibility-check"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-16 pc-paddingTop-32 pc-paddingBottom-32 pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-paddingBottom-8 pc-reset"><h4 class="pencraft pc-reset flex-grow-rzmknG align-center-y7ZD4w line-height-24-jnGwiv font-display-nhmvtD size-20-P_cSRT weight-bold-DmI9lw reset-IxiVJZ">Subscribe to Ahead of AI</h4><div class="pencraft pc-reset flex-grow-rzmknG color-secondary-ls1g8s align-center-y7ZD4w line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-regular-mUq6Gb reset-IxiVJZ">By Sebastian Raschka · Hundreds of paid subscribers</div><div class="pencraft pc-reset flex-grow-rzmknG color-primary-zABazT align-center-y7ZD4w line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-regular-mUq6Gb reset-IxiVJZ">Ahead of AI specializes in Machine Learning &amp; AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.</div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-16 pc-alignItems-center pc-reset"><div data-component-name="SubscribeWidget" class="subscribe-widget is-signed-up subscribeWidget-sIP7pQ"><div class="pencraft pc-reset button-wrapper"><div class="pencraft pc-display-flex pc-justifyContent-center pc-reset"><button tabindex="0" type="button" data-href="https://magazine.sebastianraschka.com/subscribe?utm_medium=web&amp;utm_source=post-end-cta&amp;utm_content=168650848&amp;next=https%3A%2F%2Fmagazine.sebastianraschka.com%2Fp%2Fthe-big-llm-architecture-comparison" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-RfbeYt size_md-gCDS3o"><span>Upgrade to paid</span></button></div></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-maxWidth-400 pc-reset"><div class="visibility-check"></div><label class="pencraft pc-display-flex pc-gap-8 pc-justifyContent-center pc-alignItems-center pc-reset tosCheckbox-XbLWCT"><div class="pencraft pc-reset color-secondary-ls1g8s align-center-y7ZD4w line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ">By subscribing,  I agree to Substack's <a href="https://substack.com/tos" target="_blank" class="pencraft pc-reset reset-IxiVJZ" style="text-decoration: underline;">Terms of Use</a>, and acknowledge its <a href="https://substack.com/ccpa#personal-data-collected" target="_blank" class="pencraft pc-reset reset-IxiVJZ" style="text-decoration: underline;">Information Collection Notice</a> and <a href="https://substack.com/privacy" target="_blank" class="pencraft pc-reset reset-IxiVJZ" style="text-decoration: underline;">Privacy Policy</a>.</div></label></div></div></div><div class="pencraft pc-display-flex pc-paddingTop-16 pc-paddingBottom-16 pc-reset border-top-detail-themed-k9TZAY"><div class="pencraft pc-display-flex pc-gap-16 pc-alignItems-center pc-reset color-secondary-ls1g8s"><div class="pencraft pc-display-flex pc-flexDirection-row pc-gap-8 pc-alignItems-center pc-justifyContent-flex-start pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-row pc-alignItems-center pc-justifyContent-flex-start pc-reset rtl-zsi3Q8" style="--scale: 32px; --offset: 8px; --border-width: 4px;"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/profile/14449271-rajesh-chukka" aria-label="View Rajesh Chukka&#39;s profile" class="pencraft pc-display-contents pc-reset"><div tabindex="0" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6 avatar-u8q6xB" style="--scale: 32px;"><div title="Rajesh Chukka" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!YWMg!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc90b1176-ee4a-47e6-94a2-2a51f18e81bb_96x96.jpeg 32w, https://substackcdn.com/image/fetch/$s_!YWMg!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc90b1176-ee4a-47e6-94a2-2a51f18e81bb_96x96.jpeg 64w, https://substackcdn.com/image/fetch/$s_!YWMg!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc90b1176-ee4a-47e6-94a2-2a51f18e81bb_96x96.jpeg 96w" sizes="32px"><img src="./The Big LLM Architecture Comparison_files/c90b1176-ee4a-47e6-94a2-2a51f18e81bb_96x96.jpg" sizes="32px" alt="Rajesh Chukka&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!YWMg!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc90b1176-ee4a-47e6-94a2-2a51f18e81bb_96x96.jpeg 32w, https://substackcdn.com/image/fetch/$s_!YWMg!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc90b1176-ee4a-47e6-94a2-2a51f18e81bb_96x96.jpeg 64w, https://substackcdn.com/image/fetch/$s_!YWMg!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc90b1176-ee4a-47e6-94a2-2a51f18e81bb_96x96.jpeg 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/profile/250725449-praveen-satyamsetti" aria-label="View Praveen Satyamsetti&#39;s profile" class="pencraft pc-display-contents pc-reset"><div tabindex="0" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6 avatar-u8q6xB overlap-q75iOo" style="--scale: 32px;"><div title="Praveen Satyamsetti" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!9Qyp!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42940902-9dd0-46be-8278-8b0fa84699a6_1024x1536.png 32w, https://substackcdn.com/image/fetch/$s_!9Qyp!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42940902-9dd0-46be-8278-8b0fa84699a6_1024x1536.png 64w, https://substackcdn.com/image/fetch/$s_!9Qyp!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42940902-9dd0-46be-8278-8b0fa84699a6_1024x1536.png 96w" sizes="32px"><img src="./The Big LLM Architecture Comparison_files/42940902-9dd0-46be-8278-8b0fa84699a6_1024x1536.jpg" sizes="32px" alt="Praveen Satyamsetti&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!9Qyp!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42940902-9dd0-46be-8278-8b0fa84699a6_1024x1536.png 32w, https://substackcdn.com/image/fetch/$s_!9Qyp!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42940902-9dd0-46be-8278-8b0fa84699a6_1024x1536.png 64w, https://substackcdn.com/image/fetch/$s_!9Qyp!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42940902-9dd0-46be-8278-8b0fa84699a6_1024x1536.png 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/profile/36105908-dan-thorin" aria-label="View Dan Thorin&#39;s profile" class="pencraft pc-display-contents pc-reset"><div tabindex="0" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6 avatar-u8q6xB overlap-q75iOo" style="--scale: 32px;"><div title="Dan Thorin" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!mvss!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33a3c5-c450-43a8-826c-0261ab68ae56_144x144.png 32w, https://substackcdn.com/image/fetch/$s_!mvss!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33a3c5-c450-43a8-826c-0261ab68ae56_144x144.png 64w, https://substackcdn.com/image/fetch/$s_!mvss!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33a3c5-c450-43a8-826c-0261ab68ae56_144x144.png 96w" sizes="32px"><img src="./The Big LLM Architecture Comparison_files/af33a3c5-c450-43a8-826c-0261ab68ae56_144x144.png" sizes="32px" alt="Dan Thorin&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!mvss!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33a3c5-c450-43a8-826c-0261ab68ae56_144x144.png 32w, https://substackcdn.com/image/fetch/$s_!mvss!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33a3c5-c450-43a8-826c-0261ab68ae56_144x144.png 64w, https://substackcdn.com/image/fetch/$s_!mvss!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33a3c5-c450-43a8-826c-0261ab68ae56_144x144.png 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/profile/556728-kimsia-sim" aria-label="View KimSia Sim&#39;s profile" class="pencraft pc-display-contents pc-reset"><div tabindex="0" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6 avatar-u8q6xB overlap-q75iOo" style="--scale: 32px;"><div title="KimSia Sim" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Diav!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5715eeb6-f65f-420a-bd98-657f6ff19814_591x751.jpeg 32w, https://substackcdn.com/image/fetch/$s_!Diav!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5715eeb6-f65f-420a-bd98-657f6ff19814_591x751.jpeg 64w, https://substackcdn.com/image/fetch/$s_!Diav!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5715eeb6-f65f-420a-bd98-657f6ff19814_591x751.jpeg 96w" sizes="32px"><img src="./The Big LLM Architecture Comparison_files/5715eeb6-f65f-420a-bd98-657f6ff19814_591x751.jpg" sizes="32px" alt="KimSia Sim&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!Diav!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5715eeb6-f65f-420a-bd98-657f6ff19814_591x751.jpeg 32w, https://substackcdn.com/image/fetch/$s_!Diav!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5715eeb6-f65f-420a-bd98-657f6ff19814_591x751.jpeg 64w, https://substackcdn.com/image/fetch/$s_!Diav!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5715eeb6-f65f-420a-bd98-657f6ff19814_591x751.jpeg 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/profile/241112959-tatv" aria-label="View tatv&#39;s profile" class="pencraft pc-display-contents pc-reset"><div tabindex="0" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6 avatar-u8q6xB overlap-q75iOo last-JfNEJ_" style="--scale: 32px;"><div title="tatv" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2Pck!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4668ca01-8ce4-4486-bc2b-d121f9feabd6_1024x1024.jpeg 32w, https://substackcdn.com/image/fetch/$s_!2Pck!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4668ca01-8ce4-4486-bc2b-d121f9feabd6_1024x1024.jpeg 64w, https://substackcdn.com/image/fetch/$s_!2Pck!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4668ca01-8ce4-4486-bc2b-d121f9feabd6_1024x1024.jpeg 96w" sizes="32px"><img src="./The Big LLM Architecture Comparison_files/4668ca01-8ce4-4486-bc2b-d121f9feabd6_1024x1024.jpg" sizes="32px" alt="tatv&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!2Pck!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4668ca01-8ce4-4486-bc2b-d121f9feabd6_1024x1024.jpeg 32w, https://substackcdn.com/image/fetch/$s_!2Pck!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4668ca01-8ce4-4486-bc2b-d121f9feabd6_1024x1024.jpeg 64w, https://substackcdn.com/image/fetch/$s_!2Pck!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4668ca01-8ce4-4486-bc2b-d121f9feabd6_1024x1024.jpeg 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div></div></div><div class="pencraft pc-reset color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ"><div class="pencraft pc-display-flex pc-gap-4 pc-alignItems-center pc-reset"><a class="pencraft pc-reset cursor-pointer-LYORKw color-secondary-ls1g8s decoration-hover-underline-ClDVRM reset-IxiVJZ">1,181 Likes</a>∙<div class="pencraft pc-reset color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ"><a href="https://substack.com/note/p-168650848/restacks?utm_source=substack&amp;utm_content=facepile-restacks" class="pencraft pc-reset color-secondary-ls1g8s decoration-hover-underline-ClDVRM reset-IxiVJZ">111 Restacks</a></div></div></div></div></div><div class="post-footer"><div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-16 pc-paddingBottom-16 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG border-top-detail-themed-k9TZAY border-bottom-detail-themed-Ua9186 post-ufi"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="like-button-container post-ufi-button style-button"><a role="button" aria-label="Like (1,181)" aria-pressed="false" class="post-ufi-button style-button has-label with-border"><svg role="img" width="20" height="20" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 20px; width: 20px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><div class="label">1,181</div></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><a role="button" href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison/comments" aria-label="View comments (60)" class="post-ufi-button style-button post-ufi-comment-button has-label with-border"><svg role="img" width="20" height="20" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 20px; width: 20px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg></g></svg><div class="label">60</div></a><a role="button" class="post-ufi-button style-button has-label with-border"><svg role="img" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 20px; width: 20px;"><g><title></title><path d="M21 3V8M21 8H16M21 8L18 5.29962C16.7056 4.14183 15.1038 3.38328 13.3879 3.11547C11.6719 2.84766 9.9152 3.08203 8.32951 3.79031C6.74382 4.49858 5.39691 5.65051 4.45125 7.10715C3.5056 8.5638 3.00158 10.2629 3 11.9996M3 21V16M3 16H8M3 16L6 18.7C7.29445 19.8578 8.89623 20.6163 10.6121 20.8841C12.3281 21.152 14.0848 20.9176 15.6705 20.2093C17.2562 19.501 18.6031 18.3491 19.5487 16.8925C20.4944 15.4358 20.9984 13.7367 21 12" stroke-linecap="round" stroke-linejoin="round"></path></g></svg><div class="label">111</div></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><a role="button" href="javascript:void(0)" aria-label="View share options" class="post-ufi-button style-button no-icon has-label with-border"><div class="label">Share</div></a></div></div></div></div></article></div></div></div><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div class="visibility-check"></div><div id="discussion" class="pencraft pc-display-flex pc-flexDirection-column pc-gap-16 pc-paddingTop-32 pc-paddingBottom-32 pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-32 pc-reset container"><h4 class="pencraft pc-reset line-height-24-jnGwiv font-display-nhmvtD size-20-P_cSRT weight-bold-DmI9lw reset-IxiVJZ">Discussion about this post</h4><div class="pencraft pc-alignSelf-flex-start pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-position-relative pc-minWidth-0 pc-reset bg-primary-zk6FDl pc-borderRadius-sm overflow-hidden-WdpwT6"><div aria-label="Select discussion type" role="tablist" aria-orientation="horizontal" class="pencraft pc-display-flex pc-gap-4 pc-padding-4 pc-position-relative pc-reset cursor-default-flE2S1 outline-detail-vcQLyr pc-borderRadius-sm overflow-auto-7WTsTi scrollBar-hidden-HcAIpI"><button tabindex="0" type="button" id="headlessui-tabs-tab-P0-38" role="tab" aria-selected="true" data-headlessui-state="selected" class="pencraft pc-reset pencraft segment-LBFzmC buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_tertiary-rlke8z size_sm-G3LciD">Comments</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-39" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft segment-LBFzmC buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_quaternary-kpMibu size_sm-G3LciD">Restacks</button><div class="pencraft pc-position-absolute pc-height-32 pc-reset bg-secondary-UUD3_J pc-borderRadius-xs sizing-border-box-DggLA4 highlight-U002IP" style="--highlight-width: 81.26250457763672px; --highlight-x: 0px;"></div></div><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH left-Tg8vqp"><div class="overlay-zrMCxn primary-lv_sOW"></div></div><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH right-i3oWGi"><div class="overlay-zrMCxn primary-lv_sOW"></div></div></div></div></div><div class="single-post-section comments-section"><div class="container"><div class="visibility-check"></div><div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"><div data-test-id="comment-input" class="pencraft pc-display-flex pc-reset flex-grow-rzmknG"><form class="form-CkZ7Kt"><div class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><div title="User" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!owWd!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-dark.png 32w, https://substackcdn.com/image/fetch/$s_!owWd!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-dark.png 64w, https://substackcdn.com/image/fetch/$s_!owWd!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-dark.png 96w" sizes="32px"><img src="./The Big LLM Architecture Comparison_files/default-dark.jpg" sizes="32px" alt="User&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!owWd!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-dark.png 32w, https://substackcdn.com/image/fetch/$s_!owWd!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-dark.png 64w, https://substackcdn.com/image/fetch/$s_!owWd!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-dark.png 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset flex-grow-rzmknG"><textarea name="body" placeholder="Write a comment..." aria-label="Write a comment..." rows="4" class="pencraft input-qHk4bN autogrowing-_ipn9Y textarea-GbEjRX inputText-pV_yWb" style="height: 96px;"></textarea><div style="height: 0px; transition: height 250ms var(--animation-smooth), opacity 250ms var(--animation-smooth); display: block; flex-direction: column; opacity: 0;"><div style="padding-top: 0.05px; padding-bottom: 0.05px; display: block; flex: 0 0 auto; flex-direction: column; transition: transform 250ms var(--animation-smooth);"></div></div></div></form></div></div><div class="comment-list post-page-root-comment-list"><div class="comment-list-items"><div class="comment"><div id="comment-137164765" class="comment-anchor"></div><div id="comment-137164765-reply" class="comment-anchor"></div><div role="article" aria-label="Comment by Daniel Kleine" class="pencraft pc-display-flex pc-gap-12 pc-paddingBottom-12 pc-reset comment-content"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/profile/129677294-daniel-kleine?utm_source=comment" aria-label="View Daniel Kleine&#39;s profile" class="pencraft pc-display-contents pc-reset"><div tabindex="0" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6" style="--scale: 32px;"><div title="Daniel Kleine" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!y76U!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff200d88b-772c-4998-9230-3aa57b79ee37_650x472.png 32w, https://substackcdn.com/image/fetch/$s_!y76U!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff200d88b-772c-4998-9230-3aa57b79ee37_650x472.png 64w, https://substackcdn.com/image/fetch/$s_!y76U!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff200d88b-772c-4998-9230-3aa57b79ee37_650x472.png 96w" sizes="32px"><img src="./The Big LLM Architecture Comparison_files/f200d88b-772c-4998-9230-3aa57b79ee37_650x472.jpg" sizes="32px" alt="Daniel Kleine&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!y76U!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff200d88b-772c-4998-9230-3aa57b79ee37_650x472.png 32w, https://substackcdn.com/image/fetch/$s_!y76U!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff200d88b-772c-4998-9230-3aa57b79ee37_650x472.png 64w, https://substackcdn.com/image/fetch/$s_!y76U!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff200d88b-772c-4998-9230-3aa57b79ee37_650x472.png 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-reset"><div class="pencraft pc-display-flex pc-minWidth-0 pc-gap-8 pc-alignItems-center pc-height-20 pc-reset line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-regular-mUq6Gb"><div class="pencraft pc-display-flex pc-minWidth-0 pc-gap-6 pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-6 pc-reset line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ"><span class="pencraft pc-reset weight-medium-fw81nC reset-IxiVJZ"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm inline-lJXy8b"><span class="pencraft pc-reset decoration-hover-underline-ClDVRM reset-IxiVJZ"><a href="https://substack.com/profile/129677294-daniel-kleine?utm_source=substack-feed-item" showback="true" class="link-LIBpto">Daniel Kleine</a></span></div></span></div><div class="publicationHoverCardTarget-sPJ4jb"></div><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison/comment/137164765" rel="nofollow" native="true" title="Jul 20, 2025, 10:19 PM" class="pencraft pc-reset color-secondary-ls1g8s decoration-hover-underline-ClDVRM reset-IxiVJZ"><span class="pencraft pc-reset color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ">Jul 20</span></a></div></div><div class="pencraft pc-display-flex pc-gap-4 pc-reset"><button tabindex="-1" type="button" class="pencraft pc-display-flex pc-gap-4 pc-height-20 pc-paddingLeft-6 pc-paddingRight-6 pc-paddingTop-2 pc-paddingBottom-2 pc-alignItems-center pc-reset cursor-inherit-LxLBJ6 pc-borderRadius-xs size-11-NuY2Zx weight-medium-fw81nC pencraft tag-XbOVLt theme_accent-Y2sqZY priority_secondary-outline-RpooJS"><div class="pencraft pc-display-flex pc-alignItems-center pc-reset leading-mI5Ihl fillIcon-dQ0mii"><svg role="img" width="14" height="14" viewBox="0 0 20 20" fill="var(--color-fg-primary)" stroke-width="2.5" stroke="#000" xmlns="http://www.w3.org/2000/svg" style="height: 14px; width: 14px;"><g><title></title><path stroke="none" d="M9.99915 16.7256C9.90515 16.7256 9.79102 16.692 9.65674 16.6249C9.52246 16.5622 9.3949 16.4906 9.27405 16.41C8.02974 15.6044 6.94657 14.7584 6.02454 13.8722C5.10697 12.9815 4.3953 12.0662 3.88953 11.1262C3.38375 10.1818 3.13086 9.23067 3.13086 8.27283C3.13086 7.63725 3.23157 7.05762 3.43298 6.53394C3.63888 6.01025 3.92086 5.55819 4.27893 5.17773C4.64148 4.79728 5.05774 4.50635 5.52771 4.30493C6.00216 4.09904 6.51241 3.99609 7.05847 3.99609C7.73433 3.99609 8.31844 4.16618 8.81079 4.50635C9.30762 4.84652 9.70374 5.28963 9.99915 5.83569C10.299 5.28516 10.6951 4.84204 11.1875 4.50635C11.6843 4.16618 12.2707 3.99609 12.9465 3.99609C13.4836 3.99609 13.9894 4.09904 14.4639 4.30493C14.9428 4.50635 15.3613 4.79728 15.7194 5.17773C16.0774 5.55819 16.3572 6.01025 16.5586 6.53394C16.7645 7.05762 16.8674 7.63725 16.8674 8.27283C16.8674 9.23067 16.6145 10.1818 16.1088 11.1262C15.603 12.0662 14.8891 12.9815 13.967 13.8722C13.0495 14.7584 11.9708 15.6044 10.731 16.41C10.6056 16.4906 10.4758 16.5622 10.3416 16.6249C10.2118 16.692 10.0976 16.7256 9.99915 16.7256Z"></path></g></svg></div>Liked by Sebastian Raschka, PhD</button></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"></div><div id="trigger43" aria-expanded="false" aria-haspopup="dialog" aria-controls="dialog44" aria-label="View more" class="pencraft pc-display-flex pc-reset triggerContainer-eX588u"><button tabindex="0" type="button" aria-label="Ellipsis" class="pencraft pc-reset pencraft trigger-j08Uop iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_quaternary-kpMibu"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-ellipsis"><circle cx="12" cy="12" r="1"></circle><circle cx="19" cy="12" r="1"></circle><circle cx="5" cy="12" r="1"></circle></svg></button></div></div><div class="comment-body"><p><span>Apart from the architectural differences, what would be interesting to know is on which text data the LLMs have been trained on. From my pov, it's really unfortunate that this info is typically not disclosed, even for open-source LLMs. Not just the amount of training data (e.g. number of tokens) but also the data quality as factors for a true scientific comparison.</span></p><div role="button" class="show-all-toggle"><div class="show-all-toggle-label">Expand full comment</div></div></div><div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-8 pc-justifyContent-flex-start pc-alignItems-center pc-reset comment-actions withShareButton-hQzuEn"><span class="pencraft pc-reset color-pub-secondary-text-hGQ02T decoration-hover-underline-ClDVRM reset-IxiVJZ"><a href="javascript:void(0)" class="like-button"><div class="pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset"><div class="pencraft pc-display-flex pc-reset reaction-container"><svg role="img" width="24" height="24" viewBox="0 0 24 24" fill="#000000" stroke-width="1.8" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="animation"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><svg role="img" width="16" height="16" viewBox="0 0 24 24" fill="transparent" stroke-width="2" stroke="var(--color-fg-secondary-themed)" xmlns="http://www.w3.org/2000/svg" style="height: 16px; width: 16px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="transparent" stroke="var(--color-fg-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg></div><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA like-count">Like (7)</div></div></a></span><span class="pencraft pc-reset decoration-hover-underline-ClDVRM reset-IxiVJZ"><a class="pencraft pc-reset link-_X6et2 link-LIBpto"><div class="pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="var(--color-fg-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Reply</div></div></a></span><span class="pencraft pc-reset decoration-hover-underline-ClDVRM reset-IxiVJZ"><a class="pencraft pc-reset link-_X6et2 link-LIBpto"><div class="pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="var(--color-fg-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Share</div></div></a></span></div><div style="height: 0px; transition: height 250ms var(--animation-smooth), opacity 250ms var(--animation-smooth); display: block; flex-direction: column; opacity: 0;"><div style="padding-top: 0.05px; padding-bottom: 0.05px; display: block; flex: 0 0 auto; flex-direction: column; transition: transform 250ms var(--animation-smooth);"></div></div></div></div><div class="more-replies-container"><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison/comment/137164765" class="more-replies">2 replies by Sebastian Raschka, PhD and others</a></div></div><div class="comment"><div id="comment-136865116" class="comment-anchor"></div><div id="comment-136865116-reply" class="comment-anchor"></div><div role="article" aria-label="Comment by Leo Benaharon" class="pencraft pc-display-flex pc-gap-12 pc-paddingBottom-12 pc-reset comment-content"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/profile/36452329-leo-benaharon?utm_source=comment" aria-label="View Leo Benaharon&#39;s profile" class="pencraft pc-display-contents pc-reset"><div tabindex="0" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA animate-XFJxE4 outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 pressable-sm-YIJFKJ showFocus-sk_vEm container-TAtrWj interactive-UkK0V6" style="--scale: 32px;"><div title="Leo Benaharon" class="pencraft pc-display-flex pc-width-32 pc-height-32 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 32px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!l79E!,w_32,h_32,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eb78471-0a08-48bb-892f-91bc4d6b9c4d_96x96.jpeg 32w, https://substackcdn.com/image/fetch/$s_!l79E!,w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eb78471-0a08-48bb-892f-91bc4d6b9c4d_96x96.jpeg 64w, https://substackcdn.com/image/fetch/$s_!l79E!,w_96,h_96,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eb78471-0a08-48bb-892f-91bc4d6b9c4d_96x96.jpeg 96w" sizes="32px"><img src="./The Big LLM Architecture Comparison_files/6eb78471-0a08-48bb-892f-91bc4d6b9c4d_96x96.jpg" sizes="32px" alt="Leo Benaharon&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!l79E!,w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eb78471-0a08-48bb-892f-91bc4d6b9c4d_96x96.jpeg 32w, https://substackcdn.com/image/fetch/$s_!l79E!,w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eb78471-0a08-48bb-892f-91bc4d6b9c4d_96x96.jpeg 64w, https://substackcdn.com/image/fetch/$s_!l79E!,w_96,h_96,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eb78471-0a08-48bb-892f-91bc4d6b9c4d_96x96.jpeg 96w" width="32" height="32" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></a></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-reset"><div class="pencraft pc-display-flex pc-minWidth-0 pc-gap-8 pc-alignItems-center pc-height-20 pc-reset line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-regular-mUq6Gb"><div class="pencraft pc-display-flex pc-minWidth-0 pc-gap-6 pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-6 pc-reset line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ"><span class="pencraft pc-reset weight-medium-fw81nC reset-IxiVJZ"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm inline-lJXy8b"><span class="pencraft pc-reset decoration-hover-underline-ClDVRM reset-IxiVJZ"><a href="https://substack.com/profile/36452329-leo-benaharon?utm_source=substack-feed-item" showback="true" class="link-LIBpto">Leo Benaharon</a></span></div></span></div><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison/comment/136865116" rel="nofollow" native="true" title="Jul 19, 2025, 9:27 PM" class="pencraft pc-reset color-secondary-ls1g8s decoration-hover-underline-ClDVRM reset-IxiVJZ"><span class="pencraft pc-reset color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ">Jul 19</span></a></div></div><div class="pencraft pc-display-flex pc-gap-4 pc-reset"><button tabindex="-1" type="button" class="pencraft pc-display-flex pc-gap-4 pc-height-20 pc-paddingLeft-6 pc-paddingRight-6 pc-paddingTop-2 pc-paddingBottom-2 pc-alignItems-center pc-reset cursor-inherit-LxLBJ6 pc-borderRadius-xs size-11-NuY2Zx weight-medium-fw81nC pencraft tag-XbOVLt theme_accent-Y2sqZY priority_secondary-outline-RpooJS"><div class="pencraft pc-display-flex pc-alignItems-center pc-reset leading-mI5Ihl fillIcon-dQ0mii"><svg role="img" width="14" height="14" viewBox="0 0 20 20" fill="var(--color-fg-primary)" stroke-width="2.5" stroke="#000" xmlns="http://www.w3.org/2000/svg" style="height: 14px; width: 14px;"><g><title></title><path stroke="none" d="M9.99915 16.7256C9.90515 16.7256 9.79102 16.692 9.65674 16.6249C9.52246 16.5622 9.3949 16.4906 9.27405 16.41C8.02974 15.6044 6.94657 14.7584 6.02454 13.8722C5.10697 12.9815 4.3953 12.0662 3.88953 11.1262C3.38375 10.1818 3.13086 9.23067 3.13086 8.27283C3.13086 7.63725 3.23157 7.05762 3.43298 6.53394C3.63888 6.01025 3.92086 5.55819 4.27893 5.17773C4.64148 4.79728 5.05774 4.50635 5.52771 4.30493C6.00216 4.09904 6.51241 3.99609 7.05847 3.99609C7.73433 3.99609 8.31844 4.16618 8.81079 4.50635C9.30762 4.84652 9.70374 5.28963 9.99915 5.83569C10.299 5.28516 10.6951 4.84204 11.1875 4.50635C11.6843 4.16618 12.2707 3.99609 12.9465 3.99609C13.4836 3.99609 13.9894 4.09904 14.4639 4.30493C14.9428 4.50635 15.3613 4.79728 15.7194 5.17773C16.0774 5.55819 16.3572 6.01025 16.5586 6.53394C16.7645 7.05762 16.8674 7.63725 16.8674 8.27283C16.8674 9.23067 16.6145 10.1818 16.1088 11.1262C15.603 12.0662 14.8891 12.9815 13.967 13.8722C13.0495 14.7584 11.9708 15.6044 10.731 16.41C10.6056 16.4906 10.4758 16.5622 10.3416 16.6249C10.2118 16.692 10.0976 16.7256 9.99915 16.7256Z"></path></g></svg></div>Liked by Sebastian Raschka, PhD</button></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"></div><div id="trigger45" aria-expanded="false" aria-haspopup="dialog" aria-controls="dialog46" aria-label="View more" class="pencraft pc-display-flex pc-reset triggerContainer-eX588u"><button tabindex="0" type="button" aria-label="Ellipsis" class="pencraft pc-reset pencraft trigger-j08Uop iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_quaternary-kpMibu"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-ellipsis"><circle cx="12" cy="12" r="1"></circle><circle cx="19" cy="12" r="1"></circle><circle cx="5" cy="12" r="1"></circle></svg></button></div></div><div class="comment-body"><p><span>Amazing article! This is evidence that we haven't hit a wall yet with LLMs as all these labs haven't converged to the same architectures.</span></p><p><span>Cohere Labs is also doing some great work for open source and have some interesting work. I feel a lot of people don't know who they are as they are trying to appeal to businesses/governments.</span></p><div role="button" class="show-all-toggle"><div class="show-all-toggle-label">Expand full comment</div></div></div><div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-8 pc-justifyContent-flex-start pc-alignItems-center pc-reset comment-actions withShareButton-hQzuEn"><span class="pencraft pc-reset color-pub-secondary-text-hGQ02T decoration-hover-underline-ClDVRM reset-IxiVJZ"><a href="javascript:void(0)" class="like-button"><div class="pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset"><div class="pencraft pc-display-flex pc-reset reaction-container"><svg role="img" width="24" height="24" viewBox="0 0 24 24" fill="#000000" stroke-width="1.8" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="animation"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><svg role="img" width="16" height="16" viewBox="0 0 24 24" fill="transparent" stroke-width="2" stroke="var(--color-fg-secondary-themed)" xmlns="http://www.w3.org/2000/svg" style="height: 16px; width: 16px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="transparent" stroke="var(--color-fg-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg></div><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA like-count">Like (5)</div></div></a></span><span class="pencraft pc-reset decoration-hover-underline-ClDVRM reset-IxiVJZ"><a class="pencraft pc-reset link-_X6et2 link-LIBpto"><div class="pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="var(--color-fg-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Reply</div></div></a></span><span class="pencraft pc-reset decoration-hover-underline-ClDVRM reset-IxiVJZ"><a class="pencraft pc-reset link-_X6et2 link-LIBpto"><div class="pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="var(--color-fg-secondary-themed)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">Share</div></div></a></span></div><div style="height: 0px; transition: height 250ms var(--animation-smooth), opacity 250ms var(--animation-smooth); display: block; flex-direction: column; opacity: 0;"><div style="padding-top: 0.05px; padding-bottom: 0.05px; display: block; flex: 0 0 auto; flex-direction: column; transition: transform 250ms var(--animation-smooth);"></div></div></div></div><div class="more-replies-container"><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison/comment/136865116" class="more-replies">1 reply by Sebastian Raschka, PhD</a></div></div></div></div><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison/comments" class="more-comments">58 more comments...</a></div></div></div><div class="single-post-section"><div class="container"><div class="visibility-check"></div><div aria-label="Top Posts Footer" role="region" class="pencraft pc-paddingTop-24 pc-paddingBottom-24 pc-reset" style="margin-left: -8px; margin-right: -8px;"><div class="portable-archive"><div aria-label="Archive sort tabs" role="navigation" class="pencraft pc-display-flex pc-gap-12 pc-paddingLeft-8 pc-paddingRight-8 pc-paddingBottom-16 pc-justifyContent-space-between pc-alignItems-center pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-position-relative pc-minWidth-0 pc-reset bg-primary-zk6FDl pc-borderRadius-sm overflow-hidden-WdpwT6"><div aria-label="Tabs" role="tablist" aria-orientation="horizontal" class="pencraft pc-display-flex pc-gap-4 pc-padding-4 pc-position-relative pc-reset cursor-default-flE2S1 outline-detail-vcQLyr pc-borderRadius-sm overflow-auto-7WTsTi scrollBar-hidden-HcAIpI"><button tabindex="0" type="button" id="headlessui-tabs-tab-P0-40" role="tab" aria-selected="true" data-headlessui-state="selected" class="pencraft pc-reset pencraft segment-LBFzmC buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_tertiary-rlke8z size_sm-G3LciD">Top</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-41" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft segment-LBFzmC buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_quaternary-kpMibu size_sm-G3LciD">Latest</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-42" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft segment-LBFzmC buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_quaternary-kpMibu size_sm-G3LciD">Discussions</button><div class="pencraft pc-position-absolute pc-height-32 pc-reset bg-secondary-UUD3_J pc-borderRadius-xs sizing-border-box-DggLA4 highlight-U002IP" style="--highlight-width: 39.11249923706055px; --highlight-x: 0px;"></div></div><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH left-Tg8vqp"><div class="overlay-zrMCxn primary-lv_sOW"></div></div><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH right-i3oWGi"><div class="overlay-zrMCxn primary-lv_sOW"></div></div></div><button tabindex="0" type="button" aria-label="Search" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg></button></div><div class="portable-archive-list"><div><div class="container-O1YsI6 two-column-list-BLHtzo two-column-list--with-dividers-cHfR0M"><div aria-label="Post preview for Understanding Reasoning LLMs" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="container-Qnseki"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-position-relative pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-3-lxFDfR reset-IxiVJZ" style="font-size: 19px; line-height: 26px;">Understanding Reasoning LLMs</a></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" class="pencraft pc-reset color-primary-zABazT line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">Methods and Strategies for Building and Refining Reasoning Models</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><time datetime="2025-02-05T12:11:39.216Z" class="date-rtYe1v">Feb 5</time>&nbsp;<span class="dividerChar-SbAJEi">•</span>&nbsp;<span class="pencraft pc-reset reset-IxiVJZ"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/@rasbt" class="link-HFGLqU">Sebastian Raschka, PhD</a></div></span></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset actions-YFg47u"><div class="post-ufi style-compressed justified themed"><div class="like-button-container post-ufi-button style-compressed"><a role="button" aria-label="Like (1,098)" aria-pressed="false" class="post-ufi-button style-compressed has-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><div class="label">1,098</div></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><a role="button" href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms/comments" aria-label="View comments (40)" class="post-ufi-button style-compressed post-ufi-comment-button has-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg></g></svg><div class="label">40</div></a><a role="button" class="post-ufi-button style-compressed no-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><path d="M21 3V8M21 8H16M21 8L18 5.29962C16.7056 4.14183 15.1038 3.38328 13.3879 3.11547C11.6719 2.84766 9.9152 3.08203 8.32951 3.79031C6.74382 4.49858 5.39691 5.65051 4.45125 7.10715C3.5056 8.5638 3.00158 10.2629 3 11.9996M3 21V16M3 16H8M3 16L6 18.7C7.29445 19.8578 8.89623 20.6163 10.6121 20.8841C12.3281 21.152 14.0848 20.9176 15.6705 20.2093C17.2562 19.501 18.6031 18.3491 19.5487 16.8925C20.4944 15.4358 20.9984 13.7367 21 12" stroke-linecap="round" stroke-linejoin="round"></path></g></svg></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div><a role="button" href="javascript:void(0)" aria-label="View share options" class="post-ufi-button style-compressed no-label with-border"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share icon"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg></a></div></div></div><div><div class="image-tkPTAj container-XxSyR3" style="aspect-ratio: 1.5 / 1;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QwUc!,w_320,h_213,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png"><img src="./The Big LLM Architecture Comparison_files/d6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830(1).jpg" sizes="(min-width:768px) 50vw, 100vw" alt="" width="320" height="213" loading="lazy" class="img-OACg1c image-nBNbRY pencraft pc-reset" style="aspect-ratio: 1.5 / 1;"></picture></div></div></div></div><div class="pencraft pc-display-flex pc-reset border-bottom-detail-themed-Ua9186 divider-QOeHtM"></div><div aria-label="Post preview for Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="container-Qnseki"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-position-relative pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-3-lxFDfR reset-IxiVJZ" style="font-size: 19px; line-height: 26px;">Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs</a></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention" class="pencraft pc-reset color-primary-zABazT line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama.</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><div class="icon-cvHqCn"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-lock"><rect width="18" height="11" x="3" y="11" rx="2" ry="2"></rect><path d="M7 11V7a5 5 0 0 1 10 0v4"></path></svg></div><time datetime="2024-01-14T11:55:06.449Z" class="date-rtYe1v">Jan 14, 2024</time></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset actions-YFg47u"><div class="post-ufi style-compressed justified themed"><div class="like-button-container post-ufi-button style-compressed"><a role="button" aria-label="Like (386)" aria-pressed="false" class="post-ufi-button style-compressed has-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><div class="label">386</div></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><a role="button" href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention/comments" aria-label="View comments (41)" class="post-ufi-button style-compressed post-ufi-comment-button has-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg></g></svg><div class="label">41</div></a><a role="button" class="post-ufi-button style-compressed no-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><path d="M21 3V8M21 8H16M21 8L18 5.29962C16.7056 4.14183 15.1038 3.38328 13.3879 3.11547C11.6719 2.84766 9.9152 3.08203 8.32951 3.79031C6.74382 4.49858 5.39691 5.65051 4.45125 7.10715C3.5056 8.5638 3.00158 10.2629 3 11.9996M3 21V16M3 16H8M3 16L6 18.7C7.29445 19.8578 8.89623 20.6163 10.6121 20.8841C12.3281 21.152 14.0848 20.9176 15.6705 20.2093C17.2562 19.501 18.6031 18.3491 19.5487 16.8925C20.4944 15.4358 20.9984 13.7367 21 12" stroke-linecap="round" stroke-linejoin="round"></path></g></svg></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div><a role="button" href="javascript:void(0)" aria-label="View share options" class="post-ufi-button style-compressed no-label with-border"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share icon"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg></a></div></div></div><div><div class="image-tkPTAj container-XxSyR3" style="aspect-ratio: 1.5 / 1;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3NS4!,w_320,h_213,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png"><img src="./The Big LLM Architecture Comparison_files/69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.jpg" sizes="(min-width:768px) 50vw, 100vw" alt="" width="320" height="213" loading="lazy" class="img-OACg1c image-nBNbRY pencraft pc-reset" style="aspect-ratio: 1.5 / 1;"></picture></div></div></div></div><div class="pencraft pc-display-flex pc-reset border-bottom-detail-themed-Ua9186 divider-QOeHtM"></div><div aria-label="Post preview for Understanding Large Language Models" role="article" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-relative pc-reset pc-borderRadius-sm container-H2dyKk"><div class="container-Qnseki"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-4 pc-position-relative pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models" data-testid="post-preview-title" class="pencraft pc-reset color-pub-primary-text-NyXPlw font-pub-headings-FE5byy clamp-y7pNm8 clamp-3-lxFDfR reset-IxiVJZ" style="font-size: 19px; line-height: 26px;">Understanding Large Language Models</a></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models" class="pencraft pc-reset color-primary-zABazT line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 clamp-y7pNm8 clamp-2-kM02pu reset-IxiVJZ">A Cross-Section of the Most Relevant Literature To Get Up to Speed</a></div><div class="pencraft pc-display-inline pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><time datetime="2023-04-16T12:33:46.854Z" class="date-rtYe1v">Apr 16, 2023</time>&nbsp;<span class="dividerChar-SbAJEi">•</span>&nbsp;<span class="pencraft pc-reset reset-IxiVJZ"><div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a href="https://substack.com/@rasbt" class="link-HFGLqU">Sebastian Raschka, PhD</a></div></span></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset actions-YFg47u"><div class="post-ufi style-compressed justified themed"><div class="like-button-container post-ufi-button style-compressed"><a role="button" aria-label="Like (922)" aria-pressed="false" class="post-ufi-button style-compressed has-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-heart"><path d="M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z"></path></svg></g></svg><div class="label">922</div></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><a role="button" href="https://magazine.sebastianraschka.com/p/understanding-large-language-models/comments" aria-label="View comments (53)" class="post-ufi-button style-compressed post-ufi-comment-button has-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="#000000" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg></g></svg><div class="label">53</div></a><a role="button" class="post-ufi-button style-compressed no-label with-border"><svg role="img" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="#000" xmlns="http://www.w3.org/2000/svg" class="icon" style="height: 14px; width: 14px;"><g><title></title><path d="M21 3V8M21 8H16M21 8L18 5.29962C16.7056 4.14183 15.1038 3.38328 13.3879 3.11547C11.6719 2.84766 9.9152 3.08203 8.32951 3.79031C6.74382 4.49858 5.39691 5.65051 4.45125 7.10715C3.5056 8.5638 3.00158 10.2629 3 11.9996M3 21V16M3 16H8M3 16L6 18.7C7.29445 19.8578 8.89623 20.6163 10.6121 20.8841C12.3281 21.152 14.0848 20.9176 15.6705 20.2093C17.2562 19.501 18.6031 18.3491 19.5487 16.8925C20.4944 15.4358 20.9984 13.7367 21 12" stroke-linecap="round" stroke-linejoin="round"></path></g></svg></a><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div><a role="button" href="javascript:void(0)" aria-label="View share options" class="post-ufi-button style-compressed no-label with-border"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share icon"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" x2="12" y1="2" y2="15"></line></svg></a></div></div></div><div><div class="image-tkPTAj container-XxSyR3" style="aspect-ratio: 1.5 / 1;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!RaNK!,w_320,h_213,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_center/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png"><img src="./The Big LLM Architecture Comparison_files/d9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.jpg" sizes="(min-width:768px) 50vw, 100vw" alt="" width="320" height="213" loading="lazy" class="img-OACg1c image-nBNbRY pencraft pc-reset" style="aspect-ratio: 1.5 / 1;"></picture></div></div></div></div></div></div><div class="pencraft pc-display-flex pc-paddingLeft-8 pc-paddingRight-8 pc-paddingTop-16 pc-reset"><button tabindex="0" type="button" data-testid="archive-view-all" data-href="/archive?sort=top" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_md-gCDS3o">See all<svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></button></div></div></div></div></div></div><div class="visibility-check"></div><div class="subscribe-footer"><div class="container"><p>Ready for more?</p><div><a href="https://magazine.sebastianraschka.com/subscribe?utm_source=ready-for-more" native="true" class="cta paid-cta"><button tabindex="0" type="button" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-RfbeYt size_md-gCDS3o"><b>Upgrade to paid</b></button></a></div></div></div></div></div></div><div class="footer-wrap publication-footer"><div class="visibility-check"></div><div class="footer themed-background"><div class="container"><div class="footer-blurbs"><div class="footer-copyright-blurb">© 2025 Raschka AI Research (RAIR) Lab LLC</div><div class="footer-terms-blurb"><a href="https://substack.com/privacy" target="_blank" rel="noopener" class="pencraft pc-reset decoration-underline-ClTkYc">Privacy</a><span> ∙ </span><a href="https://substack.com/tos" target="_blank" rel="noopener" class="pencraft pc-reset decoration-underline-ClTkYc">Terms</a><span> ∙ </span><a href="https://substack.com/ccpa#personal-data-collected" target="_blank" rel="noopener" class="pencraft pc-reset decoration-underline-ClTkYc">Collection notice</a></div></div><div class="footer-buttons"><a native="true" href="https://substack.com/signup?utm_source=substack&amp;utm_medium=web&amp;utm_content=footer" class="footer-substack-cta start-publishing"><svg role="img" width="1000" height="1000" viewBox="0 0 1000 1000" fill="#ff6719" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M764.166 348.371H236.319V419.402H764.166V348.371Z"></path><path d="M236.319 483.752V813.999L500.231 666.512L764.19 813.999V483.752H236.319Z"></path><path d="M764.166 213H236.319V284.019H764.166V213Z"></path></g></svg> Start writing</a><a native="true" href="https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&amp;utm_content=web-footer-button" class="footer-substack-cta get-the-app no-icon">Get the app</a></div><div translated="true" class="pencraft pc-reset reset-IxiVJZ footer-slogan-blurb"><a href="https://substack.com/" native="true">Substack</a> is the home for great culture</div></div></div></div><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup" style="z-index: 1001;"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><div class="pencraft pc-display-contents pc-reset pubAccentTheme-rgl9Hv"></div><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div><div style="left: auto; right: 16px; bottom: 16px; z-index: 1001; transform: translateY(0px);" role="region" aria-label="Notification" class="pencraft pc-position-fixed pc-reset sizing-border-box-DggLA4"></div><div><div inert="" role="dialog" class="modal typography out gone popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="profile-updater-modal"><div class="profile-updater"><h4 class="pencraft pc-paddingBottom-20 pc-reset align-left-PENAI6 line-height-24-jnGwiv font-display-nhmvtD size-20-P_cSRT weight-bold-DmI9lw reset-IxiVJZ">Create your profile</h4><div class="pencraft pc-display-flex pc-justifyContent-center pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-alignSelf-flex-start pc-reset"><button tabindex="0" type="button" aria-label="Edit profile photo" class="pencraft pc-reset pencraft buttonBase-GK1x3M"><div class="pencraft pc-display-flex pc-width-120 pc-height-120 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl border-detail-EGrm7T pc-borderRadius-full overflow-hidden-WdpwT6"><div class="pencraft pc-display-flex pc-width-120 pc-height-120 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 120px;"><div title="User" class="pencraft pc-display-flex pc-width-120 pc-height-120 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-secondary-UUD3_J flex-auto-j3S2WA outline-detail-vcQLyr pc-borderRadius-full overflow-hidden-WdpwT6 sizing-border-box-DggLA4 container-TAtrWj" style="--scale: 120px;"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!TnFC!,w_120,h_120,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 120w, https://substackcdn.com/image/fetch/$s_!TnFC!,w_240,h_240,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 240w, https://substackcdn.com/image/fetch/$s_!TnFC!,w_360,h_360,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 360w" sizes="120px"><img src="./The Big LLM Architecture Comparison_files/default-light.jpg" sizes="120px" alt="User&#39;s avatar" srcset="https://substackcdn.com/image/fetch/$s_!TnFC!,w_120,h_120,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 120w, https://substackcdn.com/image/fetch/$s_!TnFC!,w_240,h_240,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 240w, https://substackcdn.com/image/fetch/$s_!TnFC!,w_360,h_360,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png 360w" width="120" height="120" draggable="false" class="img-OACg1c object-fit-cover-u4ReeV pencraft pc-reset"></picture></div></div></div><input type="file" accept="image/*" aria-label="Upload file" tabindex="-1" class="sr-only"></button></div></div><form action="https://magazine.sebastianraschka.com/api/v1/user/profile" method="post" novalidate="" class="form "><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-20 pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><label for="name" class="pencraft pc-reset cursor-inherit-LxLBJ6 color-primary-zABazT line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-medium-fw81nC reset-IxiVJZ pencraft pencraft"><div class="pencraft pc-display-flex pc-gap-4 pc-alignItems-center pc-reset">Name<span class="pencraft pc-reset color-error-lkpu58 reset-IxiVJZ">*</span></div></label><div class="pencraft pc-display-flex pc-minWidth-0 pc-position-relative pc-reset flex-auto-j3S2WA"><input autofocus="true" placeholder="Type your name..." name="name" id="name" type="text" class="profile-name input-y4v6N4 inputText-pV_yWb"></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><label for="handle" class="pencraft pc-reset cursor-inherit-LxLBJ6 color-primary-zABazT line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-medium-fw81nC reset-IxiVJZ pencraft">Handle</label><div class="pencraft pc-display-flex pc-minWidth-0 pc-position-relative pc-reset flex-auto-j3S2WA"><input placeholder="Type your handle..." name="handle" id="handle" type="text" class="profile-name input-y4v6N4 inputText-pV_yWb"></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><label for="bio" class="pencraft pc-reset cursor-inherit-LxLBJ6 color-primary-zABazT line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-medium-fw81nC reset-IxiVJZ pencraft">Bio</label><textarea placeholder="Say something about yourself..." name="bio" id="bio" rows="4" class="pencraft textarea-GbEjRX inputText-pV_yWb"></textarea></div><input type="hidden" name="confirmation_redirect_pathname" value="/p/the-big-llm-architecture-comparison"><input type="hidden" name="photo_url"><input type="hidden" name="user_id"><input type="hidden" name="needs_photo" value="false"><input type="hidden" name="token"><div id="error-container"></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-paddingTop-20 pc-reset"><div class="pencraft pc-paddingBottom-12 pc-reset tosCheckboxContainer-vfMi_Y"><div class="visibility-check"></div><label class="pencraft pc-display-flex pc-gap-8 pc-justifyContent-center pc-alignItems-center pc-reset tosCheckbox-XbLWCT" style="display: flex;"><div class="tosCheckboxBox-KXqPaU"><div class="pencraft pc-reset pencraft"><div tabindex="-1" class="pencraft pc-display-flex pc-justifyContent-center pc-alignItems-center pc-boxShadow-xs pc-position-relative pc-reset flex-auto-j3S2WA animate-XFJxE4 cursor-pointer-LYORKw outline-detail-vcQLyr pc-borderRadius-xs sizing-border-box-DggLA4 pencraft checkbox-h4aRbX sm-rkc_jb enabled-BlbymI unchecked-ixpycg theme_accent-BLPCPx"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-check icon-JVHTJb"><path d="M20 6 9 17l-5-5"></path></svg><input type="checkbox" role="checkbox" data-state="unselected" aria-checked="false" tabindex="0" class="pencraft"></div></div></div><div class="pencraft pc-reset color-primary-zABazT align-left-PENAI6 line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-regular-mUq6Gb reset-IxiVJZ"> I agree to Substack's <a href="https://substack.com/tos" target="_blank" class="pencraft pc-reset reset-IxiVJZ" style="text-decoration: underline;">Terms of Use</a>, and acknowledge its <a href="https://substack.com/ccpa#personal-data-collected" target="_blank" class="pencraft pc-reset reset-IxiVJZ" style="text-decoration: underline;">Information Collection Notice</a> and <a href="https://substack.com/privacy" target="_blank" class="pencraft pc-reset reset-IxiVJZ" style="text-decoration: underline;">Privacy Policy</a>.</div></label></div><button tabindex="0" type="submit" disabled="" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-RfbeYt size_lg-A_bUNK">Save and post comment</button></div></form></div></div></div></div></div></div></div><div inert="" role="dialog" class="modal typography out gone popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div data-testid="paywall" data-component-name="Paywall" role="region" aria-label="Paywall" class="paywall modal-paywall"><div translated="true" class="pencraft pc-paddingBottom-0 pc-reset reset-IxiVJZ paywall-intro">Hi <b>ajaychaudhary8104@gmail.com</b></div><h2 class="paywall-title">Only paid subscribers can comment on this post</h2><div class="paywall-cta"><a href="https://magazine.sebastianraschka.com/subscribe?simple=true&amp;next=https%3A%2F%2Fmagazine.sebastianraschka.com%2Fp%2Fthe-big-llm-architecture-comparison&amp;utm_source=paywall&amp;utm_medium=web&amp;utm_content=168650848" native="true"><button tabindex="0" type="button" class="pencraft pc-reset pencraft subscribe-btn subscribeButton-LcKYi7 buttonBase-GK1x3M">Subscribe</button></a></div><div class="paywall-login"><a href="https://substack.com/sign-in?redirect=%2Fp%2Fthe-big-llm-architecture-comparison&amp;for_pub=sebastianraschka&amp;change_user=true" native="true">Already a paid subscriber? <b>Switch accounts</b></a></div></div></div></div></div></div></div></div>
            
        </div>

        
            <script src="./The Big LLM Architecture Comparison_files/6c2ff3e3828e4017b7faf7b63e24cdf8.min.js.download" crossorigin="anonymous"></script>
            <script>
                window.Sentry && window.Sentry.onLoad(function() {
                    window.Sentry.init({
                        environment: window._preloads.sentry_environment,
                        dsn: window._preloads.sentry_dsn,
                    })
                })
            </script>
        


        
        
        
        <script>window._preloads        = JSON.parse("{\"isEU\":false,\"language\":\"en\",\"country\":\"IN\",\"userLocale\":{\"language\":\"en\",\"region\":\"US\",\"source\":\"accept-language\"},\"base_url\":\"https://magazine.sebastianraschka.com\",\"stripe_publishable_key\":\"pk_live_51QfnARLDSWi1i85FBpvw6YxfQHljOpWXw8IKi5qFWEzvW8HvoD8cqTulR9UWguYbYweLvA16P7LN6WZsGdZKrNkE00uGbFaOE3\",\"captcha_site_key\":\"6LdYbsYZAAAAAIFIRh8X_16GoFRLIReh-e-q6qSa\",\"pub\":{\"apple_pay_disabled\":false,\"apex_domain\":null,\"author_id\":27393275,\"byline_images_enabled\":true,\"bylines_enabled\":true,\"chartable_token\":null,\"community_enabled\":true,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"cover_photo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/a845e33e-b40d-46af-bd79-df96459df6b7_917x450.png\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"custom_domain_optional\":false,\"custom_domain\":\"magazine.sebastianraschka.com\",\"default_comment_sort\":\"best_first\",\"default_coupon\":null,\"default_group_coupon\":\"278f5ac1\",\"default_show_guest_bios\":true,\"email_banner_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/4d4190ba-b99c-4c4c-a70b-3ff514dd086b_1100x238.png\",\"email_from_name\":null,\"email_from\":null,\"embed_tracking_disabled\":false,\"explicit\":false,\"expose_paywall_content_to_search_engines\":true,\"fb_pixel_id\":null,\"fb_site_verification_token\":null,\"flagged_as_spam\":false,\"founding_subscription_benefits\":[\"Deep appreciation for your extraordinarily generous support.\"],\"free_subscription_benefits\":[\"Receive new articles\"],\"ga_pixel_id\":null,\"google_site_verification_token\":null,\"google_tag_manager_token\":null,\"hero_image\":null,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"hide_intro_subtitle\":null,\"hide_intro_title\":null,\"hide_podcast_feed_link\":false,\"homepage_type\":\"newspaper\",\"id\":1174659,\"image_thumbnails_always_enabled\":false,\"invite_only\":false,\"language\":\"en\",\"logo_url_wide\":\"https://substackcdn.com/image/fetch/$s_!xQ0c!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5083e6d3-fbc9-4870-95b9-6e85d02f62a6_9366x2023.png\",\"logo_url\":\"https://substackcdn.com/image/fetch/$s_!96vs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"minimum_group_size\":2,\"moderation_enabled\":true,\"name\":\"Ahead of AI\",\"paid_subscription_benefits\":[\"Receive new articles and support the Ahead of AI magazine\",\"Earlier access to certain articles and occasional bonus articles\",\"Support independent research, writing, and compute costs\"],\"parsely_pixel_id\":null,\"payments_state\":\"enabled\",\"paywall_free_trial_enabled\":false,\"podcast_art_url\":null,\"paid_podcast_episode_art_url\":null,\"podcast_byline\":null,\"podcast_description\":null,\"podcast_enabled\":false,\"podcast_feed_url\":null,\"podcast_title\":null,\"post_preview_limit\":1000,\"primary_user_id\":27393275,\"require_clickthrough\":false,\"show_pub_podcast_tab\":false,\"show_recs_on_homepage\":true,\"subdomain\":\"sebastianraschka\",\"subscriber_invites\":0,\"support_email\":null,\"theme_var_background_pop\":\"#2096FF\",\"theme_var_color_links\":false,\"theme_var_cover_bg_color\":null,\"trial_end_override\":null,\"twitter_pixel_id\":null,\"type\":\"newsletter\",\"post_reaction_faces_enabled\":true,\"is_personal_mode\":false,\"plans\":[{\"id\":\"yearly60usd\",\"object\":\"plan\",\"active\":true,\"aggregate_usage\":null,\"amount\":6000,\"amount_decimal\":\"6000\",\"billing_scheme\":\"per_unit\",\"created\":1734874989,\"currency\":\"usd\",\"interval\":\"year\",\"interval_count\":1,\"livemode\":true,\"metadata\":{\"substack\":\"yes\"},\"meter\":null,\"nickname\":\"$60 a year\",\"product\":\"prod_RRis9CUfE0gdys\",\"tiers\":null,\"tiers_mode\":null,\"transform_usage\":null,\"trial_period_days\":null,\"usage_type\":\"licensed\",\"currency_options\":{\"aud\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":9500,\"unit_amount_decimal\":\"9500\"},\"brl\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":33000,\"unit_amount_decimal\":\"33000\"},\"cad\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":8500,\"unit_amount_decimal\":\"8500\"},\"chf\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":4800,\"unit_amount_decimal\":\"4800\"},\"dkk\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":38000,\"unit_amount_decimal\":\"38000\"},\"eur\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":5500,\"unit_amount_decimal\":\"5500\"},\"gbp\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":4400,\"unit_amount_decimal\":\"4400\"},\"mxn\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":112500,\"unit_amount_decimal\":\"112500\"},\"nok\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":60500,\"unit_amount_decimal\":\"60500\"},\"nzd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":10000,\"unit_amount_decimal\":\"10000\"},\"pln\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":22000,\"unit_amount_decimal\":\"22000\"},\"sek\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":57000,\"unit_amount_decimal\":\"57000\"},\"usd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":6000,\"unit_amount_decimal\":\"6000\"}}},{\"id\":\"monthly6usd\",\"object\":\"plan\",\"active\":true,\"aggregate_usage\":null,\"amount\":600,\"amount_decimal\":\"600\",\"billing_scheme\":\"per_unit\",\"created\":1734874989,\"currency\":\"usd\",\"interval\":\"month\",\"interval_count\":1,\"livemode\":true,\"metadata\":{\"substack\":\"yes\"},\"meter\":null,\"nickname\":\"$6 a month\",\"product\":\"prod_RRisuF7bHBIY1t\",\"tiers\":null,\"tiers_mode\":null,\"transform_usage\":null,\"trial_period_days\":null,\"usage_type\":\"licensed\",\"currency_options\":{\"aud\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1000,\"unit_amount_decimal\":\"1000\"},\"brl\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":3300,\"unit_amount_decimal\":\"3300\"},\"cad\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":900,\"unit_amount_decimal\":\"900\"},\"chf\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":500,\"unit_amount_decimal\":\"500\"},\"dkk\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":3800,\"unit_amount_decimal\":\"3800\"},\"eur\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":600,\"unit_amount_decimal\":\"600\"},\"gbp\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":500,\"unit_amount_decimal\":\"500\"},\"mxn\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":11500,\"unit_amount_decimal\":\"11500\"},\"nok\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":6500,\"unit_amount_decimal\":\"6500\"},\"nzd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1000,\"unit_amount_decimal\":\"1000\"},\"pln\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":2200,\"unit_amount_decimal\":\"2200\"},\"sek\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":6000,\"unit_amount_decimal\":\"6000\"},\"usd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":600,\"unit_amount_decimal\":\"600\"}}},{\"id\":\"founding10000usd\",\"name\":\"founding10000usd\",\"nickname\":\"founding10000usd\",\"active\":true,\"amount\":10000,\"currency\":\"usd\",\"interval\":\"year\",\"interval_count\":1,\"metadata\":{\"substack\":\"yes\",\"founding\":\"yes\",\"no_coupons\":\"yes\",\"short_description\":\"Founding plan\",\"short_description_english\":\"Founding plan\",\"minimum\":\"10000\",\"minimum_local\":{\"aud\":15500,\"brl\":53500,\"cad\":14000,\"chf\":8000,\"dkk\":63500,\"eur\":8500,\"gbp\":7500,\"mxn\":184000,\"nok\":99000,\"nzd\":17000,\"pln\":36500,\"sek\":93500,\"usd\":10000}},\"currency_options\":{\"aud\":{\"unit_amount\":15500,\"tax_behavior\":\"unspecified\"},\"brl\":{\"unit_amount\":53500,\"tax_behavior\":\"unspecified\"},\"cad\":{\"unit_amount\":14000,\"tax_behavior\":\"unspecified\"},\"chf\":{\"unit_amount\":8000,\"tax_behavior\":\"unspecified\"},\"dkk\":{\"unit_amount\":63500,\"tax_behavior\":\"unspecified\"},\"eur\":{\"unit_amount\":8500,\"tax_behavior\":\"unspecified\"},\"gbp\":{\"unit_amount\":7500,\"tax_behavior\":\"unspecified\"},\"mxn\":{\"unit_amount\":184000,\"tax_behavior\":\"unspecified\"},\"nok\":{\"unit_amount\":99000,\"tax_behavior\":\"unspecified\"},\"nzd\":{\"unit_amount\":17000,\"tax_behavior\":\"unspecified\"},\"pln\":{\"unit_amount\":36500,\"tax_behavior\":\"unspecified\"},\"sek\":{\"unit_amount\":93500,\"tax_behavior\":\"unspecified\"},\"usd\":{\"unit_amount\":10000,\"tax_behavior\":\"unspecified\"}}}],\"stripe_user_id\":\"acct_1Lu1X9C70OBNvDYI\",\"stripe_country\":\"US\",\"stripe_publishable_key\":\"pk_live_51Lu1X9C70OBNvDYItYBM7APq6Q7Fz9vANfiT7y1sPUHBShLA13s7wfHqcNbk7gFH9Tzi60lZUexbZf5DHiy6fvpe00q673wHzg\",\"stripe_platform_account\":\"US\",\"automatic_tax_enabled\":false,\"author_name\":\"Sebastian Raschka, PhD\",\"author_handle\":\"rasbt\",\"author_photo_url\":\"https://substackcdn.com/image/fetch/$s_!CfW_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"author_bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"twitter_screen_name\":\"rasbt\",\"has_custom_tos\":false,\"has_custom_privacy\":false,\"theme\":{\"background_pop_color\":\"#c5030c\",\"web_bg_color\":\"#ffffff\",\"cover_bg_color\":null,\"publication_id\":1174659,\"color_links\":null,\"font_preset_heading\":null,\"font_preset_body\":\"sans\",\"font_family_headings\":null,\"font_family_body\":null,\"font_family_ui\":null,\"font_size_body_desktop\":null,\"print_secondary\":null,\"custom_css_web\":null,\"custom_css_email\":null,\"home_hero\":\"newspaper\",\"home_posts\":\"custom\",\"home_show_top_posts\":true,\"hide_images_from_list\":false,\"home_hero_alignment\":\"left\",\"home_hero_show_podcast_links\":true,\"default_post_header_variant\":null},\"threads_v2_settings\":{\"photo_replies_enabled\":true,\"first_thread_email_sent_at\":null,\"create_thread_minimum_role\":\"paid\",\"activated_at\":null,\"reader_thread_notifications_enabled\":true,\"boost_free_subscriber_chat_preview_enabled\":true,\"push_suppression_enabled\":false},\"default_group_coupon_percent_off\":\"30.00\",\"pause_return_date\":null,\"has_posts\":true,\"has_recommendations\":true,\"first_post_date\":\"2022-11-04T19:43:47.949Z\",\"has_podcast\":false,\"has_free_podcast\":false,\"has_subscriber_only_podcast\":false,\"has_community_content\":true,\"rankingDetail\":\"Hundreds of paid subscribers\",\"rankingDetailFreeIncluded\":\"Hundreds of thousands of subscribers\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":100000,\"rankingDetailFreeSubscriberCount\":\"Over 132,000 subscribers\",\"rankingDetailByLanguage\":{\"de\":{\"rankingDetail\":\"Hunderte von Paid-Abonnenten\",\"rankingDetailFreeIncluded\":\"Hunderttausende von Abonnenten\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":100000,\"rankingDetailFreeSubscriberCount\":\"\u00DCber 132,000 Abonnenten\",\"freeSubscriberCount\":\"132,000\",\"freeSubscriberCountOrderOfMagnitude\":\"132K+\"},\"es\":{\"rankingDetail\":\"Cientos de suscriptores de pago\",\"rankingDetailFreeIncluded\":\"Cientos de miles de suscriptores\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":100000,\"rankingDetailFreeSubscriberCount\":\"M\u00E1s de 132,000 suscriptores\",\"freeSubscriberCount\":\"132,000\",\"freeSubscriberCountOrderOfMagnitude\":\"132K+\"},\"fr\":{\"rankingDetail\":\"Des centaines d'abonn\u00E9s payants\",\"rankingDetailFreeIncluded\":\"Des centaines de milliers d'abonn\u00E9s\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":100000,\"rankingDetailFreeSubscriberCount\":\"Plus de 132,000 abonn\u00E9s\",\"freeSubscriberCount\":\"132,000\",\"freeSubscriberCountOrderOfMagnitude\":\"132K+\"},\"pt\":{\"rankingDetail\":\"Centenas de subscritores pagos\",\"rankingDetailFreeIncluded\":\"Centenas de milhares de subscritores\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":100000,\"rankingDetailFreeSubscriberCount\":\"Mais de 132,000 subscritores\",\"freeSubscriberCount\":\"132,000\",\"freeSubscriberCountOrderOfMagnitude\":\"132K+\"},\"pt-br\":{\"rankingDetail\":\"Centenas de assinantes pagantes\",\"rankingDetailFreeIncluded\":\"Centenas de milhares de assinantes\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":100000,\"rankingDetailFreeSubscriberCount\":\"Mais de 132,000 assinantes\",\"freeSubscriberCount\":\"132,000\",\"freeSubscriberCountOrderOfMagnitude\":\"132K+\"},\"it\":{\"rankingDetail\":\"Centinaia di abbonati a pagamento\",\"rankingDetailFreeIncluded\":\"Centinaia di migliaia di abbonati\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":100000,\"rankingDetailFreeSubscriberCount\":\"Oltre 132,000 abbonati\",\"freeSubscriberCount\":\"132,000\",\"freeSubscriberCountOrderOfMagnitude\":\"132K+\"},\"en\":{\"rankingDetail\":\"Hundreds of paid subscribers\",\"rankingDetailFreeIncluded\":\"Hundreds of thousands of subscribers\",\"rankingDetailOrderOfMagnitude\":100,\"rankingDetailFreeIncludedOrderOfMagnitude\":100000,\"rankingDetailFreeSubscriberCount\":\"Over 132,000 subscribers\",\"freeSubscriberCount\":\"132,000\",\"freeSubscriberCountOrderOfMagnitude\":\"132K+\"}},\"freeSubscriberCount\":\"132,000\",\"freeSubscriberCountOrderOfMagnitude\":\"132K+\",\"author_bestseller_tier\":100,\"disable_monthly_subscriptions\":false,\"disable_annual_subscriptions\":false,\"hide_post_restacks\":false,\"notes_feed_enabled\":true,\"last_chat_post_at\":null,\"primary_profile_name\":\"Sebastian Raschka, PhD\",\"primary_profile_photo_url\":\"https://substackcdn.com/image/fetch/$s_!CfW_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"no_follow\":false,\"paywall_chat\":\"free\",\"sections\":[],\"multipub_migration\":null,\"navigationBarItems\":[{\"id\":\"2564b0cc-f5ae-4cd3-8459-425ead1af79b\",\"publication_id\":1174659,\"sibling_rank\":0,\"link_title\":null,\"link_url\":null,\"section_id\":null,\"post_id\":null,\"is_hidden\":false,\"standard_key\":\"about\",\"post_tag_id\":null,\"section\":null,\"postTag\":null,\"post\":null},{\"id\":\"8973f8de-0b4a-43d3-ada7-b259e60f4eb1\",\"publication_id\":1174659,\"sibling_rank\":0,\"link_title\":\"Support\",\"link_url\":\"\",\"section_id\":null,\"post_id\":141448215,\"is_hidden\":null,\"standard_key\":null,\"post_tag_id\":null,\"section\":null,\"postTag\":null,\"post\":{\"id\":141448215,\"publication_id\":1174659,\"is_published\":true,\"title\":\"Support Independent AI Research\",\"body\":\"s3://substack-content/post/141448215/2025-08-29T00-46-24-124Z/27393275/faf0948552c9754e74c54763c646673727a15683\",\"slug\":\"supporting-ahead-of-ai\",\"post_date\":\"2024-02-07T01:47:55.040Z\",\"draft_title\":\"Support Independent AI Research\",\"draft_body\":\"s3://substack-content/post/141448215/2025-08-29T00-46-24-124Z/27393275/faf0948552c9754e74c54763c646673727a15683\",\"draft_updated_at\":\"2025-08-29T00:46:24.197Z\",\"subtitle\":\"\",\"draft_subtitle\":\"\",\"email_sent_at\":null,\"audience\":\"everyone\",\"type\":\"page\",\"podcast_url\":\"\",\"draft_podcast_url\":\"\",\"podcast_duration\":null,\"draft_podcast_duration\":null,\"podcast_art_url\":null,\"podcast_description\":null,\"podcast_subtitle\":null,\"explicit\":null,\"podcast_content\":null,\"podcast_guid\":null,\"social_title\":null,\"description\":null,\"cover_image\":null,\"imported_podcast_url\":null,\"imported_podcast_art_url\":null,\"uuid\":\"1b287a40-f5d3-402d-a231-ae1f487a5ba0\",\"write_comment_permissions\":\"everyone\",\"should_send_email\":false,\"default_comment_sort\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"updated_at\":\"2025-08-29T00:46:31.261Z\",\"canonical_url\":null,\"subscriber_set_id\":null,\"section_id\":null,\"section_chosen\":false,\"draft_section_id\":null,\"show_guest_bios\":true,\"reply_to_post_id\":null,\"should_send_free_preview\":false,\"word_count\":1255,\"video_upload_id\":null,\"draft_video_upload_id\":null,\"draft_created_at\":\"2024-02-07T01:30:32.546Z\",\"podcast_upload_id\":null,\"draft_podcast_upload_id\":null,\"voiceover_upload_id\":null,\"draft_voiceover_upload_id\":null,\"free_unlock_required\":false,\"podcast_preview_upload_id\":null,\"draft_podcast_preview_upload_id\":null,\"legacy_podcast_file_size\":null,\"syndicate_voiceover_to_rss\":false,\"audience_before_archived\":null,\"should_send_stats_email\":true,\"exempt_from_archive_paywall\":false,\"has_explicit_paywall\":false,\"inbox_sent_at\":null,\"editor_v2\":false,\"teaser_post_eligible\":true,\"has_dismissed_tk_warning\":false,\"live_stream_id\":null,\"is_draft_hidden\":false,\"meter_type\":\"none\"}},{\"id\":\"0a3636ba-c018-452f-ad43-d429671380dd\",\"publication_id\":1174659,\"sibling_rank\":2,\"link_title\":\"LLMs From Scratch Book\",\"link_url\":\"https://amzn.to/4fqvn0D\",\"section_id\":null,\"post_id\":null,\"is_hidden\":null,\"standard_key\":null,\"post_tag_id\":null,\"section\":null,\"postTag\":null,\"post\":null},{\"id\":\"af81fc3a-7ffc-46da-bc8a-8bedc3c5b31d\",\"publication_id\":1174659,\"sibling_rank\":3,\"link_title\":\"Reasoning From Scratch Book\",\"link_url\":\"https://mng.bz/Ewrj\",\"section_id\":null,\"post_id\":null,\"is_hidden\":null,\"standard_key\":null,\"post_tag_id\":null,\"section\":null,\"postTag\":null,\"post\":null}],\"contributors\":[{\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"role\":\"admin\",\"owner\":true,\"user_id\":27393275,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\"}],\"threads_v2_enabled\":true,\"viralGiftsConfig\":{\"id\":\"2490f013-78b6-4c3a-8f8f-fad76b54c918\",\"publication_id\":1174659,\"enabled\":true,\"gifts_per_user\":5,\"gift_length_months\":1,\"send_extra_gifts\":true,\"message\":\"Machine Learning & AI trends, discussions, and educational contents to stay ahead of the field!\",\"created_at\":\"2022-11-22T16:45:58.850743+00:00\",\"updated_at\":\"2022-11-22T16:45:58.850743+00:00\",\"days_til_invite\":14,\"send_emails\":true,\"show_link\":null,\"grant_email_body\":null,\"grant_email_subject\":null},\"tier\":2,\"no_index\":false,\"can_set_google_site_verification\":true,\"can_have_sitemap\":true,\"draft_iap_advanced_plans\":[{\"sku\":\"rkFp9Ivlejn3FqgFkE\",\"publication_id\":\"1174659\",\"is_active\":true,\"price_base_units\":800,\"currency_alpha3\":\"usd\",\"period\":\"month\",\"created_at\":\"2025-04-05T02:15:57.295Z\",\"updated_at\":\"2025-04-05T02:15:57.295Z\",\"id\":\"4711\",\"payout_amount_base_units\":60,\"alternate_currencies\":{\"aud\":1300,\"brl\":4600,\"cad\":1200,\"chf\":700,\"dkk\":5500,\"eur\":800,\"gbp\":700,\"mxn\":16000,\"nok\":8500,\"nzd\":1400,\"pln\":3100,\"sek\":8000},\"display_name\":\"Ahead of AI (Monthly)\",\"display_price\":\"$8\"},{\"sku\":\"GVXZP2sSCQTJI0cybm\",\"publication_id\":\"1174659\",\"is_active\":true,\"price_base_units\":8000,\"currency_alpha3\":\"usd\",\"period\":\"year\",\"created_at\":\"2025-04-05T02:15:57.316Z\",\"updated_at\":\"2025-04-05T02:15:57.316Z\",\"id\":\"4712\",\"payout_amount_base_units\":600,\"alternate_currencies\":{\"aud\":13000,\"brl\":45500,\"cad\":11500,\"chf\":7000,\"dkk\":54500,\"eur\":7500,\"gbp\":6500,\"mxn\":160000,\"nok\":83000,\"nzd\":14000,\"pln\":31000,\"sek\":78500},\"display_name\":\"Ahead of AI (Yearly)\",\"display_price\":\"$80\"}],\"iap_advanced_plans\":[{\"sku\":\"rkFp9Ivlejn3FqgFkE\",\"publication_id\":\"1174659\",\"is_active\":true,\"price_base_units\":800,\"currency_alpha3\":\"usd\",\"period\":\"month\",\"created_at\":\"2025-04-05T02:15:57.295Z\",\"updated_at\":\"2025-04-05T02:15:57.295Z\",\"id\":\"4711\",\"payout_amount_base_units\":60,\"alternate_currencies\":{\"aud\":1300,\"brl\":4600,\"cad\":1200,\"chf\":700,\"dkk\":5500,\"eur\":800,\"gbp\":700,\"mxn\":16000,\"nok\":8500,\"nzd\":1400,\"pln\":3100,\"sek\":8000},\"display_name\":\"Ahead of AI (Monthly)\",\"display_price\":\"$8\"},{\"sku\":\"GVXZP2sSCQTJI0cybm\",\"publication_id\":\"1174659\",\"is_active\":true,\"price_base_units\":8000,\"currency_alpha3\":\"usd\",\"period\":\"year\",\"created_at\":\"2025-04-05T02:15:57.316Z\",\"updated_at\":\"2025-04-05T02:15:57.316Z\",\"id\":\"4712\",\"payout_amount_base_units\":600,\"alternate_currencies\":{\"aud\":13000,\"brl\":45500,\"cad\":11500,\"chf\":7000,\"dkk\":54500,\"eur\":7500,\"gbp\":6500,\"mxn\":160000,\"nok\":83000,\"nzd\":14000,\"pln\":31000,\"sek\":78500},\"display_name\":\"Ahead of AI (Yearly)\",\"display_price\":\"$80\"}],\"founding_plan_name_english\":\"Founding plan\",\"draft_plans\":[{\"id\":\"yearly60usd\",\"object\":\"plan\",\"active\":true,\"aggregate_usage\":null,\"amount\":6000,\"amount_decimal\":\"6000\",\"billing_scheme\":\"per_unit\",\"created\":1734874989,\"currency\":\"usd\",\"interval\":\"year\",\"interval_count\":1,\"livemode\":true,\"metadata\":{\"substack\":\"yes\"},\"meter\":null,\"nickname\":\"$60 a year\",\"product\":\"prod_RRis9CUfE0gdys\",\"tiers\":null,\"tiers_mode\":null,\"transform_usage\":null,\"trial_period_days\":null,\"usage_type\":\"licensed\",\"currency_options\":{\"aud\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":9500,\"unit_amount_decimal\":\"9500\"},\"brl\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":33000,\"unit_amount_decimal\":\"33000\"},\"cad\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":8500,\"unit_amount_decimal\":\"8500\"},\"chf\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":4800,\"unit_amount_decimal\":\"4800\"},\"dkk\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":38000,\"unit_amount_decimal\":\"38000\"},\"eur\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":5500,\"unit_amount_decimal\":\"5500\"},\"gbp\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":4400,\"unit_amount_decimal\":\"4400\"},\"mxn\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":112500,\"unit_amount_decimal\":\"112500\"},\"nok\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":60500,\"unit_amount_decimal\":\"60500\"},\"nzd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":10000,\"unit_amount_decimal\":\"10000\"},\"pln\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":22000,\"unit_amount_decimal\":\"22000\"},\"sek\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":57000,\"unit_amount_decimal\":\"57000\"},\"usd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":6000,\"unit_amount_decimal\":\"6000\"}}},{\"id\":\"monthly6usd\",\"object\":\"plan\",\"active\":true,\"aggregate_usage\":null,\"amount\":600,\"amount_decimal\":\"600\",\"billing_scheme\":\"per_unit\",\"created\":1734874989,\"currency\":\"usd\",\"interval\":\"month\",\"interval_count\":1,\"livemode\":true,\"metadata\":{\"substack\":\"yes\"},\"meter\":null,\"nickname\":\"$6 a month\",\"product\":\"prod_RRisuF7bHBIY1t\",\"tiers\":null,\"tiers_mode\":null,\"transform_usage\":null,\"trial_period_days\":null,\"usage_type\":\"licensed\",\"currency_options\":{\"aud\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1000,\"unit_amount_decimal\":\"1000\"},\"brl\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":3300,\"unit_amount_decimal\":\"3300\"},\"cad\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":900,\"unit_amount_decimal\":\"900\"},\"chf\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":500,\"unit_amount_decimal\":\"500\"},\"dkk\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":3800,\"unit_amount_decimal\":\"3800\"},\"eur\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":600,\"unit_amount_decimal\":\"600\"},\"gbp\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":500,\"unit_amount_decimal\":\"500\"},\"mxn\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":11500,\"unit_amount_decimal\":\"11500\"},\"nok\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":6500,\"unit_amount_decimal\":\"6500\"},\"nzd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":1000,\"unit_amount_decimal\":\"1000\"},\"pln\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":2200,\"unit_amount_decimal\":\"2200\"},\"sek\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":6000,\"unit_amount_decimal\":\"6000\"},\"usd\":{\"custom_unit_amount\":null,\"tax_behavior\":\"unspecified\",\"unit_amount\":600,\"unit_amount_decimal\":\"600\"}}},{\"id\":\"founding10000usd\",\"name\":\"founding10000usd\",\"nickname\":\"founding10000usd\",\"active\":true,\"amount\":10000,\"currency\":\"usd\",\"interval\":\"year\",\"interval_count\":1,\"metadata\":{\"substack\":\"yes\",\"founding\":\"yes\",\"no_coupons\":\"yes\",\"short_description\":\"Founding plan\",\"short_description_english\":\"Founding plan\",\"minimum\":\"10000\",\"minimum_local\":{\"aud\":15500,\"brl\":53500,\"cad\":14000,\"chf\":8000,\"dkk\":63500,\"eur\":8500,\"gbp\":7500,\"mxn\":184000,\"nok\":99000,\"nzd\":17000,\"pln\":36500,\"sek\":93500,\"usd\":10000}},\"currency_options\":{\"aud\":{\"unit_amount\":15500,\"tax_behavior\":\"unspecified\"},\"brl\":{\"unit_amount\":53500,\"tax_behavior\":\"unspecified\"},\"cad\":{\"unit_amount\":14000,\"tax_behavior\":\"unspecified\"},\"chf\":{\"unit_amount\":8000,\"tax_behavior\":\"unspecified\"},\"dkk\":{\"unit_amount\":63500,\"tax_behavior\":\"unspecified\"},\"eur\":{\"unit_amount\":8500,\"tax_behavior\":\"unspecified\"},\"gbp\":{\"unit_amount\":7500,\"tax_behavior\":\"unspecified\"},\"mxn\":{\"unit_amount\":184000,\"tax_behavior\":\"unspecified\"},\"nok\":{\"unit_amount\":99000,\"tax_behavior\":\"unspecified\"},\"nzd\":{\"unit_amount\":17000,\"tax_behavior\":\"unspecified\"},\"pln\":{\"unit_amount\":36500,\"tax_behavior\":\"unspecified\"},\"sek\":{\"unit_amount\":93500,\"tax_behavior\":\"unspecified\"},\"usd\":{\"unit_amount\":10000,\"tax_behavior\":\"unspecified\"}}}],\"base_url\":\"https://magazine.sebastianraschka.com\",\"hostname\":\"magazine.sebastianraschka.com\",\"is_on_substack\":false,\"spotify_podcast_settings\":null,\"podcastPalette\":{\"DarkMuted\":{\"population\":72,\"rgb\":[73,153,137]},\"DarkVibrant\":{\"population\":6013,\"rgb\":[4,100,84]},\"LightMuted\":{\"population\":7,\"rgb\":[142,198,186]},\"LightVibrant\":{\"population\":3,\"rgb\":[166,214,206]},\"Muted\":{\"population\":6,\"rgb\":[92,164,156]},\"Vibrant\":{\"population\":5,\"rgb\":[76,164,146]}},\"pageThemes\":{\"podcast\":null},\"appTheme\":{\"colors\":{\"accent\":{\"name\":\"#c5030c\",\"primary\":{\"r\":197,\"g\":3,\"b\":12,\"a\":1},\"primary_hover\":{\"r\":174,\"g\":0,\"b\":0,\"a\":1},\"primary_elevated\":{\"r\":174,\"g\":0,\"b\":0,\"a\":1},\"secondary\":{\"r\":197,\"g\":3,\"b\":12,\"a\":0.2},\"contrast\":{\"r\":255,\"g\":255,\"b\":255,\"a\":1},\"bg\":{\"r\":197,\"g\":3,\"b\":12,\"a\":0.2},\"bg_hover\":{\"r\":197,\"g\":3,\"b\":12,\"a\":0.3},\"dark\":{\"primary\":{\"r\":197,\"g\":3,\"b\":12,\"a\":1},\"primary_hover\":{\"r\":219,\"g\":43,\"b\":29,\"a\":1},\"primary_elevated\":{\"r\":219,\"g\":43,\"b\":29,\"a\":1},\"secondary\":{\"r\":197,\"g\":3,\"b\":12,\"a\":0.2},\"contrast\":{\"r\":255,\"g\":255,\"b\":255,\"a\":1},\"bg\":{\"r\":197,\"g\":3,\"b\":12,\"a\":0.2},\"bg_hover\":{\"r\":197,\"g\":3,\"b\":12,\"a\":0.3}}},\"fg\":{\"primary\":{\"r\":0,\"g\":0,\"b\":0,\"a\":0.8},\"secondary\":{\"r\":0,\"g\":0,\"b\":0,\"a\":0.6},\"tertiary\":{\"r\":0,\"g\":0,\"b\":0,\"a\":0.4},\"accent\":{\"r\":197,\"g\":3,\"b\":12,\"a\":1},\"dark\":{\"primary\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0.9},\"secondary\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0.6},\"tertiary\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0.4},\"accent\":{\"r\":239,\"g\":64,\"b\":43,\"a\":1}}},\"bg\":{\"name\":\"#ffffff\",\"hue\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0},\"tint\":{\"r\":255,\"g\":255,\"b\":255,\"a\":0},\"primary\":{\"r\":255,\"g\":255,\"b\":255,\"a\":1},\"primary_hover\":{\"r\":250,\"g\":250,\"b\":250,\"a\":1},\"primary_elevated\":{\"r\":250,\"g\":250,\"b\":250,\"a\":1},\"secondary\":{\"r\":238,\"g\":238,\"b\":238,\"a\":1},\"secondary_elevated\":{\"r\":206.90096477355226,\"g\":206.90096477355175,\"b\":206.9009647735519,\"a\":1},\"tertiary\":{\"r\":219,\"g\":219,\"b\":219,\"a\":1},\"quaternary\":{\"r\":182,\"g\":182,\"b\":182,\"a\":1},\"dark\":{\"primary\":{\"r\":22,\"g\":23,\"b\":24,\"a\":1},\"primary_hover\":{\"r\":27,\"g\":28,\"b\":29,\"a\":1},\"primary_elevated\":{\"r\":27,\"g\":28,\"b\":29,\"a\":1},\"secondary\":{\"r\":35,\"g\":37,\"b\":37,\"a\":1},\"secondary_elevated\":{\"r\":41.35899397549579,\"g\":43.405356429195315,\"b\":43.40489285041963,\"a\":1},\"tertiary\":{\"r\":54,\"g\":55,\"b\":55,\"a\":1},\"quaternary\":{\"r\":90,\"g\":91,\"b\":91,\"a\":1}}}},\"cover_image\":{\"url\":\"https://substackcdn.com/image/fetch/$s_!LheM!,w_1200,h_400,c_crop,f_auto,q_auto:best,fl_progressive:steep,g_auto,b_rgb:FFFFFF/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa845e33e-b40d-46af-bd79-df96459df6b7_917x450.png\",\"height\":450,\"width\":917}},\"live_subscriber_counts\":false,\"logoPalette\":{\"Vibrant\":{\"rgb\":[236,44,52],\"population\":630},\"DarkVibrant\":{\"rgb\":[121.64608695652176,10.95391304347825,15.566086956521787],\"population\":0},\"LightVibrant\":{\"rgb\":[232,148,148],\"population\":49},\"Muted\":{\"rgb\":[140.36086956521737,12.639130434782615,17.96086956521746],\"population\":0},\"DarkMuted\":{\"rgb\":[140.36086956521737,12.639130434782615,17.96086956521746],\"population\":0},\"LightMuted\":{\"rgb\":[246,237,238],\"population\":412}}},\"confirmedLogin\":false,\"freeSignupUserId\":391278817,\"freeSignupEmail\":\"ajaychaudhary8104@gmail.com\",\"freeSignup\":true,\"hide_intro_popup\":true,\"block_auto_login\":false,\"domainInfo\":{\"isSubstack\":false,\"customDomain\":\"magazine.sebastianraschka.com\"},\"experimentFeatures\":{\"notes_ranking_v86\":\"treatment\",\"android_vertical_post_player_2\":\"control\",\"publication_ranking_v16\":\"control\",\"dpn_user_picture_instead_of_comment\":\"treatment\",\"related_notes_variations\":\"treatment_explore_button\",\"publication_search_replatform\":\"treatment\",\"search_retrieval_v2\":\"treatment\"},\"experimentExposures\":{},\"siteConfigs\":{\"score_upsell_email\":\"experiment\",\"first_chat_email_enabled\":true,\"notes_video_max_duration_minutes\":5,\"reader-onboarding-promoted-pub\":737237,\"new_commenter_approval\":false,\"pub_update_opennode_api_key\":false,\"enable_user_report_review\":true,\"zendesk_automation_cancellations\":false,\"hide_book_a_meeting_button\":false,\"mfa_action_box_enabled\":false,\"publication_max_bylines\":35,\"no_contest_charge_disputes\":false,\"feed_posts_previously_seen_weight\":0.1,\"publication_tabs_reorder\":false,\"comp_expiry_email_new_copy\":\"NONE\",\"free_unlock_required\":false,\"traffic_rule_check_enabled\":false,\"amp_emails_enabled\":false,\"enable_post_summarization\":false,\"live_stream_host_warning_message\":\"\",\"bitcoin_enabled\":false,\"minimum_ios_os_version\":\"17.0.0\",\"show_entire_square_image\":false,\"hide_subscriber_count\":false,\"publication_author_display_override\":\"\",\"ios_webview_payments_enabled\":\"control\",\"generate_pdf_tax_report\":false,\"dpn_weight_disable\":5,\"show_generic_post_importer\":false,\"enable_pledges_modal\":true,\"include_pdf_invoice\":false,\"app_upsell_after_posting_notes\":\"experiment\",\"notes_weight_watch_video\":5,\"use_post_podcast_import_batching\":true,\"meetings_v1\":false,\"exempt_from_gtm_filter\":false,\"group_sections_and_podcasts_in_menu\":false,\"boost_optin_modal_enabled\":true,\"standards_and_enforcement_features_enabled\":false,\"pub_creation_captcha_behavior\":\"risky_pubs_or_rate_limit\",\"post_blogspot_importer\":false,\"suggested_search_metadata_web_ui\":false,\"notes_weight_short_item_boost\":0.15,\"pub_tts_override\":\"default\",\"disable_monthly_subscriptions\":false,\"skip_welcome_email\":false,\"chat_reader_thread_notification_default\":false,\"scheduled_pinned_posts\":false,\"disable_redirect_outbound_utm_params\":false,\"reader_gift_referrals_enabled\":true,\"enable_bestseller_survey_modal\":false,\"dont_show_guest_byline\":false,\"like_comments_enabled\":true,\"subscription_bar_all_debug_enabled\":false,\"temporal_livestream_ended_draft\":true,\"enable_author_note_email_toggle\":false,\"meetings_embed_publication_name\":false,\"fallback_to_archive_search_on_section_pages\":false,\"livekit_track_egress_custom_base_url\":\"http://livekit-egress-custom-recorder-participant-test.s3-website-us-east-1.amazonaws.com\",\"people_you_may_know_algorithm\":\"experiment\",\"welcome_screen_blurb_override\":\"\",\"live_stream_guest_overlay\":\"control\",\"like_posts_enabled\":true,\"apply_crm_overlay_updates\":true,\"twitter_player_card_enabled\":true,\"feed_promoted_user\":false,\"writer_beta_android_enable_post_editor_v2\":false,\"show_note_stats_for_all_notes\":false,\"section_specific_csv_imports_enabled\":false,\"disable_podcast_feed_description_cta\":false,\"bypass_profile_substack_logo_detection\":false,\"use_preloaded_player_sources\":false,\"list_pruning_enabled\":false,\"facebook_connect\":false,\"opt_in_to_sections_during_subscribe\":false,\"dpn_weight_share\":2,\"underlined_colored_links\":false,\"modal_rec_variant_user\":\"control\",\"extract_stripe_receipt_url\":false,\"enable_aligned_images\":false,\"max_image_upload_mb\":64,\"enable_android_dms_writer_beta\":false,\"threads_suggested_ios_version\":null,\"pledges_disabled\":false,\"threads_minimum_ios_version\":812,\"hide_podcast_email_setup_link\":false,\"subscribe_captcha_behavior\":\"default\",\"publication_ban_sample_rate\":0,\"grant_viral_gifts_to_gift_recipients\":\"experiment\",\"ios_enable_publication_activity_tab\":false,\"custom_themes_substack_subscribe_modal\":false,\"share_viral_gift_as_link\":\"experiment\",\"opt_in_to_sections_during_subscribe_include_main_pub_newsletter\":false,\"continue_support_cta_in_newsletter_emails\":false,\"bloomberg_syndication_enabled\":false,\"lists_enabled\":false,\"ios_feed_media_content_mode\":\"fit\",\"generated_database_maintenance_mode\":false,\"allow_document_freeze\":false,\"subscription_bar_all_debug_subdomains\":null,\"podcast_main_feed_is_firehose\":false,\"pub_app_incentive_gift\":\"\",\"no_embed_redirect\":false,\"translate_mobile_app\":false,\"customized_email_from_name_for_new_follow_emails\":\"treatment\",\"spotify_open_access_sandbox_mode\":false,\"partner_data_api_enabled\":false,\"fullstory_enabled\":false,\"chat_reply_poll_interval\":3,\"dpn_weight_follow_or_subscribe\":3,\"enable_reader_marketing_page\":false,\"force_pub_links_to_use_subdomain\":false,\"always_show_cookie_banner\":false,\"hide_media_download_option\":false,\"hide_post_restacks\":false,\"feed_item_source_debug_mode\":false,\"writer_beta_android_enable_post_editor\":false,\"thefp_enable_account_menu\":false,\"enable_user_status_ui\":false,\"publication_homepage_title_display_override\":\"\",\"pub_banned_word_list\":\"\",\"post_preview_highlight_byline\":false,\"4k_video\":false,\"enable_islands_section_intent_screen\":false,\"tfp_free_week_reg_wall\":false,\"post_metering_enabled\":false,\"notifications_disabled\":\"\",\"cross_post_notification_threshold\":1000,\"facebook_connect_prod_app\":true,\"feed_enable_live_streams\":false,\"force_into_pymk_ranking\":false,\"minimum_android_version\":756,\"live_stream_krisp_noise_suppression_enabled\":false,\"enable_transcription_translations\":false,\"ios_post_video_pager_alpha_enabled\":false,\"use_og_image_as_twitter_image_for_post_previews\":false,\"always_use_podcast_channel_art_as_episode_art_in_rss\":false,\"cookie_preference_middleware_enabled\":false,\"seo_tier_override\":\"NONE\",\"no_follow_links\":false,\"publisher_api_enabled\":false,\"zendesk_support_priority\":\"default\",\"enable_post_clips_stats\":false,\"enable_subscriber_referrals_awards\":true,\"ios_profile_themes_feed_permalink_enabled\":false,\"use_publication_language_for_transcription\":false,\"show_substack_funded_gifts_tooltip\":true,\"disable_ai_transcription\":false,\"thread_permalink_preview_min_ios_version\":4192,\"android_toggle_on_website_enabled\":false,\"internal_android_enable_post_editor\":false,\"edit_profile_feed_item\":false,\"updated_inbox_ui\":false,\"web_reader_podcasts_tab\":false,\"use_temporal_thumbnail_selection_workflow\":false,\"live_stream_creation_enabled\":false,\"disable_card_element_in_europe\":false,\"enable_web_typing_indicators\":false,\"web_vitals_sample_rate\":10,\"allow_live_stream_auto_takedown\":\"true\",\"search_ranker_variant\":\"control\",\"post_advanced_search\":\"control\",\"ai_image_generation_enabled\":true,\"disable_personal_substack_initialization\":false,\"section_specific_welcome_pages\":false,\"local_payment_methods\":\"control\",\"activity_items_reads_rollout\":0,\"posts_in_rss_feed\":20,\"post_rec_endpoint\":\"\",\"publisher_dashboard_section_selector\":false,\"reader_surveys_platform_question_order\":\"36,1,4,2,3,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35\",\"ios_toggle_on_website_enabled\":false,\"login_guard_app_link_in_email\":true,\"modal_rec_variant_content\":\"control\",\"monthly_sub_is_one_off\":false,\"unread_notes_activity_digest\":\"control\",\"display_cookie_settings\":false,\"welcome_page_query_params\":false,\"enable_free_podcast_urls\":false,\"enable_linkedin_oauth\":false,\"comp_expiry_emails_disabled\":false,\"enable_description_on_polls\":false,\"use_microlink_for_instagram_embeds\":false,\"post_notification_batch_delay_ms\":30000,\"free_signup_confirmation_behavior\":\"with_email_validation\",\"ios_post_stats_for_admins\":false,\"live_stream_concurrent_viewer_count_drawer\":false,\"use_livestream_post_media_composition\":true,\"section_specific_preambles\":false,\"android_live_stream_multihost_enabled\":false,\"show_menu_on_posts\":false,\"app_upsell_follow_prompt\":\"control\",\"ios_post_subscribe_web_routing\":true,\"opt_into_all_trending_topics\":false,\"ios_writer_stats_public_launch_v2\":false,\"min_size_for_phishing_check\":1,\"enable_android_post_stats\":false,\"ios_chat_revamp_enabled\":false,\"app_onboarding_survey_email\":false,\"post_notification_batch_chunk_size\":100,\"thefp_enable_pullquote_alignment\":false,\"thefp_enable_pullquote_color\":false,\"republishing_enabled\":false,\"app_mode\":false,\"show_phone_banner\":false,\"live_stream_video_enhancer\":\"internal\",\"enable_author_pages\":false,\"enable_decagon_chat\":true,\"first_month_upsell\":\"experiment\",\"enable_fedcm\":false,\"new_user_checklist_enabled\":\"use_follower_count\",\"ios_trending_topic_note_badge\":\"experiment\",\"enable_updated_webview_checkout\":false,\"show_attached_profile_for_pub_setting\":false,\"welcome_page_update_desktop_visuals_limited\":\"experiment\",\"rss_verification_code\":\"\",\"notification_post_emails\":\"experiment\",\"ios_profile_subdomain_chips\":true,\"chat_suppress_contributor_push_option_enabled\":false,\"live_stream_invite_ttl_seconds\":600,\"feed_ranking_per_post_clip_cap\":2,\"export_hooks_enabled\":false,\"audio_encoding_bitrate\":null,\"bestseller_pub_override\":false,\"extra_seats_coupon_type\":false,\"post_subdomain_universal_links\":false,\"post_import_max_file_size\":26214400,\"notes_weight_follow\":4,\"livekit_reconnect_slate_url\":\"https://mux-livestream-assets.s3.us-east-1.amazonaws.com/custom-disconnect-slate-tall.png\",\"exclude_from_pymk_suggestions\":false,\"minimum_ios_version\":2200,\"disable_annual_subscriptions\":false,\"enable_bestseller_survey_modal_override\":false,\"enable_livestream_rtmp_invites\":false,\"enable_android_dms\":false,\"feed_ranker_use_user_comment_reaction_cache\":true,\"pub_auto_moderation_enabled\":false,\"disable_live_stream_ai_trimming_by_default\":false,\"recipes_enabled\":false,\"disable_deletion\":false,\"ios_default_coupon_enabled\":false,\"notes_weight_read_post\":5,\"notes_weight_reply\":3,\"livekit_egress_custom_base_url\":\"http://livekit-egress-custom-recorder.s3-website-us-east-1.amazonaws.com\",\"clip_focused_video_upload_flow\":false,\"live_stream_max_guest_users\":2,\"enable_video_seo_data\":false,\"can_reimport_unsubscribed_users_with_2x_optin\":false,\"feed_posts_weight_subscribed\":0,\"included_in_demo_feed\":false,\"live_event_mixin\":\"\",\"review_incoming_email\":\"default\",\"app_install_prompts\":\"native_banner_if_supported\",\"enable_founding_gifts\":false,\"ios_chat_uikit\":false,\"enable_sponsorship_campaigns\":false,\"thread_permalink_preview_min_android_version\":2037,\"tabbed_notes_search\":\"control\",\"default_thumbnail_time\":10,\"pub_ranking_weight_immediate_engagement\":1,\"pub_ranking_weight_retained_engagement\":1,\"load_test_unichat\":false,\"notes_read_post_baseline\":0,\"live_stream_head_alignment_guide\":false,\"show_open_post_as_pdf_button\":false,\"free_press_combo_subscribe_flow_enabled\":false,\"restack_with_image\":false,\"free_press_tabbed_subscribe_flow\":\"control\",\"gift_from_substack_modal\":\"experiment\",\"gifts_from_substack_feature_available\":true,\"disable_ai_clips\":false,\"thefp_enable_web_livestream_kicking\":false,\"enable_elevenlabs_voiceovers\":false,\"use_snowflake_crm_bool\":true,\"android_upgrade_alert_dialog\":true,\"headline_testing_enabled\":true,\"translated_notifications_enabled\":false,\"show_simple_post_editor\":false,\"enable_live_stream_auto_publish_flow\":true,\"search_ranker_query_augmentation\":\"enabled\",\"enable_publication_podcasts_page\":false,\"ios_payment_connection_enabled\":true,\"app_install_reminder_email\":\"experiment\",\"use_landscape_livestream_for_post_draft\":false,\"thefp_enable_dynamic_toaster\":false,\"live_stream_install_app_reminders_enabled\":true,\"use_unified_livestream_workflow\":true,\"thefp_enable_america_250\":true,\"ios_note_composer_settings_enabled\":false,\"android_v2_post_video_player_enabled\":false,\"enable_direct_message_request_bypass\":false,\"enable_apple_news_sync\":false,\"postsById_batch_size\":20,\"free_press_newsletter_promo_enabled\":false,\"enable_ios_livestream_stats\":false,\"disable_live_stream_reactions\":false,\"enable_high_follower_dm\":true,\"ios_welcome_video_profile_prompt\":false,\"clip_generation_3rd_party_vendor\":\"internal\",\"ios_notification_settings_enabled\":false,\"notes_weight_negative\":1,\"ios_discover_tab_min_installed_date\":\"2025-06-09T16:56:58+0000\",\"notes_weight_click_see_more\":2,\"activity_items_writes_rollout\":0,\"edit_profile_theme_colors\":false,\"backend_enable_subscription_bar\":true,\"disable_clipping_for_readers\":false,\"android_enable_subscription_bar\":false,\"apple_fee_percent\":15,\"allow_anonymous_personal_pub_creation\":false,\"feed_posts_weight_reply\":3,\"feed_posts_weight_negative\":5,\"feed_posts_weight_like\":1.5,\"feed_posts_weight_share\":3,\"feed_posts_weight_save\":3,\"enable_press_kit_preview_modal\":false,\"use_snowflake_crm_rollout_percentage\":1,\"feed_posts_weight_sign_up\":4,\"live_stream_video_degradation_preference\":\"maintainFramerate\",\"render_high_quality_clips\":false,\"pause_app_badges\":false,\"android_enable_publication_activity_tab\":false,\"notes_weight_like\":2,\"profile_feed_expanded_inventory\":false,\"phone_verification_fallback_to_twilio\":false,\"livekit_mux_latency_mode\":\"low\",\"feed_posts_weight_long_click\":1,\"feed_juiced_user\":0,\"vertical_video_player_in_feed_1\":\"experiment\",\"show_branded_intro_setting\":true,\"free_press_single_screen_subscribe_flow_enabled\":false,\"notes_click_see_more_baseline\":0.35,\"android_edit_user_links\":true,\"android_move_feed_tabs\":false,\"android_enable_user_status_ui\":false,\"use_advanced_commerce_api_for_iap\":false,\"skip_free_preview_language_in_podcast_notes\":false,\"larger_wordmark_on_publication_homepage\":false,\"video_editor_full_screen\":false,\"enable_mobile_stats_for_admins\":false,\"ios_profile_themes_note_composer_enabled\":false,\"reduce_post_search_fuzziness\":\"treatment\",\"related_posts_web\":\"experiment\",\"notes_weight_click_item\":3,\"enable_milestone_notifications\":true,\"notes_weight_long_visit\":1,\"bypass_single_unlock_token_limit\":false,\"notes_watch_video_baseline\":0.08,\"add_section_and_tag_metadata\":false,\"daily_promoted_notes_enabled\":true,\"feed_ranker_use_user_feed_restack_comment_cache\":true,\"enable_islands_cms\":false,\"enable_livestream_combined_stats\":false,\"search_retrieval_variant\":\"experiment\",\"enable_web_explore\":false,\"enable_drip_campaigns\":false,\"ios_offline_mode_enabled\":false,\"post_management_search_engine\":\"elasticsearch\",\"new_bestseller_leaderboard_feed_item_enabled\":false,\"feed_main_disabled\":false,\"enable_account_settings_revamp\":false,\"allowed_email_domains\":\"one\",\"thefp_enable_fp_recirc_block\":false,\"ios_web_subscription_payments\":\"experiment\",\"publication_search_replatform\":\"experiment\",\"ios_full_search_results\":\"control\",\"enable_debug_logs_ios\":false,\"show_pub_content_on_profile_for_pub_id\":0,\"show_pub_content_on_profile\":false,\"livekit_track_egress\":true,\"video_tab_mixture_pattern\":\"npnnnn\",\"enable_theme_contexts\":false,\"onboarding_suggestions_search\":\"experiment\",\"feed_tuner_enabled\":false,\"enable_arr_milestone_notifications\":true,\"enable_pledges_milestone_notifications\":true,\"livekit_mux_latency_mode_rtmp\":\"low\",\"notes_weight_follow_boost\":3,\"web_notes_trending_topics_enabled\":\"control\",\"ios_trending_topics_feed_item\":\"control\",\"direct_device_push_notifications_ios\":\"control\",\"fcm_high_priority\":false,\"suggested_search_instead_of_dpn\":\"control\",\"unfinished_post_push_notification\":\"treatment\",\"subscription_bar_top_selection_strategy_v2\":\"destination_wau_pub_score\",\"enable_live_stream_audio_enhancements_flow\":false,\"iap_announcement_blog_url\":\"\",\"android_onboarding_progress_persistence\":\"control\",\"related_notes_variations\":\"experiment\",\"dpn_weight_tap_bonus_subscribed\":3,\"ios_custom_buttons_enabled\":true,\"ios_livestream_feedback\":false,\"reader_sharing_flow\":\"control\",\"ios_iap_opt_out_enabled\":false,\"android_view_post_share_assets_employees_only\":false,\"android_trending_topics_feed_item\":\"control\",\"thefp_show_fixed_footer_paywall\":false,\"android_subscription_queue_experiment\":\"experiment\",\"ios_viral_gift_entry_points\":\"treatment\",\"ios_subscription_pogs\":\"experiment\",\"dpn_weight_like\":3,\"use_elasticsearch_for_category_tabs\":\"control\",\"enable_subscribers_milestone_notifications\":true,\"android_enable_edit_profile_theme\":false,\"android_enable_view_profile_theme\":false,\"enable_refresh_token_deduplication\":true,\"dpn_weight_follow\":3,\"live_stream_audio_enhancer_v2\":\"auphonic\",\"dpn_weight_reply\":2.5,\"enable_speaker_focus_clips\":true,\"speaker_focus_variant_generation_enabled\":true,\"search_ranker_load_test_ranking_window\":2000,\"ios_new_post_sharing_flow_enabled\":false,\"search_ranker_load_test_pct\":0,\"profile_feed_expanded_inventory_experiment_driven\":\"control\",\"ignore_video_in_notes_length_limit\":false,\"web_show_scores_on_sports_tab\":false,\"notes_weight_click_share\":3,\"direct_device_push_notifications\":false,\"allow_long_videos\":true,\"dpn_user_picture_instead_of_comment\":\"experiment\",\"ios_post_video_pager_enabled\":\"control\",\"dpn_score_threshold\":0,\"thefp_enable_follow_module\":false,\"dpn_weight_follow_bonus\":0.5,\"ios_post_subscribe_follow_related\":\"control\",\"use_intro_clip_and_branded_intro_by_default\":false,\"publisher_banner\":\"\",\"new_user_subscribe_follow_prompt_override\":\"none\",\"iap_broad_launch_enabled\":true,\"dpn_weight_open\":2.5,\"ios_subscription_pogs_new_users\":\"experiment\",\"ios_subscription_pogs_old_users\":\"experiment\",\"android_subscription_queue_experiment_2\":\"experiment\",\"enable_viewing_all_livestream_viewers\":false,\"serve_suggested_searches_table\":false,\"enable_clip_prompt_variant_filtering\":true,\"dpn_ranking_enabled\":true,\"sequential_retrieval_model_pct\":100,\"dpn_model_variant\":\"experiment\",\"skip_default_welcome_email_for_app_users\":\"control\",\"dpn_weight_long_session\":2.5,\"dpn_weight_tap\":3,\"android_vertical_post_player\":\"control\",\"new_note_communication_notification_experiment\":\"control\",\"enable_notes_admins\":false,\"suggested_search_notifications\":\"control\",\"enable_suggested_searches\":true,\"maximum_affinity_index_for_trending_topic_notification\":5,\"android_synchronous_push_notif_handling\":\"control\",\"animate_all_explore_video_suggestions\":true,\"android_vertical_post_player_2\":\"experiment\",\"suggested_search_metadata_web_market_ui\":false,\"dpn_weight_restack\":2,\"use_new_snowflake_user_rollout_percentage\":1,\"session_version_invalidation_enabled\":false,\"dpn_weight_negative\":1},\"publicationSettings\":{\"block_ai_crawlers\":false,\"credit_token_enabled\":true,\"custom_tos_and_privacy\":false,\"did_identity\":null,\"disable_optimistic_bank_payments\":false,\"display_welcome_page_details\":true,\"enable_meetings\":false,\"payment_pledges_enabled\":false,\"enable_post_page_conversion\":true,\"enable_prev_next_nav\":false,\"enable_restacking\":true,\"gifts_from_substack_disabled\":false,\"google_analytics_4_token\":\"G-Z4BJTTV5MZ\",\"group_sections_and_podcasts_in_menu_enabled\":false,\"live_stream_homepage_visibility\":\"contributorsAndAdmins\",\"live_stream_homepage_style\":\"autoPlay\",\"medium_length_description\":\"Artificial intelligence is a fast-moving field. Ahead of AI helps you keep up with the latest developments and research trends in the fields of machine learning, deep learning, and artificial intelligence.\",\"notes_feed_enabled\":true,\"paywall_unlock_tokens\":true,\"post_preview_crop_gravity\":\"center\",\"reader_referrals_enabled\":false,\"reader_referrals_leaderboard_enabled\":false,\"seen_coming_soon_explainer\":false,\"seen_google_analytics_migration_modal\":false,\"local_currency_modal_seen\":true,\"local_payment_methods_modal_seen\":true,\"twitter_pixel_signup_event_id\":null,\"twitter_pixel_subscribe_event_id\":null,\"use_local_currency\":true,\"welcome_page_opt_out_text\":\"No thanks\",\"cookie_settings\":\"\",\"show_restacks_below_posts\":true,\"holiday_gifting_post_header\":false,\"homepage_message_text\":\"\",\"homepage_message_link\":\"\",\"about_us_author_ids\":\"\",\"archived_section_ids\":\"\",\"column_section_ids\":\"\",\"fp_primary_column_section_ids\":\"\",\"event_section_ids\":\"\",\"podcasts_metadata\":\"\",\"video_section_ids\":\"\",\"post_metering_enabled\":false},\"publicationUserSettings\":null,\"userSettings\":{\"user_id\":null,\"activity_likes_enabled\":true,\"artist_mode_enabled\":false,\"dashboard_nav_refresh_enabled\":false,\"hasDismissedSectionToNewsletterRename\":false,\"is_guest_post_enabled\":true,\"feed_web_nux_seen_at\":null,\"has_seen_select_to_restack_tooltip_nux\":false,\"invite_friends_nux_dismissed_at\":null,\"suggestions_feed_item_last_shown_at\":null,\"has_seen_select_to_restack_modal\":false,\"last_home_tab\":null,\"last_notification_alert_shown_at\":null,\"disable_reply_hiding\":false,\"newest_seen_chat_item_published_at\":null,\"explicitContentEnabled\":false,\"contactMatchingEnabled\":false,\"messageRequestLevel\":\"everyone\",\"liveStreamAcceptableInviteLevel\":\"everyone\",\"liveStreamAcceptableChatLevel\":\"everyone\",\"creditTokensTreatmentExposed\":false,\"appBadgeIncludesChat\":false,\"autoPlayVideo\":true,\"smart_delivery_enabled\":false,\"chatbotTermsLastAcceptedAt\":null,\"has_seen_notes_post_app_upsell\":false,\"substack_summer_nux_dismissed_at\":null,\"first_note_id\":null,\"show_concurrent_live_stream_viewers\":false,\"has_dismissed_fp_download_pdf_nux\":false,\"edit_profile_feed_item_dismissed_at\":null,\"mobile_permalink_app_upsell_seen_at\":null,\"new_user_checklist_enabled\":false,\"new_user_follow_subscribe_prompt_dismissed_at\":null,\"has_seen_youtube_shorts_auto_publish_announcement\":false,\"notificationQualityFilterEnabled\":true,\"hasSeenOnboardingNewslettersScreen\":false},\"subscriberCountDetails\":\"hundreds of thousands of subscribers\",\"mux_env_key\":\"u42pci814i6011qg3segrcpp9\",\"sentry_environment\":\"production\",\"launchWelcomePage\":false,\"pendingInviteForActiveLiveStream\":null,\"blurbs\":[{\"id\":431753,\"recommended_publication_id\":1174659,\"recommending_publication_id\":265424,\"created_at\":\"2022-11-16T00:01:06.468Z\",\"updated_at\":\"2022-11-16T00:01:06.468Z\",\"blurb_active\":true,\"description\":\"Sebastian is an incredible educator and always has invaluable insights--do keep up with his work!\",\"email_sent_at\":\"2022-11-16T00:34:49.184Z\",\"recommendingPublication\":{\"id\":265424,\"name\":\"The Gradient\",\"subdomain\":\"thegradientpub\",\"custom_domain\":null,\"custom_domain_optional\":false,\"hero_text\":\"Articles, interviews, and news coverage about AI brought to you by a team of AI researchers and builders.\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/33e22926-7401-4e09-8c7c-1e6b0f179f76_1196x1196.png\",\"author_id\":25322056,\"primary_user_id\":3550791,\"theme_var_background_pop\":\"#786CFF\",\"created_at\":\"2021-01-18T22:35:25.588Z\",\"email_from_name\":\"The Gradient\",\"copyright\":\"The Gradient\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"magaziney\",\"is_personal_mode\":false,\"author\":{\"id\":25322056,\"name\":\"The Gradient\",\"handle\":\"thegradientpub\",\"previous_name\":null,\"photo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/1ab6eeb8-808d-4094-b09a-42a3980ca045_400x400.jpeg\",\"bio\":\"The Gradient is a digital magazine that aims to be a place for discussion about research and trends in artificial intelligence and machine learning. We provide \",\"profile_set_up_at\":\"2021-04-30T00:51:15.962Z\",\"reader_installed_at\":null}},\"author_name\":\"The Gradient\",\"author\":{\"id\":25322056,\"photo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/1ab6eeb8-808d-4094-b09a-42a3980ca045_400x400.jpeg\"}},{\"id\":466622,\"recommended_publication_id\":1174659,\"recommending_publication_id\":1199871,\"created_at\":\"2022-11-28T19:26:18.920Z\",\"updated_at\":\"2022-12-16T23:31:15.215Z\",\"blurb_active\":true,\"description\":\"All the latest scoop on AI from one of the best educators out there!\",\"email_sent_at\":null,\"recommendingPublication\":{\"id\":1199871,\"name\":\"Gradient Ascent\",\"subdomain\":\"artofsaience\",\"custom_domain\":\"newsletter.artofsaience.com\",\"custom_domain_optional\":false,\"hero_text\":\"Gradient Ascent is your weekly guide to AI, trusted by Silicon Valley's top tech firms and the best academic labs worldwide. \",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/01dfb858-3107-4656-b289-cf13de969a17_800x800.png\",\"author_id\":85853406,\"primary_user_id\":85853406,\"theme_var_background_pop\":\"#FD5353\",\"created_at\":\"2022-11-18T15:58:31.925Z\",\"email_from_name\":\"Sairam from The Art of Saience\",\"copyright\":\"Sairam Sundaresan\",\"founding_plan_name\":\"\\\"I can expense it\\\" \",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"paused\",\"language\":null,\"explicit\":false,\"homepage_type\":\"magaziney\",\"is_personal_mode\":false,\"author\":{\"id\":85853406,\"name\":\"Sairam Sundaresan\",\"handle\":\"sairamsundaresan\",\"previous_name\":\"Sairam\",\"photo_url\":\"https://substackcdn.com/image/fetch/$s_!3vud!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79cc4b2d-3161-4743-85d8-97910007711b_1463x1463.jpeg\",\"bio\":\"I'm an AI engineering leader with 14+ years of industry experience across different sectors including mobile devices, data centers, and autonomous vehicles. I'll help you understand AI and break into the field.\",\"profile_set_up_at\":\"2022-04-28T18:19:37.215Z\",\"reader_installed_at\":\"2022-11-03T19:27:08.434Z\"}},\"author_name\":\"Sairam Sundaresan\",\"author\":{\"id\":85853406,\"photo_url\":\"https://substackcdn.com/image/fetch/$s_!3vud!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79cc4b2d-3161-4743-85d8-97910007711b_1463x1463.jpeg\"}}],\"homepageData\":{\"contentBlockData\":{\"contentBlocks\":[{\"id\":\"2fb7e752-6e5c-4eb4-ae81-38e8bcd756fb\",\"publication_id\":1174659,\"post_source\":\"latest\",\"section_id\":null,\"post_tag_id\":null,\"block_type\":\"grid\",\"primary_sidebar_modules\":[],\"secondary_sidebar_modules\":[],\"order\":1,\"num_posts\":null,\"num_rows\":3,\"attrs\":{},\"section\":null,\"postTag\":null,\"contentBlockPins\":[]},{\"id\":\"4b6e2495-df31-4a11-8b5a-87f55f039485\",\"publication_id\":1174659,\"post_source\":\"none\",\"section_id\":null,\"post_tag_id\":null,\"block_type\":\"subscribe\",\"primary_sidebar_modules\":[],\"secondary_sidebar_modules\":[],\"order\":2,\"num_posts\":null,\"num_rows\":null,\"attrs\":{\"non_subscriber_message\":\"Subscribe to receive new in-depth research insights on AI and machine learning.\",\"free_subscriber_message\":\"Support independent AI research with a paid upgrade and access additional, in-depth content.\",\"paid_subscriber_message\":\"Your support as a paid subscriber makes independent AI research possible\u2014thank you!\"},\"section\":null,\"postTag\":null,\"contentBlockPins\":[]}],\"contributors\":[],\"latestPodcastPosts\":null,\"latestPostByContributorId\":{},\"latestPostFromSections\":{},\"postsBySectionId\":{},\"postsByTagId\":{},\"postsForContentBlockPins\":{}},\"homepageLinks\":[],\"newPosts\":[{\"id\":172832845,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding and Implementing Qwen3 From Scratch\",\"social_title\":\"Understanding and Implementing Qwen3 From Scratch\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"qwen3-from-scratch\",\"post_date\":\"2025-09-06T11:10:21.239Z\",\"audience\":\"only_paid\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"only_paid\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/qwen3-from-scratch\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":60},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"A Detailed Look at One of the Leading Open-Source LLMs\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/00515b82-fd01-4c5b-b1b9-2d4b38100be0_1236x775.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":null,\"videoUpload\":null,\"podcastFields\":{\"post_id\":172832845,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"A Detailed Look at One of the Leading Open-Source LLMs\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"Previously, I compared the most notable open-weight architectures of 2025 in The Big LLM Architecture Comparison. Then, I zoomed in and discussed the various architecture components in From GPT-2 to gpt-oss: Analyzing the Architectural Advances on a conceptual level.\",\"wordcount\":7156,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5cf0ea83-392e-495a-9745-dc0aaa7f7d4f\",\"publication_id\":1174659,\"name\":\"Reasoning Models\",\"slug\":\"reasoning-models\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[61,183,252],\"population\":18},\"DarkVibrant\":{\"rgb\":[180,140,44],\"population\":1},\"LightVibrant\":{\"rgb\":[140,210,252],\"population\":28},\"Muted\":{\"rgb\":[108,140,156],\"population\":1},\"DarkMuted\":{\"rgb\":[42,52,62],\"population\":23},\"LightMuted\":{\"rgb\":[174,199,212],\"population\":91}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":60,\"comment_count\":4,\"child_comment_count\":3,\"audio_items\":[{\"post_id\":172832845,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":null,\"type\":\"tts\",\"status\":\"paywalled\"}],\"is_geoblocked\":false,\"hidden\":true,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":170506328,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"From GPT-2 to gpt-oss: Analyzing the Architectural Advances\",\"social_title\":\"From GPT-2 to gpt-oss: Analyzing the Architectural Advances\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"from-gpt-2-to-gpt-oss-analyzing-the\",\"post_date\":\"2025-08-09T11:23:07.237Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":563},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"And How They Stack Up Against Qwen3\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/529c4cc7-161c-4d7c-b186-06e68c771776_1564x926.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":170506328,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"And How They Stack Up Against Qwen3\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"OpenAI just released their new open-weight LLMs this week: gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019. And yes, thanks to some clever optimizations, they can run locally (but more about this later).\",\"wordcount\":5221,\"postTags\":[{\"id\":\"5cf0ea83-392e-495a-9745-dc0aaa7f7d4f\",\"publication_id\":1174659,\"name\":\"Reasoning Models\",\"slug\":\"reasoning-models\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[92,199,180],\"population\":16},\"DarkVibrant\":{\"rgb\":[33.90684931506849,98.69315068493151,87.18904109589042],\"population\":0},\"LightVibrant\":{\"rgb\":[135,164,235],\"population\":14},\"Muted\":{\"rgb\":[100,116,148],\"population\":1},\"DarkMuted\":{\"rgb\":[68,68,68],\"population\":5},\"LightMuted\":{\"rgb\":[135,188,181],\"population\":318}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":563,\"comment_count\":45,\"child_comment_count\":23,\"audio_items\":[{\"post_id\":170506328,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/170506328/tts/d2f3c50e-08e5-4c36-a942-d3a52acf3214/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":168650848,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"The Big LLM Architecture Comparison\",\"social_title\":\"The Big LLM Architecture Comparison\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"the-big-llm-architecture-comparison\",\"post_date\":\"2025-07-19T11:11:10.901Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[1174659],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":1181},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"From DeepSeek-V3 to gpt-oss: A Look At Modern LLM Architecture Design\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/45c50202-0e8b-4e64-8296-4e2ccf4cb287_1756x1227.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":168650848,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"It has been seven years since the original GPT architecture was developed. At first glance, looking back at GPT-2 (2019) and forward to DeepSeek-V3 and Llama 4 (2024-2025), one might be surprised at how structurally similar these models still are.\",\"wordcount\":7249,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":{\"post_id\":168650848,\"starts_at\":\"2025-07-19T11:11:11.298Z\",\"ended_at\":\"2025-07-19T12:11:11.487Z\",\"planned_duration_ms\":3600000,\"test_cohort_percent\":5,\"test_cohort_emailed\":true,\"main_cohort_emailed\":true,\"created_at\":\"2025-07-18T17:48:26.613Z\",\"updated_at\":\"2025-07-19T12:11:26.309Z\",\"creator_id\":27393275,\"ended_early\":false,\"winner_overridden\":false},\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[89,193,146],\"population\":57},\"DarkVibrant\":{\"rgb\":[28,25,68],\"population\":350},\"LightVibrant\":{\"rgb\":[251,228,197],\"population\":9},\"Muted\":{\"rgb\":[100,116,148],\"population\":2},\"DarkMuted\":{\"rgb\":[94,99,128],\"population\":49},\"LightMuted\":{\"rgb\":[189,168,205],\"population\":245}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":1181,\"comment_count\":60,\"child_comment_count\":24,\"audio_items\":[{\"post_id\":168650848,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/168650848/tts/582abd24-ce98-43b6-81ac-1339a0d403d5/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":166943621,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"LLM Research Papers: The 2025 List (January to June)\",\"social_title\":\"LLM Research Papers: The 2025 List (January to June)\",\"search_engine_title\":null,\"search_engine_description\":\"The latest in LLM research with a hand-curated, topic-organized list of over 200 research papers from 2025.\",\"type\":\"newsletter\",\"slug\":\"llm-research-papers-2025-list-one\",\"post_date\":\"2025-07-01T11:11:45.123Z\",\"audience\":\"only_paid\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"only_paid\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":70},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\" A topic-organized collection of 200+ LLM research papers from 2025\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!cRyQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e920533-c926-461b-95bb-c7446f3df382_1289x619.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":null,\"videoUpload\":null,\"podcastFields\":{\"post_id\":166943621,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"A topic-organized collection of 200+ LLM research papers from 2025\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"As some of you know, I keep a running list of research papers I (want to) read and reference.\",\"wordcount\":3976,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5cf0ea83-392e-495a-9745-dc0aaa7f7d4f\",\"publication_id\":1174659,\"name\":\"Reasoning Models\",\"slug\":\"reasoning-models\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":false,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[74,132,170],\"population\":50},\"DarkVibrant\":{\"rgb\":[40.214754098360665,71.73442622950819,92.38524590163934],\"population\":0},\"LightVibrant\":{\"rgb\":[180,148,212],\"population\":2},\"Muted\":{\"rgb\":[100,159,100],\"population\":13},\"DarkMuted\":{\"rgb\":[68,108,73],\"population\":7},\"LightMuted\":{\"rgb\":[166,186,204],\"population\":4}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":70,\"comment_count\":5,\"child_comment_count\":3,\"audio_items\":[{\"post_id\":166943621,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":null,\"type\":\"tts\",\"status\":\"paywalled\"}],\"is_geoblocked\":false,\"hidden\":true,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":166106178,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding and Coding the KV Cache in LLMs from Scratch\",\"social_title\":\"Understanding and Coding the KV Cache in LLMs from Scratch\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"coding-the-kv-cache-in-llms\",\"post_date\":\"2025-06-17T10:55:34.121Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/coding-the-kv-cache-in-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":385},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"\",\"cover_image\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":166106178,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"KV caches are one of the most critical techniques for efficient inference in LLMs in production.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"KV caches are one of the most critical techniques for efficient inference in LLMs in production. KV caches are an important component for compute-efficient LLM inference in production. This article explains how they work conceptually and in code with a from-scratch, human-readable implementation.\",\"wordcount\":2849,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":{\"post_id\":166106178,\"starts_at\":\"2025-06-17T10:55:34.788Z\",\"ended_at\":\"2025-06-17T11:26:50.707Z\",\"planned_duration_ms\":1800000,\"test_cohort_percent\":5,\"test_cohort_emailed\":true,\"main_cohort_emailed\":true,\"created_at\":\"2025-06-16T21:55:49.523Z\",\"updated_at\":\"2025-06-17T11:26:59.428Z\",\"creator_id\":27393275,\"ended_early\":false,\"winner_overridden\":false},\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[132,188,212],\"population\":1},\"DarkVibrant\":{\"rgb\":[34.34819277108433,79.08072289156628,98.25180722891568],\"population\":0},\"LightVibrant\":{\"rgb\":[140,188,220],\"population\":1},\"Muted\":{\"rgb\":[124,164,188],\"population\":1},\"DarkMuted\":{\"rgb\":[108,108,108],\"population\":8},\"LightMuted\":{\"rgb\":[188,172,204],\"population\":4}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":385,\"comment_count\":27,\"child_comment_count\":16,\"audio_items\":[{\"post_id\":166106178,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/166106178/tts/a4265993-8c0d-4708-b7a7-ea42208e893a/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":162934205,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Coding LLMs from the Ground Up: A Complete Course\",\"social_title\":\"Coding LLMs from the Ground Up: A Complete Course\",\"search_engine_title\":null,\"search_engine_description\":\"Why build an LLM from scratch? It's probably the best and most efficient way to learn how LLMs really work. Plus, many readers have told me they had a lot of fun doing it.\",\"type\":\"newsletter\",\"slug\":\"coding-llms-from-the-ground-up\",\"post_date\":\"2025-05-10T11:03:17.769Z\",\"audience\":\"only_paid\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":true,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/coding-llms-from-the-ground-up\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":233},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/752b6486-73db-4293-9465-854d4a005a5c_2406x1350.jpeg\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":null,\"videoUpload\":null,\"podcastFields\":{\"post_id\":162934205,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Why build LLMs from scratch? It's probably the best and most efficient way to learn how LLMs really work. Plus, many readers have told me they had a lot of fun doing it.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"I wrote a lot about reasoning models in recent months (4 articles in a row)! Next to everything \\\"agentic,\\\" reasoning is one of the biggest LLM topics of 2025.\",\"wordcount\":997,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":false,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[57.24489795918357,130.10204081632648,197.75510204081644],\"population\":0},\"DarkVibrant\":{\"rgb\":[29.767346938775457,67.65306122448978,102.83265306122455],\"population\":0},\"LightVibrant\":{\"rgb\":[217,231,244],\"population\":9},\"Muted\":{\"rgb\":[108,116,132],\"population\":2},\"DarkMuted\":{\"rgb\":[52,52,52],\"population\":5},\"LightMuted\":{\"rgb\":[156,180,204],\"population\":110}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":233,\"comment_count\":4,\"child_comment_count\":3,\"audio_items\":[{\"post_id\":162934205,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":null,\"type\":\"tts\",\"status\":\"paywalled\"}],\"is_geoblocked\":false,\"hidden\":true,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":161572341,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"The State of Reinforcement Learning for LLM Reasoning\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"the-state-of-llm-reasoning-model-training\",\"post_date\":\"2025-04-19T11:02:44.098Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":432},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Understanding GRPO and New Insights from Reasoning Model Papers\",\"cover_image\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":161572341,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Understanding GRPO and New Insights from Reasoning Model Papers\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"A lot has happened this month, especially with the releases of new flagship models like GPT-4.5 and Llama 4. But you might have noticed that reactions to these releases were relatively muted. Why? One reason could be that GPT-4.5 and Llama 4 remain conventional models, which means they were trained without explicit reinforcement learning for reasoning.\",\"wordcount\":7371,\"postTags\":[{\"id\":\"5cf0ea83-392e-495a-9745-dc0aaa7f7d4f\",\"publication_id\":1174659,\"name\":\"Reasoning Models\",\"slug\":\"reasoning-models\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[14,122,189],\"population\":536},\"DarkVibrant\":{\"rgb\":[9.144827586206898,79.69064039408869,123.45517241379311],\"population\":0},\"LightVibrant\":{\"rgb\":[133,185,222],\"population\":33},\"Muted\":{\"rgb\":[131,131,132],\"population\":733},\"DarkMuted\":{\"rgb\":[76,76,76],\"population\":4},\"LightMuted\":{\"rgb\":[203,204,220],\"population\":736}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":432,\"comment_count\":31,\"child_comment_count\":23,\"audio_items\":[{\"post_id\":161572341,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/161572341/tts/cd672520-7f7e-451a-b651-e4dc94781b46/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":159880090,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"First Look at Reasoning From Scratch: Chapter 1\",\"social_title\":\"First Look at Reasoning From Scratch: Chapter 1\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"first-look-at-reasoning-from-scratch\",\"post_date\":\"2025-03-29T11:11:41.742Z\",\"audience\":\"only_paid\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"only_paid\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/first-look-at-reasoning-from-scratch\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":56},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"An introduction to reasoning in today's LLMs\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/01d8ee9a-933b-4e66-8ad8-28056a0fdd73_1628x1076.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":null,\"videoUpload\":null,\"podcastFields\":{\"post_id\":159880090,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Welcome to the next stage of large language models (LLMs): reasoning. LLMs have transformed how we process and generate text, but their success has been largely driven by statistical pattern recognition. However, new advances in reasoning methodologies now enable LLMs to tackle more complex tasks, such as solving logical puzzles or multi-step arithmetic. Understanding these methodologies is the central focus of this book.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"Hi everyone,\",\"wordcount\":3577,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":false,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[4,172,140],\"population\":485},\"DarkVibrant\":{\"rgb\":[27,178,152],\"population\":41},\"LightVibrant\":{\"rgb\":[147,202,211],\"population\":20},\"Muted\":{\"rgb\":[116,116,116],\"population\":3},\"DarkMuted\":{\"rgb\":[52,52,52],\"population\":2},\"LightMuted\":{\"rgb\":[177,188,188],\"population\":98}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":56,\"comment_count\":15,\"child_comment_count\":7,\"audio_items\":[{\"post_id\":159880090,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":null,\"type\":\"tts\",\"status\":\"paywalled\"}],\"is_geoblocked\":false,\"hidden\":true,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":158620387,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"The State of LLM Reasoning Model Inference\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"state-of-llm-reasoning-and-inference-scaling\",\"post_date\":\"2025-03-08T12:11:42.532Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":389},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Inference-Time Compute Scaling Methods to Improve Reasoning Models\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!IOSP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":158620387,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Inference-Time Compute Scaling Methods to Improve Reasoning Models\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"Improving the reasoning abilities of large language models (LLMs) has become one of the hottest topics in 2025, and for good reason. Stronger reasoning skills allow LLMs to tackle more complex problems, making them more capable across a wide range of tasks users care about.\",\"wordcount\":4291,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[166,28,9],\"population\":48},\"DarkVibrant\":{\"rgb\":[152,80,72],\"population\":2},\"LightVibrant\":{\"rgb\":[252,200,201],\"population\":6},\"Muted\":{\"rgb\":[113,129,131],\"population\":100},\"DarkMuted\":{\"rgb\":[92,95,95],\"population\":17},\"LightMuted\":{\"rgb\":[180,212,211],\"population\":134}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":389,\"comment_count\":10,\"child_comment_count\":8,\"audio_items\":[{\"post_id\":158620387,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/158620387/tts/02a569db-ee04-4fb0-a5d5-4f944a6a09cb/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":156484949,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding Reasoning LLMs\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"understanding-reasoning-llms\",\"post_date\":\"2025-02-05T12:11:39.216Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/understanding-reasoning-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":1098},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Methods and Strategies for Building and Refining Reasoning Models\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!QwUc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":156484949,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Methods and Strategies for Building and Refining Reasoning Models\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"This article describes the four main approaches to building reasoning models, or how we can enhance LLMs with reasoning capabilities. I hope this provides valuable insights and helps you navigate the rapidly evolving literature and hype surrounding this topic.\",\"wordcount\":4028,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[169,25,6],\"population\":87},\"DarkVibrant\":{\"rgb\":[128.0537142857143,18.942857142857143,4.546285714285711],\"population\":0},\"LightVibrant\":{\"rgb\":[252,235,228],\"population\":408},\"Muted\":{\"rgb\":[116,132,148],\"population\":1},\"DarkMuted\":{\"rgb\":[68,68,68],\"population\":7},\"LightMuted\":{\"rgb\":[164,188,200],\"population\":2}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":1098,\"comment_count\":40,\"child_comment_count\":21,\"audio_items\":[{\"post_id\":156484949,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/156484949/tts/410051a7-e938-4017-8671-68e5f1fae779/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":153692738,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Noteworthy AI Research Papers of 2024 (Part Two)\",\"social_title\":\"Noteworthy AI Research Papers of 2024 (Part Two)\",\"search_engine_title\":null,\"search_engine_description\":\"Six influential AI papers from July to December, from  improving LLMs by scaling inference-time compute to gains from synthetic training data.\",\"type\":\"newsletter\",\"slug\":\"ai-research-papers-2024-part-2\",\"post_date\":\"2025-01-15T12:11:49.791Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":172},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Six influential AI papers from July to December\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/950f1c27-f0ee-4392-a7f4-906125c58738_1888x1670.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":153692738,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Six influential AI papers from July to December\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"I hope your 2025 is off to a great start! To kick off the year, I've finally been able to complete the draft and second part of this AI Research Highlights of 2024 article. It covers a variety of relevant topics, from mixture-of-experts models to new LLM scaling laws for precision.\",\"wordcount\":5450,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[69,148,202],\"population\":23},\"DarkVibrant\":{\"rgb\":[36,92,140],\"population\":1},\"LightVibrant\":{\"rgb\":[225,194,149],\"population\":15},\"Muted\":{\"rgb\":[100,172,161],\"population\":3},\"DarkMuted\":{\"rgb\":[75,98,105],\"population\":101},\"LightMuted\":{\"rgb\":[191,195,196],\"population\":181}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":172,\"comment_count\":10,\"child_comment_count\":5,\"audio_items\":[{\"post_id\":153692738,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/153692738/tts/4a7d14a6-fd5d-4949-8479-4e7f054324d0/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":153341037,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Noteworthy AI Research Papers of 2024 (Part One)\",\"social_title\":\"Noteworthy AI Research Papers of 2024 (Part One)\",\"search_engine_title\":null,\"search_engine_description\":\"Six influential AI papers from January to June, from mixture-of-experts LLMs to new LoRA variants.\",\"type\":\"newsletter\",\"slug\":\"ai-research-papers-2024-part-1\",\"post_date\":\"2024-12-31T12:21:42.892Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":361},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Six influential AI papers from January to June\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/cace23cf-5185-4045-a894-bb67b20728c8_1850x1348.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":153341037,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Six influential AI papers from January to June\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"To kick off the year, I've finally been able to complete the draft of this AI Research Highlights of 2024 article. It covers a variety of topics, from mixture-of-experts models to new LLM scaling laws for precision.\",\"wordcount\":3234,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[8,163,249],\"population\":156},\"DarkVibrant\":{\"rgb\":[15,124,188],\"population\":24},\"LightVibrant\":{\"rgb\":[94,196,252],\"population\":8},\"Muted\":{\"rgb\":[135,148,116],\"population\":9},\"DarkMuted\":{\"rgb\":[51,51,51],\"population\":18},\"LightMuted\":{\"rgb\":[186,198,161],\"population\":33}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":361,\"comment_count\":28,\"child_comment_count\":17,\"audio_items\":[{\"post_id\":153341037,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/153341037/tts/0cc251e8-b1cf-46e1-b486-673fac99d065/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":152305086,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"LLM Research Papers: The 2024 List\",\"social_title\":\"LLM Research Papers: The 2024 List\",\"search_engine_title\":null,\"search_engine_description\":\"A curated list of interesting LLM-related research papers from 2024, shared for those looking for something to read over the holidays.\",\"type\":\"newsletter\",\"slug\":\"llm-research-papers-the-2024-list\",\"post_date\":\"2024-12-08T12:11:04.618Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":296},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/7ba1c2ab-7ae7-4f22-86df-f116f2914cd5_1272x1232.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":152305086,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"A curated list of interesting LLM-related research papers from 2024, shared for those looking for something to read over the holidays.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"It\u2019s been a very eventful and exciting year in AI research. This is especially true if you are interested in LLMs.\",\"wordcount\":6491,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[127.5,127.5,127.5],\"population\":0},\"DarkVibrant\":{\"rgb\":[66.3,66.3,66.3],\"population\":0},\"LightVibrant\":{\"rgb\":[188.7,188.7,188.7],\"population\":0},\"Muted\":{\"rgb\":[144,146,146],\"population\":26},\"DarkMuted\":{\"rgb\":[76.5,76.5,76.5],\"population\":0},\"LightMuted\":{\"rgb\":[188,188,188],\"population\":231}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":296,\"comment_count\":28,\"child_comment_count\":23,\"audio_items\":[{\"post_id\":152305086,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/152305086/tts/69edd8db-4544-4336-819b-9684a0d3bc22/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":151078631,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding Multimodal LLMs\",\"social_title\":\"Understanding Multimodal LLMs\",\"search_engine_title\":null,\"search_engine_description\":\"An introduction to the main multimodal LLM techniques and latest models\",\"type\":\"newsletter\",\"slug\":\"understanding-multimodal-llms\",\"post_date\":\"2024-11-03T12:44:00.421Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/understanding-multimodal-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":540},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"An introduction to the main techniques and latest models\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/c534f387-f776-41eb-9c65-f0032b91daee_1988x1430.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":151078631,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"An introduction to the main techniques and latest models\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"It was a wild two months. There have once again been many developments in AI research, with two Nobel Prizes awarded to AI and several interesting research papers published.\",\"wordcount\":4421,\"postTags\":[{\"id\":\"1496f84a-084b-4bf8-9930-a33eca71b372\",\"publication_id\":1174659,\"name\":\"Computer Vision\",\"slug\":\"computer-vision\",\"hidden\":false},{\"id\":\"2e792e1c-c4e1-4a7c-a303-bdac25c1f3a8\",\"publication_id\":1174659,\"name\":\"Vision Transformers\",\"slug\":\"vision-transformers\",\"hidden\":false},{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[180,22,5],\"population\":151},\"DarkVibrant\":{\"rgb\":[4,116,188],\"population\":58},\"LightVibrant\":{\"rgb\":[197,222,239],\"population\":75},\"Muted\":{\"rgb\":[145,103,93],\"population\":24},\"DarkMuted\":{\"rgb\":[98,73,50],\"population\":17},\"LightMuted\":{\"rgb\":[207,178,173],\"population\":41}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":540,\"comment_count\":57,\"child_comment_count\":28,\"audio_items\":[{\"post_id\":151078631,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/151078631/tts/1548c000-1339-4ad7-9834-b5616d93e113/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":148679739,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Building A GPT-Style LLM Classifier From Scratch\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"building-a-gpt-style-llm-classifier\",\"post_date\":\"2024-09-21T12:07:44.166Z\",\"audience\":\"only_paid\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/building-a-gpt-style-llm-classifier\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":175},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Finetuning a GPT Model for Spam Classification\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!mmsV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":null,\"videoUpload\":null,\"podcastFields\":{\"post_id\":148679739,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Finetuning a GPT Model for Spam Classification\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"In this article, I want to show you how to transform pretrained large language models (LLMs) into strong text classifiers.\",\"wordcount\":3937,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[202.50000000000003,127.5,52.499999999999986],\"population\":0},\"DarkVibrant\":{\"rgb\":[105.30000000000001,66.3,27.3],\"population\":0},\"LightVibrant\":{\"rgb\":[248,238,228],\"population\":7},\"Muted\":{\"rgb\":[176,176,176],\"population\":32},\"DarkMuted\":{\"rgb\":[121.50000000000001,76.5,31.499999999999986],\"population\":0},\"LightMuted\":{\"rgb\":[216,185,184],\"population\":30}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":175,\"comment_count\":12,\"child_comment_count\":5,\"audio_items\":[{\"post_id\":148679739,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":null,\"type\":\"tts\",\"status\":\"paywalled\"}],\"is_geoblocked\":false,\"hidden\":true,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":148329414,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Building LLMs from the Ground Up: A 3-hour Coding Workshop\",\"social_title\":\"Building LLMs from the Ground Up: A 3-hour Coding Workshop\",\"search_engine_title\":null,\"search_engine_description\":\"A 3-hour coding workshop presentation on implementing, training, and using LLMs.\",\"type\":\"newsletter\",\"slug\":\"building-llms-from-the-ground-up\",\"post_date\":\"2024-08-31T10:39:35.940Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":433},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/367b547c-9d22-4a3d-b466-1d56ccc6b055_1844x1224.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":148329414,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"If your weekend plans include catching up on AI developments and understanding Large Language Models (LLMs), I've prepared a 1-hour presentation on the development cycle of LLMs, covering everything from architectural implementation to the finetuning stages.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"If you\u2019d like to spend a few hours this weekend to dive into Large Language Models (LLMs) and understand how they work, I've prepared a 3-hour coding workshop presentation on implementing, training, and using LLMs.\",\"wordcount\":343,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[204,124,164],\"population\":34},\"DarkVibrant\":{\"rgb\":[64,40,20],\"population\":2},\"LightVibrant\":{\"rgb\":[236,143,189],\"population\":45},\"Muted\":{\"rgb\":[101,130,152],\"population\":17},\"DarkMuted\":{\"rgb\":[84,44,72],\"population\":2},\"LightMuted\":{\"rgb\":[167,178,191],\"population\":23}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":433,\"comment_count\":16,\"child_comment_count\":11,\"audio_items\":[{\"post_id\":148329414,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/148329414/tts/ead5fd00-a385-4a95-a7dc-9ff942b29fb3/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":147749119,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"New LLM Pre-training and Post-training Paradigms\",\"social_title\":\"New LLM Pre-training and Post-training Paradigms\",\"search_engine_title\":null,\"search_engine_description\":\"New LLM Pre-training and Post-training Paradigms: A Look at How Moderns LLMs Are Trained\",\"type\":\"newsletter\",\"slug\":\"new-llm-pre-training-and-post-training\",\"post_date\":\"2024-08-17T11:55:09.678Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":319},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"A Look at How Modern LLMs Are Trained\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/02b82c11-c899-4202-9594-19c0db1e147b_1514x1298.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":147749119,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"A Look at How Moderns LLMs Are Trained\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"The development of large language models (LLMs) has come a long way, from the early GPT models to the sophisticated open-weight LLMs we have today. Initially, the LLM training process focused solely on pre-training, but it has since expanded to include both pre-training and post-training. Post-training typically encompasses supervised instruction fine-tuning and alignment, which was popularized by ChatGPT.\",\"wordcount\":4270,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[25,127,192],\"population\":25},\"DarkVibrant\":{\"rgb\":[15.27649769585254,77.6046082949309,117.32350230414747],\"population\":0},\"LightVibrant\":{\"rgb\":[156,198,226],\"population\":12},\"Muted\":{\"rgb\":[127,127,128],\"population\":68},\"DarkMuted\":{\"rgb\":[47,47,47],\"population\":31},\"LightMuted\":{\"rgb\":[164,188,204],\"population\":1}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":319,\"comment_count\":29,\"child_comment_count\":13,\"audio_items\":[{\"post_id\":147749119,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/147749119/tts/e1681903-b00d-4c46-b393-906654656b12/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":146761957,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Instruction Pretraining LLMs\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":\"The Latest Research in LLM Instruction Finetuning\",\"type\":\"newsletter\",\"slug\":\"instruction-pretraining-llms\",\"post_date\":\"2024-07-20T11:11:11.771Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/instruction-pretraining-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":183},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"The Latest Research in Instruction Finetuning\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!0QPB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9aef22-3864-4212-ac39-cf73d569831b_1600x1175.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":146761957,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"The Latest Research in Instruction Finetuning\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"A lot has happened last month: Apple announced the integration of on-device LLMs, Nvidia shared their large Nemotron model, FlashAttention-3 was announced, Google's Gemma 2 came out, and much more.\",\"wordcount\":6406,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[38.73417721518991,142.02531645569613,216.2658227848101],\"population\":0},\"DarkVibrant\":{\"rgb\":[20.14177215189876,73.853164556962,112.45822784810125],\"population\":0},\"LightVibrant\":{\"rgb\":[188,220,243],\"population\":141},\"Muted\":{\"rgb\":[142,120,115],\"population\":10},\"DarkMuted\":{\"rgb\":[76,76,76],\"population\":4},\"LightMuted\":{\"rgb\":[182,195,206],\"population\":195}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":183,\"comment_count\":17,\"child_comment_count\":5,\"audio_items\":[{\"post_id\":146761957,\"voice_id\":\"en-US-JennyNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/146761957/tts/a94d437e-f4e9-4369-832b-cd382555018b/en-US-JennyNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":145442372,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Developing an LLM: Building, Training, Finetuning\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"llms-building-training-finetuning\",\"post_date\":\"2024-06-08T13:04:07.929Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/llms-building-training-finetuning\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":360},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"A Deep Dive into the Lifecycle of LLM Development\",\"cover_image\":\"https://substackcdn.com/image/youtube/w_728,c_limit/kPGTx4wcm_w\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":145442372,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"A Deep Dive into the Lifecycle of LLM Development\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"If your weekend plans include catching up on AI developments and understanding Large Language Models (LLMs), I've prepared a 1-hour presentation on the development cycle of LLMs, covering everything from architectural implementation to the finetuning stages.\",\"wordcount\":203,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":null,\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":360,\"comment_count\":18,\"child_comment_count\":13,\"audio_items\":[{\"post_id\":145442372,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/145442372/tts/544e4725-9372-4fe2-8609-cc94c5a8b69c/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":145134347,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"LLM Research Insights: Instruction Masking and New LoRA Finetuning Experiments\",\"social_title\":\"LLM Research Insights: Instruction Masking and New LoRA Finetuning Experiments\",\"search_engine_title\":null,\"search_engine_description\":\"This article covers three new papers related to instruction finetuning and parameter-efficient finetuning with LoRA in large language models (LLMs). I work with these methods on a daily basis, so it's always exciting to see new research that provides practical insights.\",\"type\":\"newsletter\",\"slug\":\"llm-research-insights-instruction\",\"post_date\":\"2024-06-02T11:03:45.456Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/llm-research-insights-instruction\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":80},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Discussing the Latest Model Releases and AI Research in May 2024\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/5bfabe9e-ade8-45e9-974b-1b53452808bb_2046x1370.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":145134347,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Discussing the Latest Model Releases and AI Research in May 2024\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"This month, I am covering three new papers related to instruction finetuning and parameter-efficient finetuning with LoRA in large language models (LLMs). I work with these methods on a daily basis, so it's always exciting to see new research that provides practical insights.\",\"wordcount\":4043,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[127.5,127.5,127.5],\"population\":0},\"DarkVibrant\":{\"rgb\":[66.3,66.3,66.3],\"population\":0},\"LightVibrant\":{\"rgb\":[188.7,188.7,188.7],\"population\":0},\"Muted\":{\"rgb\":[124,124,124],\"population\":135},\"DarkMuted\":{\"rgb\":[66,66,66],\"population\":7},\"LightMuted\":{\"rgb\":[188,188,188],\"population\":65}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":80,\"comment_count\":9,\"child_comment_count\":3,\"audio_items\":[{\"post_id\":145134347,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/145134347/tts/b77fa56d-96a2-4f87-b365-df7cd4c39fb1/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":144110727,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?\",\"social_title\":\"How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"how-good-are-the-latest-open-llms\",\"post_date\":\"2024-05-12T11:02:46.703Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":120},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Discussing the Latest Model Releases and AI Research in April 2024\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/399d678a-7dbf-4533-874c-7300fb03830e_1034x610.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":144110727,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Discussing the Latest Model Releases and AI Research in April 2024\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"April 2024, what a month! My birthday, a new book release, spring is finally here, and four major open LLM releases: Mixtral, Meta AI's Llama 3, Microsoft's Phi-3, and Apple's OpenELM.\",\"wordcount\":5326,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[252,137,52],\"population\":3},\"DarkVibrant\":{\"rgb\":[4,76,132],\"population\":8},\"LightVibrant\":{\"rgb\":[246,196,152],\"population\":13},\"Muted\":{\"rgb\":[115,150,176],\"population\":10},\"DarkMuted\":{\"rgb\":[80,76,68],\"population\":2},\"LightMuted\":{\"rgb\":[156,188,204],\"population\":8}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":120,\"comment_count\":4,\"child_comment_count\":2,\"audio_items\":[{\"post_id\":144110727,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/144110727/tts/cbe2afdb-8d4b-4390-9b24-1098e846c012/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":143687732,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Using and Finetuning Pretrained Transformers\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"using-and-finetuning-pretrained-transformers\",\"post_date\":\"2024-04-20T11:02:21.015Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/using-and-finetuning-pretrained-transformers\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":126},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/9a2f144c-3665-4f77-81bb-9bbbbbe756b6_1604x1224.jpeg\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":143687732,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"What are the different ways to use and finetune pretrained large language models (LLMs)?\u00A0The most common ways to use and finetune pretrained LLMs include a feature-based approach, in-context prompting, and updating a subset of the model parameters.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"This week has been filled with developments, including exciting new AI research that I\u2019ll be discussing in my usual end-of-month write-ups.\",\"wordcount\":3396,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[16,165,252],\"population\":91},\"DarkVibrant\":{\"rgb\":[10,80,133],\"population\":173},\"LightVibrant\":{\"rgb\":[116,204,252],\"population\":1},\"Muted\":{\"rgb\":[94,139,173],\"population\":81},\"DarkMuted\":{\"rgb\":[60,60,60],\"population\":5},\"LightMuted\":{\"rgb\":[131,164,188],\"population\":17}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":126,\"comment_count\":13,\"child_comment_count\":5,\"audio_items\":[{\"post_id\":143687732,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/143687732/tts/31663b91-3ae8-44b8-8959-332e303e0794/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":142924793,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Tips for LLM Pretraining and Evaluating Reward Models\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":\"Discussing Simple and Scalable Strategies to Continually Pre-train Large Language Models and Evaluating Reward Modeling for Language Modeling.\",\"type\":\"newsletter\",\"slug\":\"tips-for-llm-pretraining-and-evaluating-rms\",\"post_date\":\"2024-03-31T11:02:29.688Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":142},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"subtitle\":\"Discussing AI Research Papers in March 2024\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!stc4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fabd9ec-29f1-4c1c-85fb-8781e7c6ce0b_1600x436.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":142924793,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Discussing AI Research Papers in March 2024\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"It's another month in AI research, and it's hard to pick favorites.\",\"wordcount\":5099,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[249,98,75],\"population\":87},\"DarkVibrant\":{\"rgb\":[175,68,53],\"population\":14},\"LightVibrant\":{\"rgb\":[148,212,252],\"population\":355},\"Muted\":{\"rgb\":[95,133,160],\"population\":31},\"DarkMuted\":{\"rgb\":[92,48,45],\"population\":10},\"LightMuted\":{\"rgb\":[195,172,196],\"population\":349}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":142,\"comment_count\":16,\"child_comment_count\":9,\"audio_items\":[{\"post_id\":142924793,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/142924793/tts/98044847-f8b9-4395-911c-fbd39dd0af1e/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false}],\"numRecommendations\":13,\"pinnedPosts\":[{\"id\":168650848,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"The Big LLM Architecture Comparison\",\"social_title\":\"The Big LLM Architecture Comparison\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"the-big-llm-architecture-comparison\",\"post_date\":\"2025-07-19T11:11:10.901Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[1174659],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":1181},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":1,\"subtitle\":\"From DeepSeek-V3 to gpt-oss: A Look At Modern LLM Architecture Design\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/45c50202-0e8b-4e64-8296-4e2ccf4cb287_1756x1227.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":168650848,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"It has been seven years since the original GPT architecture was developed. At first glance, looking back at GPT-2 (2019) and forward to DeepSeek-V3 and Llama 4 (2024-2025), one might be surprised at how structurally similar these models still are.\",\"wordcount\":7249,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":{\"post_id\":168650848,\"starts_at\":\"2025-07-19T11:11:11.298Z\",\"ended_at\":\"2025-07-19T12:11:11.487Z\",\"planned_duration_ms\":3600000,\"test_cohort_percent\":5,\"test_cohort_emailed\":true,\"main_cohort_emailed\":true,\"created_at\":\"2025-07-18T17:48:26.613Z\",\"updated_at\":\"2025-07-19T12:11:26.309Z\",\"creator_id\":27393275,\"ended_early\":false,\"winner_overridden\":false},\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[89,193,146],\"population\":57},\"DarkVibrant\":{\"rgb\":[28,25,68],\"population\":350},\"LightVibrant\":{\"rgb\":[251,228,197],\"population\":9},\"Muted\":{\"rgb\":[100,116,148],\"population\":2},\"DarkMuted\":{\"rgb\":[94,99,128],\"population\":49},\"LightMuted\":{\"rgb\":[189,168,205],\"population\":245}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":1181,\"comment_count\":60,\"child_comment_count\":24,\"audio_items\":[{\"post_id\":168650848,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/168650848/tts/582abd24-ce98-43b6-81ac-1339a0d403d5/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false}],\"postsByGroupId\":{},\"recommendations\":[{\"id\":1101061,\"recommended_publication_id\":1092659,\"recommending_publication_id\":1174659,\"created_at\":\"2023-05-11T15:25:32.918Z\",\"updated_at\":\"2023-05-11T15:25:32.918Z\",\"blurb_active\":true,\"description\":\"Hands-down my favorite newsletter for technical deep learning content!\",\"email_sent_at\":\"2023-05-11T16:41:31.050Z\",\"recommendedPublication\":{\"id\":1092659,\"name\":\"Deep (Learning) Focus\",\"subdomain\":\"cameronrwolfe\",\"custom_domain\":null,\"custom_domain_optional\":false,\"hero_text\":\"I contextualize and explain important topics in AI research.\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/ab9b43fb-52d5-40da-995d-5b7cd3f91064_896x896.png\",\"author_id\":29736521,\"primary_user_id\":29736521,\"theme_var_background_pop\":\"#6C0095\",\"created_at\":\"2022-09-17T15:12:33.160Z\",\"email_from_name\":\"Deep (Learning) Focus\",\"copyright\":\"Cameron R. Wolfe\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false,\"author\":{\"id\":29736521,\"name\":\"Cameron R. Wolfe, Ph.D.\",\"handle\":\"cwolferesearch\",\"previous_name\":\"Cameron R. Wolfe\",\"photo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/69aba7df-b571-4609-aa47-fc2d031c11b8_1242x1595.jpeg\",\"bio\":\"Research @ Netflix \u2022 Rice University PhD \u2022 I make AI understandable\",\"profile_set_up_at\":\"2022-09-17T15:11:34.083Z\",\"reader_installed_at\":\"2023-01-10T11:25:00.723Z\"}},\"subscribe_auth_token\":\"eyJwdWJJZHMiOlsxMDkyNjU5XSwiaWF0IjoxNzU4NDg1MzAwLCJleHAiOjE3NTg1NzE3MDAsInN1YiI6InN1YnNjcmliZV9hdXRoX3Rva2VuIn0.gyP-EG-IOTuTspz73iSQ66eHdAHGv3y3LGhHt7KFH_o\"},{\"id\":1282159,\"recommended_publication_id\":1084089,\"recommending_publication_id\":1174659,\"created_at\":\"2023-06-28T14:00:45.447Z\",\"updated_at\":\"2023-06-28T14:00:45.447Z\",\"blurb_active\":false,\"description\":null,\"email_sent_at\":\"2023-06-28T16:19:27.364Z\",\"recommendedPublication\":{\"id\":1084089,\"name\":\"Latent.Space\",\"subdomain\":\"swyx\",\"custom_domain\":\"www.latent.space\",\"custom_domain_optional\":false,\"hero_text\":\"The AI Engineer newsletter + Top 10 US Tech podcast. Exploring AI UX, Agents, Devtools, Infra, Open Source Models. See https://latent.space/about for highlights from Chris Lattner, Andrej Karpathy, George Hotz, Simon Willison, Soumith Chintala et al!\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/73b0838a-bd14-46a1-801c-b6a2046e5c1e_1130x1130.png\",\"author_id\":89230629,\"primary_user_id\":89230629,\"theme_var_background_pop\":\"#0068EF\",\"created_at\":\"2022-09-12T05:38:09.694Z\",\"email_from_name\":\"Latent.Space\",\"copyright\":\"Latent.Space\",\"founding_plan_name\":\"Latent Spacenaut\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"magaziney\",\"is_personal_mode\":false,\"author\":{\"id\":89230629,\"name\":\"Latent.Space\",\"handle\":\"swyx\",\"previous_name\":\"swyx\",\"photo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/703cf3dd-3bab-4f7b-86fa-f4443f15f8a4_152x152.jpeg\",\"bio\":\"Writer, curator, latent space explorer. Main blog: https://swyx.io Devrel/Dev community: https://dx.tips/ Twitter: https://twitter.com/swyx\",\"profile_set_up_at\":\"2022-04-29T22:19:27.544Z\",\"reader_installed_at\":\"2022-12-28T23:31:54.445Z\"}},\"subscribe_auth_token\":\"eyJwdWJJZHMiOlsxMDg0MDg5XSwiaWF0IjoxNzU4NDg1MzAwLCJleHAiOjE3NTg1NzE3MDAsInN1YiI6InN1YnNjcmliZV9hdXRoX3Rva2VuIn0.dT14L5AAaVAO8g3FOU1ficX6aZNTTx_qzSKOYD1vnfY\"},{\"id\":634697,\"recommended_publication_id\":1176501,\"recommending_publication_id\":1174659,\"created_at\":\"2023-01-19T15:48:27.585Z\",\"updated_at\":\"2023-01-19T15:48:27.585Z\",\"blurb_active\":false,\"description\":null,\"email_sent_at\":\"2023-01-19T16:41:01.867Z\",\"recommendedPublication\":{\"id\":1176501,\"name\":\"The Palindrome\",\"subdomain\":\"thepalindrome\",\"custom_domain\":\"thepalindrome.org\",\"custom_domain_optional\":false,\"hero_text\":\"mathematics \u222A machine learning\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f8b68cf8-d3f4-42f6-b8dd-cccde036005f_720x720.png\",\"author_id\":10322584,\"primary_user_id\":10322584,\"theme_var_background_pop\":\"#9D6FFF\",\"created_at\":\"2022-11-05T19:02:46.937Z\",\"email_from_name\":\"The Palindrome\",\"copyright\":\"Tivadar Danka\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false,\"author\":{\"id\":10322584,\"name\":\"Tivadar Danka\",\"handle\":\"tivadardanka\",\"previous_name\":\"Tivadar\",\"photo_url\":\"https://substackcdn.com/image/fetch/$s_!09ow!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3b26cd48-153a-4207-b1e3-e14e1ec8d5e8_400x400.jpeg\",\"bio\":\"Just an Eastern European punk, writing about tech, math, and machine learning. INTJ personality. Chaotic good.\",\"profile_set_up_at\":\"2022-11-05T18:59:57.000Z\",\"reader_installed_at\":\"2022-12-09T10:24:21.362Z\"}},\"subscribe_auth_token\":\"eyJwdWJJZHMiOlsxMTc2NTAxXSwiaWF0IjoxNzU4NDg1MzAwLCJleHAiOjE3NTg1NzE3MDAsInN1YiI6InN1YnNjcmliZV9hdXRoX3Rva2VuIn0.FGIdSdwM9-zDBC3ZaNywaEHSzvw8ZTqH3EV8rSv5V5w\"},{\"id\":581053,\"recommended_publication_id\":1273940,\"recommending_publication_id\":1174659,\"created_at\":\"2023-01-06T13:02:34.335Z\",\"updated_at\":\"2023-01-06T13:02:34.335Z\",\"blurb_active\":false,\"description\":\"I really enjoyed reading the insightful essays that discuss the limitations of current AI language models.\",\"email_sent_at\":\"2023-01-06T13:39:44.723Z\",\"recommendedPublication\":{\"id\":1273940,\"name\":\"AI: A Guide for Thinking Humans\",\"subdomain\":\"aiguide\",\"custom_domain\":null,\"custom_domain_optional\":false,\"hero_text\":\"I write about interesting new developments in AI.\",\"logo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/b8e2ab82-bc00-40f5-9d8b-d818e893dda0_883x883.png\",\"author_id\":15187849,\"primary_user_id\":15187849,\"theme_var_background_pop\":\"#B599F1\",\"created_at\":\"2022-12-30T19:41:21.051Z\",\"email_from_name\":null,\"copyright\":\"Melanie Mitchell\",\"founding_plan_name\":null,\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"disabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false,\"author\":{\"id\":15187849,\"name\":\"Melanie Mitchell\",\"handle\":\"aiguide\",\"previous_name\":null,\"photo_url\":\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/9546a58b-b372-439d-aa96-9e3b01dbba61_1070x883.jpeg\",\"bio\":\"Melanie Mitchell, Professor at the Santa Fe Institute, is the award-winning author of Artificial Intelligence: A Guide for Thinking Humans. She works in the fields of AI, cognitive science, and complex systems.\",\"profile_set_up_at\":\"2022-12-30T19:40:31.037Z\",\"reader_installed_at\":null}},\"subscribe_auth_token\":\"eyJwdWJJZHMiOlsxMjczOTQwXSwiaWF0IjoxNzU4NDg1MzAwLCJleHAiOjE3NTg1NzE3MDAsInN1YiI6InN1YnNjcmliZV9hdXRoX3Rva2VuIn0.uAfxkkWQBUdU_eSBUDUYkB8USoP3hGHG4F-pEm378fg\"},{\"id\":4458001,\"recommended_publication_id\":48206,\"recommending_publication_id\":1174659,\"created_at\":\"2024-11-05T14:45:02.555Z\",\"updated_at\":\"2024-11-05T14:45:02.555Z\",\"blurb_active\":false,\"description\":\"Good and timely coverage of LLM related happenings with a nice focus on evaluations and RLHF research.\",\"email_sent_at\":\"2024-11-05T15:32:21.004Z\",\"recommendedPublication\":{\"id\":48206,\"name\":\"Interconnects\",\"subdomain\":\"robotic\",\"custom_domain\":\"www.interconnects.ai\",\"custom_domain_optional\":false,\"hero_text\":\"The cutting edge of AI, from inside the frontier AI labs, minus the hype. The border between high-level and technical thinking. Read by leading engineers, researchers, and investors on Wednesday mornings.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/e70f9dbf-4fe6-404c-b6bb-1831d1b7ed0b_590x590.png\",\"author_id\":10472909,\"primary_user_id\":10472909,\"theme_var_background_pop\":\"#ff6b00\",\"created_at\":\"2020-05-21T02:59:47.895Z\",\"email_from_name\":\"Interconnects by Nathan Lambert\",\"copyright\":\"Interconnects AI, LLC\",\"founding_plan_name\":\"Founding Member\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false,\"author\":{\"id\":10472909,\"name\":\"Nathan Lambert\",\"handle\":\"natolambert\",\"previous_name\":null,\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fedcdfb-e137-4f6a-9089-a46add6c6242_500x500.jpeg\",\"bio\":\"ML researcher making sense of AI research, products, and the uncertain technological future. PhD from Berkeley AI. Experience at Meta, DeepMind, HuggingFace.\",\"profile_set_up_at\":\"2021-04-24T01:19:33.371Z\",\"reader_installed_at\":\"2022-03-09T17:52:30.690Z\"}},\"subscribe_auth_token\":\"eyJwdWJJZHMiOls0ODIwNl0sImlhdCI6MTc1ODQ4NTMwMCwiZXhwIjoxNzU4NTcxNzAwLCJzdWIiOiJzdWJzY3JpYmVfYXV0aF90b2tlbiJ9.Y-wo9TCz0KDcZxhGn7FbmVpdHOKehN-kyuvQTvOs7do\"}],\"topPosts\":[{\"id\":168650848,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"The Big LLM Architecture Comparison\",\"social_title\":\"The Big LLM Architecture Comparison\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"the-big-llm-architecture-comparison\",\"post_date\":\"2025-07-19T11:11:10.901Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[1174659],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":1181},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":1,\"subtitle\":\"From DeepSeek-V3 to gpt-oss: A Look At Modern LLM Architecture Design\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/45c50202-0e8b-4e64-8296-4e2ccf4cb287_1756x1227.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":168650848,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"It has been seven years since the original GPT architecture was developed. At first glance, looking back at GPT-2 (2019) and forward to DeepSeek-V3 and Llama 4 (2024-2025), one might be surprised at how structurally similar these models still are.\",\"wordcount\":7249,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":{\"post_id\":168650848,\"starts_at\":\"2025-07-19T11:11:11.298Z\",\"ended_at\":\"2025-07-19T12:11:11.487Z\",\"planned_duration_ms\":3600000,\"test_cohort_percent\":5,\"test_cohort_emailed\":true,\"main_cohort_emailed\":true,\"created_at\":\"2025-07-18T17:48:26.613Z\",\"updated_at\":\"2025-07-19T12:11:26.309Z\",\"creator_id\":27393275,\"ended_early\":false,\"winner_overridden\":false},\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[89,193,146],\"population\":57},\"DarkVibrant\":{\"rgb\":[28,25,68],\"population\":350},\"LightVibrant\":{\"rgb\":[251,228,197],\"population\":9},\"Muted\":{\"rgb\":[100,116,148],\"population\":2},\"DarkMuted\":{\"rgb\":[94,99,128],\"population\":49},\"LightMuted\":{\"rgb\":[189,168,205],\"population\":245}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":1181,\"comment_count\":60,\"child_comment_count\":24,\"audio_items\":[{\"post_id\":168650848,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/168650848/tts/582abd24-ce98-43b6-81ac-1339a0d403d5/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":156484949,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding Reasoning LLMs\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"understanding-reasoning-llms\",\"post_date\":\"2025-02-05T12:11:39.216Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/understanding-reasoning-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":1098},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":2,\"subtitle\":\"Methods and Strategies for Building and Refining Reasoning Models\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!QwUc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":156484949,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Methods and Strategies for Building and Refining Reasoning Models\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"This article describes the four main approaches to building reasoning models, or how we can enhance LLMs with reasoning capabilities. I hope this provides valuable insights and helps you navigate the rapidly evolving literature and hype surrounding this topic.\",\"wordcount\":4028,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[169,25,6],\"population\":87},\"DarkVibrant\":{\"rgb\":[128.0537142857143,18.942857142857143,4.546285714285711],\"population\":0},\"LightVibrant\":{\"rgb\":[252,235,228],\"population\":408},\"Muted\":{\"rgb\":[116,132,148],\"population\":1},\"DarkMuted\":{\"rgb\":[68,68,68],\"population\":7},\"LightMuted\":{\"rgb\":[164,188,200],\"population\":2}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":1098,\"comment_count\":40,\"child_comment_count\":21,\"audio_items\":[{\"post_id\":156484949,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/156484949/tts/410051a7-e938-4017-8671-68e5f1fae779/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":140464659,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":\"This article codes the self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama from scratch in PyTorch.\",\"type\":\"newsletter\",\"slug\":\"understanding-and-coding-self-attention\",\"post_date\":\"2024-01-14T11:55:06.449Z\",\"audience\":\"only_paid\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":386},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":3,\"subtitle\":\"\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":null,\"videoUpload\":null,\"podcastFields\":{\"post_id\":140464659,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama. Self-attention and related mechanisms are core components of LLMs, making them a useful topic to understand when working with these models.\",\"wordcount\":4738,\"postTags\":[{\"id\":\"2e792e1c-c4e1-4a7c-a303-bdac25c1f3a8\",\"publication_id\":1174659,\"name\":\"Vision Transformers\",\"slug\":\"vision-transformers\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[172,144,204],\"population\":2},\"DarkVibrant\":{\"rgb\":[64.66296296296294,41.74444444444444,90.85555555555557],\"population\":0},\"LightVibrant\":{\"rgb\":[204,173,241],\"population\":13},\"Muted\":{\"rgb\":[104,104,128],\"population\":2},\"DarkMuted\":{\"rgb\":[102,97,112],\"population\":6},\"LightMuted\":{\"rgb\":[178,212,212],\"population\":5}},\"publishedBylines\":[],\"reaction\":false,\"reaction_count\":386,\"comment_count\":41,\"child_comment_count\":18,\"audio_items\":[{\"post_id\":140464659,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":null,\"type\":\"tts\",\"status\":\"paywalled\"}],\"is_geoblocked\":false,\"hidden\":true,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":115060492,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding Large Language Models\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":\"Explore the transformative power of large language models in AI. Dive into a curated reading list for ML enthusiasts. Discover the impact of transformers on NLP, vision, and biology.\",\"type\":\"newsletter\",\"slug\":\"understanding-large-language-models\",\"post_date\":\"2023-04-16T12:33:46.854Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/understanding-large-language-models\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":922},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":4,\"subtitle\":\"A Cross-Section of the Most Relevant Literature To Get Up to Speed\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/d9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":115060492,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"A Cross-Section of the Most Relevant Literature To Get Up to Speed\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"Large language models have taken the public attention by storm \u2013 no pun intended. In just half a decade large language models \u2013 transformers \u2013 have almost completely changed the field of natural language processing. Moreover, they have also begun to revolutionize fields such as computer vision and computational biology.\",\"wordcount\":3347,\"postTags\":[],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[172,120,228],\"population\":6},\"DarkVibrant\":{\"rgb\":[64.66296296296302,22.100000000000016,110.49999999999999],\"population\":0},\"LightVibrant\":{\"rgb\":[188,148,236],\"population\":1},\"Muted\":{\"rgb\":[148,116,188],\"population\":1},\"DarkMuted\":{\"rgb\":[116,108,108],\"population\":1},\"LightMuted\":{\"rgb\":[204,204,164],\"population\":2}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":922,\"comment_count\":53,\"child_comment_count\":25,\"audio_items\":[{\"post_id\":115060492,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/115060492/tts/1c050ea7-59a1-42f7-aae5-0db5e030919e/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":170506328,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"From GPT-2 to gpt-oss: Analyzing the Architectural Advances\",\"social_title\":\"From GPT-2 to gpt-oss: Analyzing the Architectural Advances\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"from-gpt-2-to-gpt-oss-analyzing-the\",\"post_date\":\"2025-08-09T11:23:07.237Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":563},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":5,\"subtitle\":\"And How They Stack Up Against Qwen3\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/529c4cc7-161c-4d7c-b186-06e68c771776_1564x926.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":170506328,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"And How They Stack Up Against Qwen3\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"OpenAI just released their new open-weight LLMs this week: gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019. And yes, thanks to some clever optimizations, they can run locally (but more about this later).\",\"wordcount\":5221,\"postTags\":[{\"id\":\"5cf0ea83-392e-495a-9745-dc0aaa7f7d4f\",\"publication_id\":1174659,\"name\":\"Reasoning Models\",\"slug\":\"reasoning-models\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[92,199,180],\"population\":16},\"DarkVibrant\":{\"rgb\":[33.90684931506849,98.69315068493151,87.18904109589042],\"population\":0},\"LightVibrant\":{\"rgb\":[135,164,235],\"population\":14},\"Muted\":{\"rgb\":[100,116,148],\"population\":1},\"DarkMuted\":{\"rgb\":[68,68,68],\"population\":5},\"LightMuted\":{\"rgb\":[135,188,181],\"population\":318}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":563,\"comment_count\":45,\"child_comment_count\":23,\"audio_items\":[{\"post_id\":170506328,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/170506328/tts/d2f3c50e-08e5-4c36-a942-d3a52acf3214/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":138081202,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"practical-tips-for-finetuning-llms\",\"post_date\":\"2023-11-19T12:11:26.382Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":295},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":6,\"subtitle\":\"Things I Learned From Hundreds of Experiments\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!i3QH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":138081202,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Things I Learned From Hundreds of Experiments\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"Low-rank adaptation (LoRA) is among the most widely used and effective techniques for efficiently training custom LLMs. For those interested in open-source LLMs, it's an essential technique worth familiarizing oneself with.\",\"wordcount\":3592,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[5,116,187],\"population\":193},\"DarkVibrant\":{\"rgb\":[4,100,156],\"population\":1},\"LightVibrant\":{\"rgb\":[169,203,227],\"population\":40},\"Muted\":{\"rgb\":[127,127,127],\"population\":68},\"DarkMuted\":{\"rgb\":[68,68,68],\"population\":4},\"LightMuted\":{\"rgb\":[188,188,188],\"population\":42}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":295,\"comment_count\":48,\"child_comment_count\":20,\"audio_items\":[{\"post_id\":138081202,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/138081202/tts/947b81e4-5402-4fa3-b078-19b86a1b0a74/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":151078631,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding Multimodal LLMs\",\"social_title\":\"Understanding Multimodal LLMs\",\"search_engine_title\":null,\"search_engine_description\":\"An introduction to the main multimodal LLM techniques and latest models\",\"type\":\"newsletter\",\"slug\":\"understanding-multimodal-llms\",\"post_date\":\"2024-11-03T12:44:00.421Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/understanding-multimodal-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":540},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":7,\"subtitle\":\"An introduction to the main techniques and latest models\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/c534f387-f776-41eb-9c65-f0032b91daee_1988x1430.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":151078631,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"An introduction to the main techniques and latest models\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"It was a wild two months. There have once again been many developments in AI research, with two Nobel Prizes awarded to AI and several interesting research papers published.\",\"wordcount\":4421,\"postTags\":[{\"id\":\"1496f84a-084b-4bf8-9930-a33eca71b372\",\"publication_id\":1174659,\"name\":\"Computer Vision\",\"slug\":\"computer-vision\",\"hidden\":false},{\"id\":\"2e792e1c-c4e1-4a7c-a303-bdac25c1f3a8\",\"publication_id\":1174659,\"name\":\"Vision Transformers\",\"slug\":\"vision-transformers\",\"hidden\":false},{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[180,22,5],\"population\":151},\"DarkVibrant\":{\"rgb\":[4,116,188],\"population\":58},\"LightVibrant\":{\"rgb\":[197,222,239],\"population\":75},\"Muted\":{\"rgb\":[145,103,93],\"population\":24},\"DarkMuted\":{\"rgb\":[98,73,50],\"population\":17},\"LightMuted\":{\"rgb\":[207,178,173],\"population\":41}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":540,\"comment_count\":57,\"child_comment_count\":28,\"audio_items\":[{\"post_id\":151078631,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/151078631/tts/1548c000-1339-4ad7-9834-b5616d93e113/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":162934205,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Coding LLMs from the Ground Up: A Complete Course\",\"social_title\":\"Coding LLMs from the Ground Up: A Complete Course\",\"search_engine_title\":null,\"search_engine_description\":\"Why build an LLM from scratch? It's probably the best and most efficient way to learn how LLMs really work. Plus, many readers have told me they had a lot of fun doing it.\",\"type\":\"newsletter\",\"slug\":\"coding-llms-from-the-ground-up\",\"post_date\":\"2025-05-10T11:03:17.769Z\",\"audience\":\"only_paid\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":true,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/coding-llms-from-the-ground-up\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":233},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":8,\"subtitle\":\"\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/752b6486-73db-4293-9465-854d4a005a5c_2406x1350.jpeg\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":null,\"videoUpload\":null,\"podcastFields\":{\"post_id\":162934205,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Why build LLMs from scratch? It's probably the best and most efficient way to learn how LLMs really work. Plus, many readers have told me they had a lot of fun doing it.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"I wrote a lot about reasoning models in recent months (4 articles in a row)! Next to everything \\\"agentic,\\\" reasoning is one of the biggest LLM topics of 2025.\",\"wordcount\":997,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":false,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[57.24489795918357,130.10204081632648,197.75510204081644],\"population\":0},\"DarkVibrant\":{\"rgb\":[29.767346938775457,67.65306122448978,102.83265306122455],\"population\":0},\"LightVibrant\":{\"rgb\":[217,231,244],\"population\":9},\"Muted\":{\"rgb\":[108,116,132],\"population\":2},\"DarkMuted\":{\"rgb\":[52,52,52],\"population\":5},\"LightMuted\":{\"rgb\":[156,180,204],\"population\":110}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":233,\"comment_count\":4,\"child_comment_count\":3,\"audio_items\":[{\"post_id\":162934205,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":null,\"type\":\"tts\",\"status\":\"paywalled\"}],\"is_geoblocked\":false,\"hidden\":true,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":148329414,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Building LLMs from the Ground Up: A 3-hour Coding Workshop\",\"social_title\":\"Building LLMs from the Ground Up: A 3-hour Coding Workshop\",\"search_engine_title\":null,\"search_engine_description\":\"A 3-hour coding workshop presentation on implementing, training, and using LLMs.\",\"type\":\"newsletter\",\"slug\":\"building-llms-from-the-ground-up\",\"post_date\":\"2024-08-31T10:39:35.940Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":433},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":9,\"subtitle\":\"\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/367b547c-9d22-4a3d-b466-1d56ccc6b055_1844x1224.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":148329414,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"If your weekend plans include catching up on AI developments and understanding Large Language Models (LLMs), I've prepared a 1-hour presentation on the development cycle of LLMs, covering everything from architectural implementation to the finetuning stages.\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"If you\u2019d like to spend a few hours this weekend to dive into Large Language Models (LLMs) and understand how they work, I've prepared a 3-hour coding workshop presentation on implementing, training, and using LLMs.\",\"wordcount\":343,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"b0939572-2637-42f9-8d08-9792c2228a8c\",\"publication_id\":1174659,\"name\":\"PyTorch\",\"slug\":\"pytorch\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[204,124,164],\"population\":34},\"DarkVibrant\":{\"rgb\":[64,40,20],\"population\":2},\"LightVibrant\":{\"rgb\":[236,143,189],\"population\":45},\"Muted\":{\"rgb\":[101,130,152],\"population\":17},\"DarkMuted\":{\"rgb\":[84,44,72],\"population\":2},\"LightMuted\":{\"rgb\":[167,178,191],\"population\":23}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":433,\"comment_count\":16,\"child_comment_count\":11,\"audio_items\":[{\"post_id\":148329414,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/148329414/tts/ead5fd00-a385-4a95-a7dc-9ff942b29fb3/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false}]},\"maybeLaunchWelcomePage\":true,\"topThreePosts\":[{\"id\":168650848,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"The Big LLM Architecture Comparison\",\"social_title\":\"The Big LLM Architecture Comparison\",\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"the-big-llm-architecture-comparison\",\"post_date\":\"2025-07-19T11:11:10.901Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[1174659],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":1181},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":1,\"subtitle\":\"From DeepSeek-V3 to gpt-oss: A Look At Modern LLM Architecture Design\",\"cover_image\":\"https://substack-post-media.s3.amazonaws.com/public/images/45c50202-0e8b-4e64-8296-4e2ccf4cb287_1756x1227.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":168650848,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"It has been seven years since the original GPT architecture was developed. At first glance, looking back at GPT-2 (2019) and forward to DeepSeek-V3 and Llama 4 (2024-2025), one might be surprised at how structurally similar these models still are.\",\"wordcount\":7249,\"postTags\":[{\"id\":\"3c6fd695-d92d-4589-888a-ce59b6cebe46\",\"publication_id\":1174659,\"name\":\"Deep Learning\",\"slug\":\"deep-learning\",\"hidden\":false},{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"67aecb81-dcbe-4fed-8c4a-eee7d97c5bb0\",\"publication_id\":1174659,\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"hidden\":false},{\"id\":\"6e0b1665-018b-43ee-a244-f438573378e1\",\"publication_id\":1174659,\"name\":\"Attention\",\"slug\":\"attention\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false},{\"id\":\"e904cab3-967f-4c2e-bda2-b44d159bc31d\",\"publication_id\":1174659,\"name\":\"Open-Source\",\"slug\":\"open-source\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":{\"post_id\":168650848,\"starts_at\":\"2025-07-19T11:11:11.298Z\",\"ended_at\":\"2025-07-19T12:11:11.487Z\",\"planned_duration_ms\":3600000,\"test_cohort_percent\":5,\"test_cohort_emailed\":true,\"main_cohort_emailed\":true,\"created_at\":\"2025-07-18T17:48:26.613Z\",\"updated_at\":\"2025-07-19T12:11:26.309Z\",\"creator_id\":27393275,\"ended_early\":false,\"winner_overridden\":false},\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[89,193,146],\"population\":57},\"DarkVibrant\":{\"rgb\":[28,25,68],\"population\":350},\"LightVibrant\":{\"rgb\":[251,228,197],\"population\":9},\"Muted\":{\"rgb\":[100,116,148],\"population\":2},\"DarkMuted\":{\"rgb\":[94,99,128],\"population\":49},\"LightMuted\":{\"rgb\":[189,168,205],\"population\":245}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":1181,\"comment_count\":60,\"child_comment_count\":24,\"audio_items\":[{\"post_id\":168650848,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/168650848/tts/582abd24-ce98-43b6-81ac-1339a0d403d5/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false},{\"id\":156484949,\"editor_v2\":false,\"publication_id\":1174659,\"title\":\"Understanding Reasoning LLMs\",\"social_title\":null,\"search_engine_title\":null,\"search_engine_description\":null,\"type\":\"newsletter\",\"slug\":\"understanding-reasoning-llms\",\"post_date\":\"2025-02-05T12:11:39.216Z\",\"audience\":\"everyone\",\"podcast_duration\":null,\"video_upload_id\":null,\"podcast_upload_id\":null,\"write_comment_permissions\":\"everyone\",\"should_send_free_preview\":false,\"free_unlock_required\":false,\"default_comment_sort\":null,\"canonical_url\":\"https://magazine.sebastianraschka.com/p/understanding-reasoning-llms\",\"section_id\":null,\"top_exclusions\":[],\"pins\":[],\"is_section_pinned\":false,\"section_slug\":null,\"section_name\":null,\"reactions\":{\"\u2764\":1098},\"restacked_post_id\":null,\"restacked_post_slug\":null,\"restacked_pub_name\":null,\"restacked_pub_logo_url\":null,\"position\":2,\"subtitle\":\"Methods and Strategies for Building and Refining Reasoning Models\",\"cover_image\":\"https://substackcdn.com/image/fetch/$s_!QwUc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png\",\"cover_image_is_square\":false,\"cover_image_is_explicit\":false,\"podcast_url\":\"\",\"videoUpload\":null,\"podcastFields\":{\"post_id\":156484949,\"podcast_episode_number\":null,\"podcast_season_number\":null,\"podcast_episode_type\":null,\"should_syndicate_to_other_feed\":null,\"syndicate_to_section_id\":null,\"hide_from_feed\":false,\"free_podcast_url\":null,\"free_podcast_duration\":null},\"podcast_preview_upload_id\":null,\"podcastUpload\":null,\"podcastPreviewUpload\":null,\"voiceover_upload_id\":null,\"voiceoverUpload\":null,\"has_voiceover\":false,\"description\":\"Methods and Strategies for Building and Refining Reasoning Models\",\"body_json\":null,\"body_html\":null,\"truncated_body_text\":\"This article describes the four main approaches to building reasoning models, or how we can enhance LLMs with reasoning capabilities. I hope this provides valuable insights and helps you navigate the rapidly evolving literature and hype surrounding this topic.\",\"wordcount\":4028,\"postTags\":[{\"id\":\"5e263784-abbd-4549-a8c3-69a981a6c7f2\",\"publication_id\":1174659,\"name\":\"AI\",\"slug\":\"ai\",\"hidden\":false},{\"id\":\"6fc1db5b-5505-4e83-8cc9-00977c4f24e3\",\"publication_id\":1174659,\"name\":\"AI Research\",\"slug\":\"ai-research\",\"hidden\":false},{\"id\":\"c396248a-8283-4a1c-90cb-4cc6677a0260\",\"publication_id\":1174659,\"name\":\"Large language models\",\"slug\":\"large-language-models\",\"hidden\":false},{\"id\":\"d363cca5-e117-4aed-8ef4-0c07863fea32\",\"publication_id\":1174659,\"name\":\"LLMs\",\"slug\":\"llms\",\"hidden\":false}],\"teaser_post_eligible\":true,\"postCountryBlocks\":[],\"headlineTest\":null,\"coverImagePalette\":{\"Vibrant\":{\"rgb\":[169,25,6],\"population\":87},\"DarkVibrant\":{\"rgb\":[128.0537142857143,18.942857142857143,4.546285714285711],\"population\":0},\"LightVibrant\":{\"rgb\":[252,235,228],\"population\":408},\"Muted\":{\"rgb\":[116,132,148],\"population\":1},\"DarkMuted\":{\"rgb\":[68,68,68],\"population\":7},\"LightMuted\":{\"rgb\":[164,188,200],\"population\":2}},\"publishedBylines\":[{\"id\":27393275,\"name\":\"Sebastian Raschka, PhD\",\"handle\":\"rasbt\",\"previous_name\":\"Sebastian Raschka\",\"photo_url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg\",\"bio\":\"I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI & LLM research focusing on code-driven implementations. I am also the author of \\\"Build a Large Language Model From Scratch\\\" (amzn.to/4fqvn0D).\",\"profile_set_up_at\":\"2022-10-09T16:19:59.744Z\",\"reader_installed_at\":\"2022-11-07T19:56:32.129Z\",\"publicationUsers\":[{\"id\":1127862,\"user_id\":27393275,\"publication_id\":1174659,\"role\":\"admin\",\"public\":true,\"is_primary\":true,\"publication\":{\"id\":1174659,\"name\":\"Ahead of AI\",\"subdomain\":\"sebastianraschka\",\"custom_domain\":\"magazine.sebastianraschka.com\",\"custom_domain_optional\":false,\"hero_text\":\"Ahead of AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field.\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/49f25d0a-212b-4853-8bcb-128d0a3edbbf_1196x1196.png\",\"author_id\":27393275,\"primary_user_id\":27393275,\"theme_var_background_pop\":\"#2096FF\",\"created_at\":\"2022-11-04T18:30:05.218Z\",\"email_from_name\":null,\"copyright\":\"Raschka AI Research (RAIR) Lab LLC\",\"founding_plan_name\":\"Founding plan\",\"community_enabled\":true,\"invite_only\":false,\"payments_state\":\"enabled\",\"language\":null,\"explicit\":false,\"homepage_type\":\"newspaper\",\"is_personal_mode\":false}}],\"twitter_screen_name\":\"rasbt\",\"is_guest\":false,\"bestseller_tier\":100,\"status\":{\"bestsellerTier\":100,\"subscriberTier\":1,\"leaderboard\":null,\"vip\":false,\"badge\":{\"type\":\"bestseller\",\"tier\":100}}}],\"reaction\":false,\"reaction_count\":1098,\"comment_count\":40,\"child_comment_count\":21,\"audio_items\":[{\"post_id\":156484949,\"voice_id\":\"en-US-OnyxTurboMultilingualNeural\",\"audio_url\":\"https://substack-video.s3.amazonaws.com/video_upload/post/156484949/tts/410051a7-e938-4017-8671-68e5f1fae779/en-US-OnyxTurboMultilingualNeural.mp3\",\"type\":\"tts\",\"status\":\"completed\"}],\"is_geoblocked\":false,\"hasCashtag\":false,\"is_saved\":false,\"saved_at\":null,\"is_viewed\":false,\"read_progress\":0,\"max_read_progress\":0,\"audio_progress\":0,\"max_audio_progress\":0,\"video_progress\":0,\"max_video_progress\":0,\"restacked\":false}],\"topThreeSubscribers\":[{\"id\":2007748,\"name\":\"Olivier Ziller\",\"photo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/8191e859-21ae-4495-bf56-59156580ac72_96x96.png\",\"bestseller_tier\":null,\"primary_publication\":{\"id\":4455692,\"subdomain\":\"olivierziller734764\",\"custom_domain_optional\":false,\"name\":\"Olivier Ziller\",\"author_id\":2007748,\"user_id\":2007748,\"handles_enabled\":false,\"explicit\":false,\"is_personal_mode\":false,\"payments_state\":\"disabled\",\"pledges_enabled\":true},\"is_subscribed\":false},{\"id\":2080303,\"name\":\"Sugato Ray\",\"photo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/46d1bee2-5f77-4abd-91ba-2ffb6ce5b6cc_1284x1282.jpeg\",\"bestseller_tier\":null,\"primary_publication\":{\"id\":1135033,\"subdomain\":\"sugatoray\",\"custom_domain_optional\":false,\"name\":\"Sugato\u2019s Newsletter\",\"author_id\":2080303,\"user_id\":2080303,\"handles_enabled\":false,\"explicit\":false,\"is_personal_mode\":false,\"payments_state\":\"disabled\",\"pledges_enabled\":true},\"is_subscribed\":false},{\"id\":2712973,\"name\":\"Bernardo Neves\",\"photo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/f35cb4d1-0d20-4ddd-b2d7-8a9623188daa_1664x1664.png\",\"bestseller_tier\":null,\"primary_publication\":{\"id\":3672612,\"subdomain\":\"prescriptionai\",\"custom_domain_optional\":false,\"name\":\"Prescription AI\",\"logo_url\":\"https://substack-post-media.s3.amazonaws.com/public/images/b7226ab9-79f0-4875-976d-05b167b6e56c_1024x1024.png\",\"author_id\":2712973,\"user_id\":2712973,\"handles_enabled\":false,\"explicit\":false,\"is_personal_mode\":false,\"payments_state\":\"enabled\",\"pledges_enabled\":false},\"is_subscribed\":false}],\"twitterCardUrl\":null,\"activeLiveStream\":null,\"freeTrialCoupon\":null,\"isChatActive\":false,\"isMeetingsActive\":false,\"hasViralGiftsCount\":0,\"iba\":false,\"features\":{},\"browser\":{\"name\":\"Chrome\",\"version\":\"140.0.0.0\",\"major\":\"140\"},\"showCookieBanner\":false,\"disabledCookies\":[],\"dd_env\":\"prod\",\"dd_ti\":true}")</script>
        <script>window._analyticsConfig = JSON.parse("{\"anonymousId\":\"acef0839-2611-45cc-ab3b-ce6483a56022\",\"properties\":{\"subdomain\":\"sebastianraschka\",\"publication_id\":1174659,\"has_plans\":true,\"pub_community_enabled\":true,\"is_personal_publication\":false,\"is_subscribed\":false,\"is_free_subscribed\":false,\"is_author\":false,\"is_contributor\":false,\"is_admin\":false,\"is_founding\":false,\"country\":\"IN\",\"language\":\"en\"},\"adwordsAccountId\":\"AW-316245675\",\"adwordsEventSendTo\":\"Tf76CKqcyL4DEKuN5pYB\"}")</script>

        
        
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8909.2cd480ea.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2199.5f552266.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8546.e24694b7.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2020.01118b77.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/5621.0f954901.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4490.70115a71.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/881.17a1b129.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9106.5a053775.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/5977.e8fdd0e5.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1832.1b6d6095.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/6722.99f03731.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/247.b024ec59.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8438.d892045d.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/7491.7d7275c8.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/76.68b85afd.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8206.c027e28b.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/main.2f5c3c95.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/540.48305f0f.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2013.78bb41a5.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1553.a169fcec.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8665.6d3fde9f.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/6400.58c45c46.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/828.471f75be.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9335.d48205f9.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/6371.a9a82180.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1797.abe73545.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2250.76b28e19.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4511.273a77df.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/7833.aed8ad3f.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1481.ca002e3f.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9294.960be875.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/7248.f0da3333.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9601.706f3481.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4121.0de680ac.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/236.50c1d138.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8336.23f1718a.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2736.1a64d68d.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8491.f6cc4a9e.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/5790.4f8dd4a8.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4903.bd262ac4.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9669.41147939.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/6163.1db4d192.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9923.3d24f4ff.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1484.8ed70740.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8618.2a44cf5f.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9609.17e84027.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8041.0e22e6a7.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/6225.47ee0d5e.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/5758.1a205ca5.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/6247.078b07a1.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9366.d3c0286f.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4163.c990856e.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1262.33c17ad0.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2768.41ca10b2.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/7913.673eeb90.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1076.e716f4fe.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2344.ec68af70.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/3451.d0ad0bd5.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8549.dce26afd.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1333.d399a473.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2341.cd432220.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2909.97509e9d.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8915.9972e506.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/6364.29606904.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4036.033a5f72.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/221.6a595268.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2779.c2bec061.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1482.1a779c15.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/907.64fff851.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/5924.78fb1361.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2035.58660d40.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4388.1d6b35ab.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8381.15d661d8.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1697.16cf9de0.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2889.e27ae69a.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1354.47971ce3.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2291.f09a5dde.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/546.1e81f03f.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9760.3b93fbba.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/7065.18c0dbcf.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/6042.8ebed607.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/8038.010acf71.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4405.88ee6abc.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/3324.e3ed73f8.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/9124.59032715.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/3414.c27139fa.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/7541.c14a4f48.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/7797.cda2dec6.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/4278.ca482e8f.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/7424.95847c04.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/3191.b5bcd62d.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/2044.cc604cb5.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/3961.b9d0d3a1.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/1421.52f2b09a.js.download" charset="utf-8"></script>
            
                <script defer="" type="module" src="./The Big LLM Architecture Comparison_files/5581.21b7568d.js.download" charset="utf-8"></script>
            
        
        <script nomodule="">
            (function() {
                var message = 'Your browser does not support modern JavaScript modules. Please upgrade your browser for the best experience.';
                var warningDiv = document.createElement('div');
                warningDiv.style.color = 'red';
                warningDiv.style.padding = '10px';
                warningDiv.style.margin = '10px 0';
                warningDiv.style.border = '1px solid red';
                warningDiv.style.backgroundColor = 'lightyellow';
                warningDiv.innerText = message;
                document.body.prepend(warningDiv);
            })();
        </script>

        
            <!-- Datadog Analytics -->
            <script>
              (function(h,o,u,n,d) {
                h=h[d]=h[d]||{q:[],onReady:function(c){h.q.push(c)}}
                d=o.createElement(u);d.async=1;d.src=n
                n=o.getElementsByTagName(u)[0];n.parentNode.insertBefore(d,n)
              })(window,document,'script','https://www.datadoghq-browser-agent.com/us1/v5/datadog-rum.js','DD_RUM')
              window.DD_RUM.onReady(function() {
                window.DD_RUM.init({
                  clientToken: 'puba71073f072643721169b68f352438710',
                  applicationId: '2e321b35-c76b-4073-8d04-cc9a10461793',
                  site: 'datadoghq.com',
                  service: 'substack-web',
                  env: window._preloads.dd_env,
                  version: '5b5ec257d2826e0dd7cbc7c88ae99980ebfdf815',
                  sessionSampleRate: 1,
                  sessionReplaySampleRate: 100,
                  trackUserInteractions: window._preloads.dd_ti,
                  trackResources: true,
                  trackLongTasks: true,
                  defaultPrivacyLevel: 'mask-user-input',
                  allowedTracingUrls: [/https?:\/\/(.+\/.)?substack(cdn)?\.com/]
                });
              })
            </script>
            <!-- End Datadog Analytics -->

            <!-- Cloudflare Web Analytics -->
            <script defer="" src="./The Big LLM Architecture Comparison_files/beacon.min.js.download" data-cf-beacon="{&quot;token&quot;: &quot;216309cffb464db4b0e02daf0b8e8060&quot;}"></script>
            <!-- End Cloudflare Web Analytics -->
        

        <!-- Fallback tracking pixels -->
        

        

        <noscript>
    <style>
        #nojs-banner {
            position: fixed;
            bottom: 0;
            left: 0;
            padding: 16px 16px 16px 32px;
            width: 100%;
            box-sizing: border-box;
            background: red;
            color: white;
            font-family: -apple-system, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            font-size: 13px;
            line-height: 13px;
        }
        #nojs-banner a {
            color: inherit;
            text-decoration: underline;
        }
    </style>

    <div id="nojs-banner">
        This site requires JavaScript to run correctly. Please <a href="https://enable-javascript.com/" target="_blank">turn on JavaScript</a> or unblock scripts
    </div>
</noscript>


        

        

        
        
    <script defer="" src="./The Big LLM Architecture Comparison_files/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon="{&quot;rayId&quot;:&quot;982c31a50b1414c8&quot;,&quot;serverTiming&quot;:{&quot;name&quot;:{&quot;cfExtPri&quot;:true,&quot;cfEdge&quot;:true,&quot;cfOrigin&quot;:true,&quot;cfL4&quot;:true,&quot;cfSpeedBrain&quot;:true,&quot;cfCacheStatus&quot;:true}},&quot;version&quot;:&quot;2025.9.1&quot;,&quot;token&quot;:&quot;68cfe66b5c4749e2ba64d4d9640c04c0&quot;}" crossorigin="anonymous"></script><iframe height="0" width="0" style="display: none; visibility: hidden;" src="./The Big LLM Architecture Comparison_files/saved_resource(2).html"></iframe>


</body></html>